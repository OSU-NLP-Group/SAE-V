# saev/nn.py

```python
"""
Neural network architectures for sparse autoencoders.
"""

import io
import json
import logging
import os
import typing

import beartype
import einops
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import config


class Loss(typing.NamedTuple):
    """The composite loss terms for an autoencoder training batch."""

    mse: Float[Tensor, ""]
    """Reconstruction loss (mean squared error)."""
    sparsity: Float[Tensor, ""]
    """Sparsity loss, typically lambda * L1."""
    ghost_grad: Float[Tensor, ""]
    """Ghost gradient loss, if any."""
    l0: Float[Tensor, ""]
    """L0 magnitude of hidden activations."""
    l1: Float[Tensor, ""]
    """L1 magnitude of hidden activations."""

    @property
    def loss(self) -> Float[Tensor, ""]:
        """Total loss."""
        return self.mse + self.sparsity + self.ghost_grad


@jaxtyped(typechecker=beartype.beartype)
class SparseAutoencoder(torch.nn.Module):
    """
    Sparse auto-encoder (SAE) using L1 sparsity penalty.
    """

    cfg: config.SparseAutoencoder

    def __init__(self, cfg: config.SparseAutoencoder):
        super().__init__()

        self.cfg = cfg

        self.W_enc = torch.nn.Parameter(
            torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_vit, cfg.d_sae))
        )
        self.b_enc = torch.nn.Parameter(torch.zeros(cfg.d_sae))

        self.W_dec = torch.nn.Parameter(
            torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_sae, cfg.d_vit))
        )
        self.b_dec = torch.nn.Parameter(torch.zeros(cfg.d_vit))

        self.logger = logging.getLogger(f"sae(seed={cfg.seed})")

    def forward(
        self, x: Float[Tensor, "batch d_model"]
    ) -> tuple[Float[Tensor, "batch d_model"], Float[Tensor, "batch d_sae"], Loss]:
        """
        Given x, calculates the reconstructed x_hat, the intermediate activations f_x, and the loss.

        Arguments:
            x: a batch of ViT activations.
        """

        # Remove encoder bias as per Anthropic
        h_pre = (
            einops.einsum(
                x - self.b_dec, self.W_enc, "... d_vit, d_vit d_sae -> ... d_sae"
            )
            + self.b_enc
        )
        f_x = torch.nn.functional.relu(h_pre)
        x_hat = self.decode(f_x)

        # Some values of x and x_hat can be very large. We can calculate a safe MSE
        mse_loss = safe_mse(x_hat, x)

        mse_loss = mse_loss.mean()
        l0 = (f_x > 0).float().sum(axis=1).mean(axis=0)
        l1 = f_x.sum(axis=1).mean(axis=0)
        sparsity_loss = self.cfg.sparsity_coeff * l1
        # Ghost loss is included for backwards compatibility.
        ghost_loss = torch.zeros_like(mse_loss)

        return x_hat, f_x, Loss(mse_loss, sparsity_loss, ghost_loss, l0, l1)

    def decode(
        self, f_x: Float[Tensor, "batch d_sae"]
    ) -> Float[Tensor, "batch d_model"]:
        x_hat = (
            einops.einsum(f_x, self.W_dec, "... d_sae, d_sae d_vit -> ... d_vit")
            + self.b_dec
        )
        return x_hat

    @torch.no_grad()
    def init_b_dec(self, vit_acts: Float[Tensor, "n d_vit"]):
        if self.cfg.n_reinit_samples <= 0:
            self.logger.info("Skipping init_b_dec.")
            return
        previous_b_dec = self.b_dec.clone().cpu()
        vit_acts = vit_acts[: self.cfg.n_reinit_samples]
        assert len(vit_acts) == self.cfg.n_reinit_samples
        mean = vit_acts.mean(axis=0)
        previous_distances = torch.norm(vit_acts - previous_b_dec, dim=-1)
        distances = torch.norm(vit_acts - mean, dim=-1)
        self.logger.info(
            "Prev dist: %.3f; new dist: %.3f",
            previous_distances.median(axis=0).values.mean().item(),
            distances.median(axis=0).values.mean().item(),
        )
        self.b_dec.data = mean.to(self.b_dec.dtype).to(self.b_dec.device)

    @torch.no_grad()
    def normalize_w_dec(self):
        """
        Set W_dec to unit-norm columns.
        """
        if self.cfg.normalize_w_dec:
            self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)

    @torch.no_grad()
    def remove_parallel_grads(self):
        """
        Update grads so that they remove the parallel component
            (d_sae, d_vit) shape
        """
        if not self.cfg.remove_parallel_grads:
            return

        parallel_component = einops.einsum(
            self.W_dec.grad,
            self.W_dec.data,
            "d_sae d_vit, d_sae d_vit -> d_sae",
        )

        self.W_dec.grad -= einops.einsum(
            parallel_component,
            self.W_dec.data,
            "d_sae, d_sae d_vit -> d_sae d_vit",
        )


@jaxtyped(typechecker=beartype.beartype)
def ref_mse(
    x_hat: Float[Tensor, "*d"], x: Float[Tensor, "*d"], norm: bool = True
) -> Float[Tensor, "*d"]:
    mse_loss = torch.pow((x_hat - x.float()), 2)

    if norm:
        mse_loss /= (x**2).sum(dim=-1, keepdim=True).sqrt()
    return mse_loss


@jaxtyped(typechecker=beartype.beartype)
def safe_mse(
    x_hat: Float[Tensor, "*batch d"], x: Float[Tensor, "*batch d"], norm: bool = False
) -> Float[Tensor, "*batch d"]:
    upper = x.abs().max()
    x = x / upper
    x_hat = x_hat / upper

    mse = (x_hat - x) ** 2
    # (sam): I am now realizing that we normalize by the L2 norm of x.
    if norm:
        mse /= torch.linalg.norm(x, axis=-1, keepdim=True) + 1e-12
        return mse * upper

    return mse * upper * upper


@beartype.beartype
def dump(fpath: str, sae: SparseAutoencoder):
    """
    Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).

    Arguments:
        fpath: filepath to save checkpoint to.
        sae: sparse autoencoder checkpoint to save.
    """
    kwargs = vars(sae.cfg)

    os.makedirs(os.path.dirname(fpath), exist_ok=True)
    with open(fpath, "wb") as fd:
        kwargs_str = json.dumps(kwargs)
        fd.write((kwargs_str + "\n").encode("utf-8"))
        torch.save(sae.state_dict(), fd)


@beartype.beartype
def load(fpath: str, *, device: str = "cpu") -> SparseAutoencoder:
    """
    Loads a sparse autoencoder from disk.
    """
    with open(fpath, "rb") as fd:
        kwargs = json.loads(fd.readline().decode())
        buffer = io.BytesIO(fd.read())

    cfg = config.SparseAutoencoder(**kwargs)
    model = SparseAutoencoder(cfg)
    state_dict = torch.load(buffer, weights_only=True, map_location=device)
    model.load_state_dict(state_dict)
    return model

```

# saev/test_visuals.py

```python
import torch

from . import visuals


def test_gather_batched_small():
    values = torch.arange(0, 64, dtype=torch.float).view(4, 2, 8)
    i = torch.tensor([[0], [0], [1], [1]])
    actual = visuals.gather_batched(values, i)

    expected = torch.tensor([
        [[0, 1, 2, 3, 4, 5, 6, 7]],
        [[16, 17, 18, 19, 20, 21, 22, 23]],
        [[40, 41, 42, 43, 44, 45, 46, 47]],
        [[56, 57, 58, 59, 60, 61, 62, 63]],
    ]).float()

    torch.testing.assert_close(actual, expected)

```

# saev/helpers.py

```python
"""
Useful helpers for `saev`.
"""

import logging
import os
import time

import beartype


@beartype.beartype
def get_cache_dir() -> str:
    """
    Get cache directory from environment variables, defaulting to the current working directory (.)

    Returns:
        A path to a cache directory (might not exist yet).
    """
    cache_dir = ""
    for var in ("SAEV_CACHE", "HF_HOME", "HF_HUB_CACHE"):
        cache_dir = cache_dir or os.environ.get(var, "")
    return cache_dir or "."


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress", total: int = 0):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
            total: If non-zero, how long the iterable is.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)
        self.total = total

    def __iter__(self):
        start = time.time()

        try:
            total = len(self)
        except TypeError:
            total = None

        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if total is not None:
                    pred_min = (total - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        total,
                        (i + 1) / total * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        if self.total > 0:
            return self.total

        # Will throw exception.
        return len(self.it)


###################
# FLATTENED DICTS #
###################


@beartype.beartype
def flattened(
    dct: dict[str, object], *, sep: str = "."
) -> dict[str, str | int | float | bool | None]:
    """
    Flatten a potentially nested dict to a single-level dict with `.`-separated keys.
    """
    new = {}
    for key, value in dct.items():
        if isinstance(value, dict):
            for nested_key, nested_value in flattened(value).items():
                new[key + "." + nested_key] = nested_value
            continue

        new[key] = value

    return new


@beartype.beartype
def get(dct: dict[str, object], key: str, *, sep: str = ".") -> object:
    key = key.split(sep)
    key = list(reversed(key))

    while len(key) > 1:
        popped = key.pop()
        dct = dct[popped]

    return dct[key.pop()]

```

# saev/imaging.py

```python
import math

import beartype
import matplotlib
import numpy as np
from jaxtyping import Float, jaxtyped
from PIL import Image, ImageDraw

colormap = matplotlib.colormaps.get_cmap("plasma")


@jaxtyped(typechecker=beartype.beartype)
def add_highlights(
    img: Image.Image,
    patches: Float[np.ndarray, " n_patches"],
    *,
    upper: float | None = None,
    opacity: float = 0.9,
) -> Image.Image:
    if not len(patches):
        return img

    iw_np, ih_np = int(math.sqrt(len(patches))), int(math.sqrt(len(patches)))
    iw_px, ih_px = img.size
    pw_px, ph_px = iw_px // iw_np, ih_px // ih_np
    assert iw_np * ih_np == len(patches)

    # Create a transparent overlay
    overlay = Image.new("RGBA", img.size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(overlay)

    colors = (colormap(patches / (upper + 1e-9))[:, :3] * 256).astype(np.uint8)

    for p, (val, color) in enumerate(zip(patches, colors)):
        assert upper is not None
        val /= upper + 1e-9
        x_np, y_np = p % iw_np, p // ih_np
        draw.rectangle(
            [
                (x_np * pw_px, y_np * ph_px),
                (x_np * pw_px + pw_px, y_np * ph_px + ph_px),
            ],
            fill=(*color, int(opacity * val * 256)),
        )

    # Composite the original image and the overlay
    return Image.alpha_composite(img.convert("RGBA"), overlay)

```

# saev/test_config.py

```python
from . import config


def test_expand():
    cfg = {"lr": [1, 2, 3]}
    expected = [{"lr": 1}, {"lr": 2}, {"lr": 3}]
    actual = list(config.expand(cfg))

    assert expected == actual


def test_expand_two_fields():
    cfg = {"lr": [1, 2], "wd": [3, 4]}
    expected = [
        {"lr": 1, "wd": 3},
        {"lr": 1, "wd": 4},
        {"lr": 2, "wd": 3},
        {"lr": 2, "wd": 4},
    ]
    actual = list(config.expand(cfg))

    assert expected == actual


def test_expand_nested():
    cfg = {"sae": {"dim": [1, 2, 3]}}
    expected = [{"sae": {"dim": 1}}, {"sae": {"dim": 2}}, {"sae": {"dim": 3}}]
    actual = list(config.expand(cfg))

    assert expected == actual


def test_expand_nested_and_unnested():
    cfg = {"sae": {"dim": [1, 2]}, "lr": [3, 4]}
    expected = [
        {"sae": {"dim": 1}, "lr": 3},
        {"sae": {"dim": 1}, "lr": 4},
        {"sae": {"dim": 2}, "lr": 3},
        {"sae": {"dim": 2}, "lr": 4},
    ]
    actual = list(config.expand(cfg))

    assert expected == actual


def test_expand_nested_and_unnested_backwards():
    cfg = {"a": [False, True], "b": {"c": [False, True]}}
    expected = [
        {"a": False, "b": {"c": False}},
        {"a": False, "b": {"c": True}},
        {"a": True, "b": {"c": False}},
        {"a": True, "b": {"c": True}},
    ]
    actual = list(config.expand(cfg))

    assert expected == actual


def test_expand_multiple():
    cfg = {"a": [1, 2, 3], "b": {"c": [4, 5, 6]}}
    expected = [
        {"a": 1, "b": {"c": 4}},
        {"a": 1, "b": {"c": 5}},
        {"a": 1, "b": {"c": 6}},
        {"a": 2, "b": {"c": 4}},
        {"a": 2, "b": {"c": 5}},
        {"a": 2, "b": {"c": 6}},
        {"a": 3, "b": {"c": 4}},
        {"a": 3, "b": {"c": 5}},
        {"a": 3, "b": {"c": 6}},
    ]
    actual = list(config.expand(cfg))

    assert expected == actual

```

# saev/test_nn.py

```python
"""
Uses [hypothesis]() and [hypothesis-torch](https://hypothesis-torch.readthedocs.io/en/stable/compatability/) to generate test cases to compare our normalized MSE implementation to a reference MSE implementation.
"""

import hypothesis
import hypothesis_torch
import pytest
import torch
from jaxtyping import Float
from torch import Tensor

from . import nn


def test_safe_mse_same():
    x = torch.ones((45, 12), dtype=torch.float)
    x_hat = torch.ones((45, 12), dtype=torch.float)
    expected = torch.zeros((45, 12), dtype=torch.float)
    actual = nn.safe_mse(x_hat, x)
    torch.testing.assert_close(actual, expected)


def test_safe_mse_zero_x_hat():
    x = torch.ones((3, 2), dtype=torch.float)
    x_hat = torch.zeros((3, 2), dtype=torch.float)
    expected = torch.ones((3, 2), dtype=torch.float)
    actual = nn.safe_mse(x_hat, x, norm=False)
    torch.testing.assert_close(actual, expected)


def test_safe_mse_nonzero():
    x = torch.full((3, 2), 3, dtype=torch.float)
    x_hat = torch.ones((3, 2), dtype=torch.float)
    expected = nn.ref_mse(x_hat, x)
    actual = nn.safe_mse(x_hat, x)
    torch.testing.assert_close(actual, expected)


def test_safe_mse_large_x():
    x = torch.full((3, 2), 3e28, dtype=torch.float)
    x_hat = torch.ones((3, 2), dtype=torch.float)

    ref = nn.ref_mse(x_hat, x)
    assert ref.isnan().any()

    safe = nn.safe_mse(x_hat, x)
    assert not safe.isnan().any()


@pytest.mark.slow
@hypothesis.settings(suppress_health_check=[hypothesis.HealthCheck.too_slow])
@hypothesis.given(
    x_hat=hypothesis_torch.tensor_strategy(dtype=torch.float32, shape=(1, 2, 3)),
    x=hypothesis_torch.tensor_strategy(dtype=torch.float32, shape=(1, 2, 3)),
)
def test_safe_mse_hypothesis(x_hat: Float[Tensor, "1 2 3"], x: Float[Tensor, "1 2 3"]):
    hypothesis.assume((torch.linalg.norm(x, axis=-1) > 1e-15).all())
    hypothesis.assume(not x.isinf().all())

    expected = nn.ref_mse(x_hat, x)
    actual = nn.safe_mse(x_hat, x)
    torch.testing.assert_close(actual, expected)

```

# saev/__main__.py

```python
import logging
import tomllib
import typing

import beartype
import tyro

from . import config

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)

logger = logging.getLogger("saev")


@beartype.beartype
def activations(cfg: typing.Annotated[config.Activations, tyro.conf.arg(name="")]):
    """
    Save ViT activations for use later on.

    Args:
        cfg: Configuration for activations.
    """
    import saev.activations

    saev.activations.main(cfg)


@beartype.beartype
def train(
    cfg: typing.Annotated[config.Train, tyro.conf.arg(name="")],
    sweep: str | None = None,
):
    """
    Train an SAE over activations, optionally running a parallel grid search over a set of hyperparameters.

    Args:
        cfg: Baseline config for training an SAE.
        sweep: Path to .toml file defining the sweep parameters.
    """
    import submitit

    from . import config, training

    if sweep is not None:
        with open(sweep, "rb") as fd:
            cfgs, errs = config.grid(cfg, tomllib.load(fd))

        if errs:
            for err in errs:
                logger.warning("Error in config: %s", err)
            return

    else:
        cfgs = [cfg]

    cfgs = training.split_cfgs(cfgs)

    logger.info("Running %d training jobs.", len(cfgs))

    if cfg.slurm:
        executor = submitit.SlurmExecutor(folder=cfg.log_to)
        executor.update_parameters(
            time=60,
            partition="preemptible",
            gpus_per_node=1,
            cpus_per_task=cfg.n_workers + 4,
            stderr_to_stdout=True,
            account=cfg.slurm_acct,
        )
    else:
        executor = submitit.DebugExecutor(folder=cfg.log_to)

    jobs = [executor.submit(training.main, group) for group in cfgs]
    for job in jobs:
        job.result()


@beartype.beartype
def visuals(cfg: typing.Annotated[config.Visuals, tyro.conf.arg(name="")]):
    from . import visuals

    visuals.main(cfg)


if __name__ == "__main__":
    tyro.extras.subcommand_cli_from_dict({
        "activations": activations,
        "train": train,
        "visuals": visuals,
    })
    logger.info("Done.")

```

# saev/test_training.py

```python
from . import config, training


def test_split_cfgs_on_single_key():
    cfgs = [config.Train(n_workers=12), config.Train(n_workers=16)]
    expected = [[config.Train(n_workers=12)], [config.Train(n_workers=16)]]

    actual = training.split_cfgs(cfgs)

    assert actual == expected


def test_split_cfgs_on_single_key_with_multiple_per_key():
    cfgs = [
        config.Train(n_patches=12),
        config.Train(n_patches=16),
        config.Train(n_patches=16),
        config.Train(n_patches=16),
    ]
    expected = [
        [config.Train(n_patches=12)],
        [
            config.Train(n_patches=16),
            config.Train(n_patches=16),
            config.Train(n_patches=16),
        ],
    ]

    actual = training.split_cfgs(cfgs)

    assert actual == expected


def test_split_cfgs_on_multiple_keys_with_multiple_per_key():
    cfgs = [
        config.Train(n_patches=12, track=False),
        config.Train(n_patches=12, track=True),
        config.Train(n_patches=16, track=True),
        config.Train(n_patches=16, track=True),
        config.Train(n_patches=16, track=False),
    ]
    expected = [
        [config.Train(n_patches=12, track=False)],
        [config.Train(n_patches=12, track=True)],
        [
            config.Train(n_patches=16, track=True),
            config.Train(n_patches=16, track=True),
        ],
        [config.Train(n_patches=16, track=False)],
    ]

    actual = training.split_cfgs(cfgs)

    assert actual == expected


def test_split_cfgs_no_bad_keys():
    cfgs = [
        config.Train(n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=1e-4)),
        config.Train(n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=2e-4)),
        config.Train(n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=3e-4)),
        config.Train(n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=4e-4)),
        config.Train(n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=5e-4)),
    ]
    expected = [
        [
            config.Train(
                n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=1e-4)
            ),
            config.Train(
                n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=2e-4)
            ),
            config.Train(
                n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=3e-4)
            ),
            config.Train(
                n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=4e-4)
            ),
            config.Train(
                n_patches=12, sae=config.SparseAutoencoder(sparsity_coeff=5e-4)
            ),
        ]
    ]

    actual = training.split_cfgs(cfgs)

    assert actual == expected

```

# saev/visuals.py

```python
"""
There is some important notation used only in this file to dramatically shorten variable names.

Variables suffixed with `_im` refer to entire images, and variables suffixed with `_p` refer to patches.
"""

import collections.abc
import dataclasses
import json
import logging
import math
import os
import random
import typing

import beartype
import einops
import torch
from jaxtyping import Float, Int, jaxtyped
from PIL import Image
from torch import Tensor

from . import activations, config, helpers, imaging, nn

logger = logging.getLogger("visuals")


@beartype.beartype
def safe_load(path: str) -> object:
    return torch.load(path, map_location="cpu", weights_only=True)


@jaxtyped(typechecker=beartype.beartype)
def gather_batched(
    value: Float[Tensor, "batch n dim"], i: Int[Tensor, "batch k"]
) -> Float[Tensor, "batch k dim"]:
    batch_size, n, dim = value.shape  # noqa: F841
    _, k = i.shape

    batch_i = torch.arange(batch_size, device=value.device)[:, None].expand(-1, k)
    return value[batch_i, i]


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass
class GridElement:
    img: Image.Image
    label: str
    patches: Float[Tensor, " n_patches"]


@beartype.beartype
def make_img(elem: GridElement, *, upper: float | None = None) -> Image.Image:
    # Resize to 256x256 and crop to 224x224
    resize_size_px = (512, 512)
    resize_w_px, resize_h_px = resize_size_px
    crop_size_px = (448, 448)
    crop_w_px, crop_h_px = crop_size_px
    crop_coords_px = (
        (resize_w_px - crop_w_px) // 2,
        (resize_h_px - crop_h_px) // 2,
        (resize_w_px + crop_w_px) // 2,
        (resize_h_px + crop_h_px) // 2,
    )

    img = elem.img.resize(resize_size_px).crop(crop_coords_px)
    img = imaging.add_highlights(img, elem.patches.numpy(), upper=upper)
    return img


@jaxtyped(typechecker=beartype.beartype)
def get_new_topk(
    val1: Float[Tensor, "d_sae k"],
    i1: Int[Tensor, "d_sae k"],
    val2: Float[Tensor, "d_sae k"],
    i2: Int[Tensor, "d_sae k"],
    k: int,
) -> tuple[Float[Tensor, "d_sae k"], Int[Tensor, "d_sae k"]]:
    """
    Picks out the new top k values among val1 and val2. Also keeps track of i1 and i2, then indices of the values in the original dataset.

    Args:
        val1: top k original SAE values.
        i1: the patch indices of those original top k values.
        val2: top k incoming SAE values.
        i2: the patch indices of those incoming top k values.
        k: k.

    Returns:
        The new top k values and their patch indices.
    """
    all_val = torch.cat([val1, val2], dim=1)
    new_values, top_i = torch.topk(all_val, k=k, dim=1)

    all_i = torch.cat([i1, i2], dim=1)
    new_indices = torch.gather(all_i, 1, top_i)
    return new_values, new_indices


@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -> collections.abc.Iterator[tuple[int, int]]:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop


@jaxtyped(typechecker=beartype.beartype)
def get_sae_acts(
    vit_acts: Float[Tensor, "n d_vit"], sae: nn.SparseAutoencoder, cfg: config.Visuals
) -> Float[Tensor, "n d_sae"]:
    """
    Get SAE hidden layer activations for a batch of ViT activations.

    Args:
        vit_acts: Batch of ViT activations
        sae: Sparse autoencder.
        cfg: Experimental config.
    """
    sae_acts = []
    for start, end in batched_idx(len(vit_acts), cfg.sae_batch_size):
        _, f_x, *_ = sae(vit_acts[start:end].to(cfg.device))
        sae_acts.append(f_x)

    sae_acts = torch.cat(sae_acts, dim=0)
    sae_acts = sae_acts.to(cfg.device)
    return sae_acts


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class TopKImg:
    ".. todo:: Document this class."

    top_values: Float[Tensor, "d_sae k"]
    top_i: Int[Tensor, "d_sae k"]
    mean_values: Float[Tensor, " d_sae"]
    sparsity: Float[Tensor, " d_sae"]
    distributions: Float[Tensor, "m n"]
    percentiles: Float[Tensor, " d_sae"]


@beartype.beartype
@torch.inference_mode()
def get_topk_img(cfg: config.Visuals) -> TopKImg:
    """
    Gets the top k images for each latent in the SAE.
    The top k images are for latent i are sorted by

        max over all images: f_x(cls)[i]

    Thus, we will never have duplicate images for a given latent.
    But we also will not have patch-level activations (a nice heatmap).

    Args:
        cfg: Config.

    Returns:
        A tuple of TopKImg and the first m features' activation distributions.
    """
    assert cfg.sort_by == "img"
    assert cfg.data.patches == "cls"

    sae = nn.load(cfg.ckpt).to(cfg.device)
    dataset = activations.Dataset(cfg.data)

    top_values_im_SK = torch.full((sae.cfg.d_sae, cfg.top_k), -1.0, device=cfg.device)
    top_i_im_SK = torch.zeros(
        (sae.cfg.d_sae, cfg.top_k), dtype=torch.int, device=cfg.device
    )
    sparsity_S = torch.zeros((sae.cfg.d_sae,), device=cfg.device)
    mean_values_S = torch.zeros((sae.cfg.d_sae,), device=cfg.device)

    distributions_MN = torch.zeros((cfg.n_distributions, len(dataset)), device="cpu")
    estimator = PercentileEstimator(
        cfg.percentile, len(dataset), shape=(sae.cfg.d_sae,)
    )

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.topk_batch_size,
        shuffle=False,
        num_workers=cfg.n_workers,
        drop_last=False,
    )

    logger.info("Loaded SAE and data.")

    for batch in helpers.progress(dataloader, desc="picking top-k"):
        vit_acts_BD = batch["act"]
        sae_acts_BS = get_sae_acts(vit_acts_BD, sae, cfg)

        for sae_act_S in sae_acts_BS:
            estimator.update(sae_act_S)

        sae_acts_SB = einops.rearrange(sae_acts_BS, "batch d_sae -> d_sae batch")
        distributions_MN[:, batch["image_i"]] = sae_acts_SB[: cfg.n_distributions].to(
            "cpu"
        )

        mean_values_S += einops.reduce(sae_acts_SB, "d_sae batch -> d_sae", "sum")
        sparsity_S += einops.reduce((sae_acts_SB > 0), "d_sae batch -> d_sae", "sum")

        sae_acts_SK, k = torch.topk(sae_acts_SB, k=cfg.top_k, dim=1)
        i_im_SK = batch["image_i"].to(cfg.device)[k]

        all_values_im_2SK = torch.cat((top_values_im_SK, sae_acts_SK), axis=1)

        top_values_im_SK, k = torch.topk(all_values_im_2SK, k=cfg.top_k, axis=1)
        top_i_im_SK = torch.gather(torch.cat((top_i_im_SK, i_im_SK), axis=1), 1, k)

    mean_values_S /= sparsity_S
    sparsity_S /= len(dataset)

    return TopKImg(
        top_values_im_SK,
        top_i_im_SK,
        mean_values_S,
        sparsity_S,
        distributions_MN,
        estimator.estimate.cpu(),
    )


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class TopKPatch:
    ".. todo:: Document this class."

    top_values: Float[Tensor, "d_sae k n_patches_per_img"]
    top_i: Int[Tensor, "d_sae k"]
    mean_values: Float[Tensor, " d_sae"]
    sparsity: Float[Tensor, " d_sae"]
    distributions: Float[Tensor, "m n"]
    percentiles: Float[Tensor, " d_sae"]


@beartype.beartype
@torch.inference_mode()
def get_topk_patch(cfg: config.Visuals) -> TopKPatch:
    """
    Gets the top k images for each latent in the SAE.
    The top k images are for latent i are sorted by

        max over all patches: f_x(patch)[i]

    Thus, we could end up with duplicate images in the top k, if an image has more than one patch that maximally activates an SAE latent.

    Args:
        cfg: Config.

    Returns:
        A tuple of TopKPatch and m randomly sampled activation distributions.
    """
    assert cfg.sort_by == "patch"
    assert cfg.data.patches == "patches"

    sae = nn.load(cfg.ckpt).to(cfg.device)
    dataset = activations.Dataset(cfg.data)

    top_values_p = torch.full(
        (sae.cfg.d_sae, cfg.top_k, dataset.metadata.n_patches_per_img),
        -1.0,
        device=cfg.device,
    )
    top_i_im = torch.zeros(
        (sae.cfg.d_sae, cfg.top_k), dtype=torch.int, device=cfg.device
    )

    sparsity_S = torch.zeros((sae.cfg.d_sae,), device=cfg.device)
    mean_values_S = torch.zeros((sae.cfg.d_sae,), device=cfg.device)

    distributions_MN = torch.zeros((cfg.n_distributions, len(dataset)), device="cpu")
    estimator = PercentileEstimator(
        cfg.percentile, len(dataset), shape=(sae.cfg.d_sae,)
    )

    batch_size = (
        cfg.topk_batch_size
        // dataset.metadata.n_patches_per_img
        * dataset.metadata.n_patches_per_img
    )
    n_imgs_per_batch = batch_size // dataset.metadata.n_patches_per_img

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=cfg.n_workers,
        # See if you can change this to false and still pass the beartype check.
        drop_last=True,
    )

    logger.info("Loaded SAE and data.")

    for batch in helpers.progress(dataloader, desc="picking top-k"):
        vit_acts_BD = batch["act"]
        sae_acts_BS = get_sae_acts(vit_acts_BD, sae, cfg)

        for sae_act_S in sae_acts_BS:
            estimator.update(sae_act_S)

        sae_acts_SB = einops.rearrange(sae_acts_BS, "batch d_sae -> d_sae batch")
        distributions_MN[:, batch["image_i"]] = sae_acts_SB[: cfg.n_distributions].to(
            "cpu"
        )

        mean_values_S += einops.reduce(sae_acts_SB, "d_sae batch -> d_sae", "sum")
        sparsity_S += einops.reduce((sae_acts_SB > 0), "d_sae batch -> d_sae", "sum")

        i_im = torch.sort(torch.unique(batch["image_i"])).values
        values_p = sae_acts_SB.view(
            sae.cfg.d_sae, len(i_im), dataset.metadata.n_patches_per_img
        )

        # Checks that I did my reshaping correctly.
        assert values_p.shape[1] == i_im.shape[0]
        assert len(i_im) == n_imgs_per_batch

        _, k = torch.topk(sae_acts_SB, k=cfg.top_k, dim=1)
        k_im = k // dataset.metadata.n_patches_per_img

        values_p = gather_batched(values_p, k_im)
        i_im = i_im.to(cfg.device)[k_im]

        all_values_p = torch.cat((top_values_p, values_p), axis=1)
        _, k = torch.topk(all_values_p.max(axis=-1).values, k=cfg.top_k, axis=1)

        top_values_p = gather_batched(all_values_p, k)
        top_i_im = torch.gather(torch.cat((top_i_im, i_im), axis=1), 1, k)

    mean_values_S /= sparsity_S
    sparsity_S /= len(dataset)

    return TopKPatch(
        top_values_p,
        top_i_im,
        mean_values_S,
        sparsity_S,
        distributions_MN,
        estimator.estimate.cpu(),
    )


@beartype.beartype
@torch.inference_mode()
def dump_activations(cfg: config.Visuals):
    """
    For each SAE latent, we want to know which images have the most total "activation".
    That is, we keep track of each patch
    """
    if cfg.sort_by == "img":
        topk = get_topk_img(cfg)
    elif cfg.sort_by == "patch":
        topk = get_topk_patch(cfg)
    else:
        typing.assert_never(cfg.sort_by)

    os.makedirs(cfg.root, exist_ok=True)

    torch.save(topk.top_values, cfg.top_values_fpath)
    torch.save(topk.top_i, cfg.top_img_i_fpath)
    torch.save(topk.mean_values, cfg.mean_values_fpath)
    torch.save(topk.sparsity, cfg.sparsity_fpath)
    torch.save(topk.distributions, cfg.distributions_fpath)
    torch.save(topk.percentiles, cfg.percentiles_fpath)


@jaxtyped(typechecker=beartype.beartype)
def plot_activation_distributions(
    cfg: config.Visuals, distributions: Float[Tensor, "m n"]
):
    import matplotlib.pyplot as plt
    import numpy as np

    m, _ = distributions.shape

    n_rows = int(math.sqrt(m))
    n_cols = n_rows
    fig, axes = plt.subplots(
        figsize=(4 * n_cols, 4 * n_rows),
        nrows=n_rows,
        ncols=n_cols,
        sharex=True,
        sharey=True,
    )

    _, bins = np.histogram(np.log10(distributions[distributions > 0].numpy()), bins=100)

    percentiles = [90, 95, 99, 100]
    colors = ("red", "darkorange", "gold", "lime")

    for dist, ax in zip(distributions, axes.reshape(-1)):
        vals = np.log10(dist[dist > 0].numpy())

        ax.hist(vals, bins=bins)

        if vals.size == 0:
            continue

        for i, (percentile, color) in enumerate(
            zip(np.percentile(vals, percentiles), colors)
        ):
            ax.axvline(percentile, color=color, label=f"{percentiles[i]}th %-ile")

        for i, (percentile, color) in enumerate(zip(percentiles, colors)):
            estimator = PercentileEstimator(percentile, len(vals))
            for v in vals:
                estimator.update(v)
            ax.axvline(
                estimator.estimate,
                color=color,
                linestyle="--",
                label=f"Est. {percentiles[i]}th %-ile",
            )

    ax.legend()

    fig.tight_layout()
    return fig


@beartype.beartype
@torch.inference_mode()
def main(cfg: config.Visuals):
    """
    .. todo:: document this function.

    Dump top-k images to a directory.

    Args:
        cfg: Configuration object.
    """

    try:
        top_values = safe_load(cfg.top_values_fpath)
        sparsity = safe_load(cfg.sparsity_fpath)
        mean_values = safe_load(cfg.mean_values_fpath)
        top_i = safe_load(cfg.top_img_i_fpath)
        distributions = safe_load(cfg.distributions_fpath)
        _ = safe_load(cfg.percentiles_fpath)
    except FileNotFoundError as err:
        logger.warning("Need to dump files: %s", err)
        dump_activations(cfg)
        return main(cfg)

    d_sae, cached_topk, *rest = top_values.shape
    # Check that the data is at least shaped correctly.
    assert cfg.top_k == cached_topk
    if cfg.sort_by == "img":
        assert len(rest) == 0
    elif cfg.sort_by == "patch":
        assert len(rest) == 1
        n_patches = rest[0]
        assert n_patches > 0
    else:
        typing.assert_never(cfg.sort_by)

    logger.info("Loaded sorted data.")

    os.makedirs(cfg.root, exist_ok=True)
    fig_fpath = os.path.join(
        cfg.root, f"{cfg.n_distributions}_activation_distributions.png"
    )
    plot_activation_distributions(cfg, distributions).savefig(fig_fpath, dpi=300)
    logger.info(
        "Saved %d activation distributions to '%s'.", cfg.n_distributions, fig_fpath
    )

    dataset = activations.get_dataset(cfg.images, img_transform=None)

    min_log_freq, max_log_freq = cfg.log_freq_range
    min_log_value, max_log_value = cfg.log_value_range

    mask = (
        (min_log_freq < torch.log10(sparsity))
        & (torch.log10(sparsity) < max_log_freq)
        & (min_log_value < torch.log10(mean_values))
        & (torch.log10(mean_values) < max_log_value)
    )

    neurons = cfg.include_latents
    random_neurons = torch.arange(d_sae)[mask.cpu()].tolist()
    random.seed(cfg.seed)
    random.shuffle(random_neurons)
    neurons += random_neurons[: cfg.n_latents]

    for i in helpers.progress(neurons, desc="saving visuals"):
        neuron_dir = os.path.join(cfg.root, "neurons", str(i))
        os.makedirs(neuron_dir, exist_ok=True)

        # Image grid
        elems = []
        seen_i_im = set()
        for i_im, values_p in zip(top_i[i].tolist(), top_values[i]):
            if i_im in seen_i_im:
                continue

            example = dataset[i_im]
            if cfg.sort_by == "img":
                elem = GridElement(example["image"], example["label"], torch.tensor([]))
            elif cfg.sort_by == "patch":
                elem = GridElement(example["image"], example["label"], values_p)
            else:
                typing.assert_never(cfg.sort_by)
            elems.append(elem)

            seen_i_im.add(i_im)

        # How to scale values.
        upper = None
        if top_values[i].numel() > 0:
            upper = top_values[i].max().item()

        for j, elem in enumerate(elems):
            img = make_img(elem, upper=upper)
            img.save(os.path.join(neuron_dir, f"{j}.png"))
            with open(os.path.join(neuron_dir, f"{j}.txt"), "w") as fd:
                fd.write(elem.label + "\n")

        # Metadata
        metadata = {
            "neuron": i,
            "log10_freq": torch.log10(sparsity[i]).item(),
            "log10_value": torch.log10(mean_values[i]).item(),
        }
        with open(os.path.join(neuron_dir, "metadata.json"), "w") as fd:
            json.dump(metadata, fd)


@beartype.beartype
class PercentileEstimator:
    def __init__(
        self,
        percentile: float | int,
        total: int,
        lr: float = 1e-3,
        shape: tuple[int, ...] = (),
    ):
        self.percentile = percentile
        self.total = total
        self.lr = lr

        self._estimate = torch.zeros(shape)
        self._step = 0

    def update(self, x):
        """
        Update the estimator with a new value.

        This method maintains the marker positions using the P2 algorithm rules.
        When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions.

        Arguments:
            x: The new value to incorporate into the estimation
        """
        self._step += 1

        step_size = self.lr * (self.total - self._step) / self.total

        # Is a no-op if it's already on the same device.
        self._estimate = self._estimate.to(x.device)

        self._estimate += step_size * (
            torch.sign(x - self._estimate) + 2 * self.percentile / 100 - 1.0
        )

    @property
    def estimate(self):
        return self._estimate


@beartype.beartype
def test_online_quantile_estimation(true: float, percentile: float):
    import matplotlib.pyplot as plt
    import numpy as np
    import tqdm

    rng = np.random.default_rng(seed=0)
    n = 3_000_000
    estimator = PercentileEstimator(percentile, n)

    dist, preds = np.zeros(n), np.zeros(n)
    for i in tqdm.tqdm(range(n), desc="Getting estimates."):
        sampled = rng.normal(true)
        estimator.update(sampled)
        dist[i] = sampled
        preds[i] = estimator.estimate

    fig, ax = plt.subplots()
    ax.plot(preds, label=f"Pred. {percentile * 100}th %-ile")
    ax.axhline(
        np.percentile(dist, percentile * 100),
        label=f"True {percentile * 100}th %-ile",
        color="tab:red",
    )
    ax.legend()
    fig.tight_layout()
    fig.savefig("online_median_normal.png")


if __name__ == "__main__":
    import tyro

    tyro.cli(test_online_quantile_estimation)

```

# saev/training.py

```python
"""
Trains many SAEs in parallel to amortize the cost of loading a single batch of data over many SAE training runs.
"""

import dataclasses
import json
import logging
import os.path

import beartype
import einops
import numpy as np
import torch
from jaxtyping import Float
from torch import Tensor

import wandb

from . import activations, config, helpers, nn

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger("train")


@torch.no_grad()
def init_b_dec_batched(saes: torch.nn.ModuleList, dataset: activations.Dataset):
    n_samples = max(sae.cfg.n_reinit_samples for sae in saes)
    if not n_samples:
        return
    # Pick random samples using first SAE's seed.
    perm = np.random.default_rng(seed=saes[0].cfg.seed).permutation(len(dataset))
    perm = perm[:n_samples]
    examples, _, _ = zip(*[
        dataset[p.item()]
        for p in helpers.progress(perm, every=25_000, desc="examples to re-init b_dec")
    ])
    vit_acts = torch.stack(examples)
    for sae in saes:
        sae.init_b_dec(vit_acts[: sae.cfg.n_reinit_samples])


@beartype.beartype
def make_saes(
    cfgs: list[config.SparseAutoencoder],
) -> tuple[torch.nn.ModuleList, list[dict[str, object]]]:
    param_groups = []
    saes = []
    for cfg in cfgs:
        sae = nn.SparseAutoencoder(cfg)
        saes.append(sae)
        # Use an empty LR because our first step is warmup.
        param_groups.append({"params": sae.parameters(), "lr": 0.0})

    return torch.nn.ModuleList(saes), param_groups


##################
# Parallel Wandb #
##################


MetricQueue = list[tuple[int, dict[str, object]]]


class ParallelWandbRun:
    """
    Inspired by https://community.wandb.ai/t/is-it-possible-to-log-to-multiple-runs-simultaneously/4387/3.
    """

    def __init__(
        self, project: str, cfgs: list[config.Train], mode: str, tags: list[str]
    ):
        cfg, *cfgs = cfgs
        self.project = project
        self.cfgs = cfgs
        self.mode = mode
        self.tags = tags

        self.live_run = wandb.init(project=project, config=cfg, mode=mode, tags=tags)

        self.metric_queues: list[MetricQueue] = [[] for _ in self.cfgs]

    def log(self, metrics: list[dict[str, object]], *, step: int):
        metric, *metrics = metrics
        self.live_run.log(metric, step=step)
        for queue, metric in zip(self.metric_queues, metrics):
            queue.append((step, metric))

    def finish(self) -> list[str]:
        ids = [self.live_run.id]
        # Log the rest of the runs.
        self.live_run.finish()

        for queue, cfg in zip(self.metric_queues, self.cfgs):
            run = wandb.init(
                project=self.project,
                config=cfg,
                mode=self.mode,
                tags=self.tags + ["queued"],
            )
            for step, metric in queue:
                run.log(metric, step=step)
            ids.append(run.id)
            run.finish()

        return ids


@beartype.beartype
def main(cfgs: list[config.Train]) -> list[str]:
    saes, run, steps = train(cfgs)
    # Cheap(-ish) evaluation
    eval_metrics = evaluate(cfgs, saes)
    metrics = [metric.for_wandb() for metric in eval_metrics]
    run.log(metrics, step=steps)
    ids = run.finish()

    for cfg, id, metric, sae in zip(cfgs, ids, eval_metrics, saes):
        logger.info(
            "Checkpoint %s has %d dense features (%.1f)",
            id,
            metric.n_dense,
            metric.n_dense / sae.cfg.d_sae * 100,
        )
        logger.info(
            "Checkpoint %s has %d dead features (%.1f%%)",
            id,
            metric.n_dead,
            metric.n_dead / sae.cfg.d_sae * 100,
        )
        logger.info(
            "Checkpoint %s has %d *almost* dead (<1e-7) features (%.1f)",
            id,
            metric.n_almost_dead,
            metric.n_almost_dead / sae.cfg.d_sae * 100,
        )

        ckpt_fpath = os.path.join(cfg.ckpt_path, id, "sae.pt")
        nn.dump(ckpt_fpath, sae)
        logger.info("Dumped checkpoint to '%s'.", ckpt_fpath)
        cfg_fpath = os.path.join(cfg.ckpt_path, id, "config.json")
        with open(cfg_fpath, "w") as fd:
            json.dump(dataclasses.asdict(cfg), fd, indent=4)

    return ids


@beartype.beartype
def train(
    cfgs: list[config.Train],
) -> tuple[torch.nn.ModuleList, ParallelWandbRun, int]:
    """
    Explicitly declare the optimizer, schedulers, dataloader, etc outside of `main` so that all the variables are dropped from scope and can be garbage collected.
    """
    if len(split_cfgs(cfgs)) != 1:
        raise ValueError("Configs are not parallelizeable: {cfgs}.")

    err_msg = "ghost grads are disabled in current codebase."
    assert all(not c.sae.ghost_grads for c in cfgs), err_msg

    logger.info("Parallelizing %d runs.", len(cfgs))

    cfg = cfgs[0]
    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than
        # float16 and almost as accurate as float32
        # This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    dataset = activations.Dataset(cfg.data)
    saes, param_groups = make_saes([c.sae for c in cfgs])

    mode = "online" if cfg.track else "disabled"
    tags = [cfg.tag] if cfg.tag else []
    run = ParallelWandbRun(cfg.wandb_project, cfgs, mode, tags)

    optimizer = torch.optim.Adam(param_groups, fused=True)
    lr_schedulers = [Warmup(0.0, c.lr, c.n_lr_warmup) for c in cfgs]
    sparsity_schedulers = [
        Warmup(0.0, c.sae.sparsity_coeff, c.n_sparsity_warmup) for c in cfgs
    ]

    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=cfg.sae_batch_size, num_workers=cfg.n_workers, shuffle=True
    )

    dataloader = BatchLimiter(dataloader, cfg.n_patches)

    saes.train()
    saes = saes.to(cfg.device)

    global_step, n_patches_seen = 0, 0

    for batch in helpers.progress(dataloader, every=cfg.log_every):
        acts_BD = batch["act"].to(cfg.device, non_blocking=True)
        for sae in saes:
            sae.normalize_w_dec()
        # Forward passes
        _, _, losses = zip(*(sae(acts_BD) for sae in saes))

        n_patches_seen += len(acts_BD)

        with torch.no_grad():
            if (global_step + 1) % cfg.log_every == 0:
                metrics = [
                    {
                        "losses/mse": loss.mse.item(),
                        "losses/l1": loss.l1.item(),
                        "losses/sparsity": loss.sparsity.item(),
                        "losses/ghost_grad": loss.ghost_grad.item(),
                        "losses/loss": loss.loss.item(),
                        "metrics/l0": loss.l0.item(),
                        "metrics/l1": loss.l1.item(),
                        "progress/n_patches_seen": n_patches_seen,
                        "progress/learning_rate": group["lr"],
                        "progress/sparsity_coeff": sae.sparsity_coeff,
                    }
                    for loss, sae, group in zip(losses, saes, optimizer.param_groups)
                ]
                run.log(metrics, step=global_step)

                logger.info(
                    "loss: %.5f, mse loss: %.5f, sparsity loss: %.5f, l0: %.5f, l1: %.5f",
                    losses[0].loss.item(),
                    losses[0].mse.item(),
                    losses[0].sparsity.item(),
                    losses[0].l0.item(),
                    losses[0].l1.item(),
                )

        for loss in losses:
            loss.loss.backward()

        for sae in saes:
            sae.remove_parallel_grads()

        optimizer.step()

        # Update LR and sparsity coefficients.
        for param_group, scheduler in zip(optimizer.param_groups, lr_schedulers):
            param_group["lr"] = scheduler.step()

        for sae, scheduler in zip(saes, sparsity_schedulers):
            sae.sparsity_coeff = scheduler.step()

        # Don't need these anymore.
        optimizer.zero_grad()

        global_step += 1

    return saes, run, global_step


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class EvalMetrics:
    """Results of evaluating a trained SAE on a datset."""

    l0: float
    """Mean L0 across all examples."""
    l1: float
    """Mean L1 across all examples."""
    mse: float
    """Mean MSE across all examples."""
    n_dead: int
    """Number of neurons that never fired on any example."""
    n_almost_dead: int
    """Number of neurons that fired on fewer than `almost_dead_threshold` of examples."""
    n_dense: int
    """Number of neurons that fired on more than `dense_threshold` of examples."""

    freqs: Float[Tensor, " d_sae"]
    """How often each feature fired."""
    mean_values: Float[Tensor, " d_sae"]
    """The mean value for each feature when it did fire."""

    almost_dead_threshold: float
    """Threshold for an "almost dead" neuron."""
    dense_threshold: float
    """Threshold for a dense neuron."""

    def for_wandb(self) -> dict[str, int | float]:
        dct = dataclasses.asdict(self)
        # Store arrays as tables.
        dct["freqs"] = wandb.Table(columns=["freq"], data=dct["freqs"][:, None].numpy())
        dct["mean_values"] = wandb.Table(
            columns=["mean_value"], data=dct["mean_values"][:, None].numpy()
        )
        return {f"eval/{key}": value for key, value in dct.items()}


@beartype.beartype
@torch.no_grad()
def evaluate(cfgs: list[config.Train], saes: torch.nn.ModuleList) -> list[EvalMetrics]:
    """
    Evaluates SAE quality by counting the number of dead features and the number of dense features.
    Also makes histogram plots to help human qualitative comparison.

    .. todo:: Develop automatic methods to use histogram and feature frequencies to evaluate quality with a single number.
    """

    torch.cuda.empty_cache()

    if len(split_cfgs(cfgs)) != 1:
        raise ValueError("Configs are not parallelizeable: {cfgs}.")

    saes.eval()

    cfg = cfgs[0]

    almost_dead_lim = 1e-7
    dense_lim = 1e-2

    dataset = activations.Dataset(cfg.data)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=cfg.sae_batch_size, num_workers=cfg.n_workers, shuffle=False
    )

    n_fired = torch.zeros((len(cfgs), saes[0].cfg.d_sae))
    values = torch.zeros((len(cfgs), saes[0].cfg.d_sae))
    total_l0 = torch.zeros(len(cfgs))
    total_l1 = torch.zeros(len(cfgs))
    total_mse = torch.zeros(len(cfgs))

    for batch in helpers.progress(dataloader, desc="eval", every=cfg.log_every):
        acts_BD = batch["act"].to(cfg.device, non_blocking=True)
        for i, sae in enumerate(saes):
            _, f_x_BS, loss = sae(acts_BD)
            n_fired[i] += einops.reduce(f_x_BS > 0, "batch d_sae -> d_sae", "sum").cpu()
            values[i] += einops.reduce(f_x_BS, "batch d_sae -> d_sae", "sum").cpu()
            total_l0[i] += loss.l0.cpu()
            total_l1[i] += loss.l1.cpu()
            total_mse[i] += loss.mse.cpu()

    mean_values = values / n_fired
    freqs = n_fired / len(dataset)

    l0 = (total_l0 / len(dataloader)).tolist()
    l1 = (total_l1 / len(dataloader)).tolist()
    mse = (total_mse / len(dataloader)).tolist()

    n_dead = einops.reduce(freqs == 0, "n_saes d_sae -> n_saes", "sum").tolist()
    n_almost_dead = einops.reduce(
        freqs < almost_dead_lim, "n_saes d_sae -> n_saes", "sum"
    ).tolist()
    n_dense = einops.reduce(freqs > dense_lim, "n_saes d_sae -> n_saes", "sum").tolist()

    metrics = []
    for row in zip(l0, l1, mse, n_dead, n_almost_dead, n_dense, freqs, mean_values):
        metrics.append(EvalMetrics(*row, almost_dead_lim, dense_lim))

    return metrics


class BatchLimiter:
    """
    Limits the number of batches to only return `n_samples` total samples.
    """

    def __init__(self, dataloader: torch.utils.data.DataLoader, n_samples: int):
        self.dataloader = dataloader
        self.n_samples = n_samples
        self.batch_size = dataloader.batch_size

    def __len__(self) -> int:
        return self.n_samples // self.batch_size

    def __iter__(self):
        self.n_seen = 0
        while True:
            for batch in self.dataloader:
                yield batch

                # Sometimes we underestimate because the final batch in the dataloader might not be a full batch.
                self.n_seen += self.batch_size
                if self.n_seen > self.n_samples:
                    return

            # We try to mitigate the above issue by ignoring the last batch if we don't have drop_last.
            if not self.dataloader.drop_last:
                self.n_seen -= self.batch_size


#####################
# Parallel Training #
#####################


CANNOT_PARALLELIZE = set([
    "data",
    "n_workers",
    "n_patches",
    "sae_batch_size",
    "track",
    "wandb_project",
    "tag",
    "log_every",
    "ckpt_path",
    "device",
    "slurm",
    "slurm_acct",
    "log_to",
    "sae.exp_factor",
    "sae.d_vit",
])


@beartype.beartype
def split_cfgs(cfgs: list[config.Train]) -> list[list[config.Train]]:
    """
    Splits configs into groups that can be parallelized.

    Arguments:
        A list of configs from a sweep file.

    Returns:
        A list of lists, where the configs in each sublist do not differ in any keys that are in `CANNOT_PARALLELIZE`. This means that each sublist is a valid "parallel" set of configs for `train`.
    """
    # Group configs by their values for CANNOT_PARALLELIZE keys
    groups = {}
    for cfg in cfgs:
        dct = dataclasses.asdict(cfg)

        # Create a key tuple from the values of CANNOT_PARALLELIZE keys
        key_values = []
        for key in sorted(CANNOT_PARALLELIZE):
            key_values.append((key, make_hashable(helpers.get(dct, key))))
        group_key = tuple(key_values)

        if group_key not in groups:
            groups[group_key] = []
        groups[group_key].append(cfg)

    # Convert groups dict to list of lists
    return list(groups.values())


def make_hashable(obj):
    return json.dumps(obj, sort_keys=True)


##############
# Schedulers #
##############


@beartype.beartype
class Scheduler:
    def step(self) -> float:
        err_msg = f"{self.__class__.__name__} must implement step()."
        raise NotImplementedError(err_msg)

    def __repr__(self) -> str:
        err_msg = f"{self.__class__.__name__} must implement __repr__()."
        raise NotImplementedError(err_msg)


@beartype.beartype
class Warmup(Scheduler):
    """
    Linearly increases from `init` to `final` over `n_warmup_steps` steps.
    """

    def __init__(self, init: float, final: float, n_steps: int):
        self.final = final
        self.init = init
        self.n_steps = n_steps
        self._step = 0

    def step(self) -> float:
        self._step += 1
        if self._step < self.n_steps:
            return self.init + (self.final - self.init) * (self._step / self.n_steps)

        return self.final

    def __repr__(self) -> str:
        return f"Warmup(init={self.init}, final={self.final}, n_steps={self.n_steps})"


def _plot_example_schedules():
    import matplotlib.pyplot as plt
    import numpy as np

    fig, ax = plt.subplots()

    n_steps = 1000
    xs = np.arange(n_steps)

    schedule = Warmup(0.1, 0.9, 100)
    ys = [schedule.step() for _ in xs]

    ax.plot(xs, ys, label=str(schedule))

    fig.tight_layout()
    fig.savefig("schedules.png")


if __name__ == "__main__":
    _plot_example_schedules()

```

# saev/__init__.py

```python
"""
saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.

The main entrypoint to the package is in `__main__`; use `python -m saev --help` to see the options and documentation for the script.

.. include:: ./guide.md

.. include:: ./related-work.md

.. include:: ./reproduce.md

.. include:: ./inference.md
"""

```

# saev/config.py

```python
"""
All configs for all saev jobs.

## Import Times

This module should be very fast to import so that `python main.py --help` is fast.
This means that the top-level imports should not include big packages like numpy, torch, etc.
For example, `TreeOfLife.n_imgs` imports numpy when it's needed, rather than importing it at the top level.

Also contains code for expanding configs with lists into lists of configs (grid search).
Might be expanded in the future to support pseudo-random sampling from distributions to support random hyperparameter search, as in [this file](https://github.com/samuelstevens/sax/blob/main/sax/sweep.py).
"""

import collections.abc
import dataclasses
import itertools
import os
import typing

import beartype


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class ImagenetDataset:
    """Configuration for HuggingFace Imagenet."""

    name: str = "ILSVRC/imagenet-1k"
    """Dataset name on HuggingFace. Don't need to change this.."""
    split: str = "train"
    """Dataset split. For the default ImageNet-1K dataset, can either be 'train', 'validation' or 'test'."""

    @property
    def n_imgs(self) -> int:
        """Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires loading the dataset. If you need to reference this number very often, cache it in a local variable."""
        import datasets

        dataset = datasets.load_dataset(
            self.name, split=self.split, trust_remote_code=True
        )
        return len(dataset)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class ImageFolderDataset:
    """Configuration for a generic image folder dataset."""

    root: str = os.path.join(".", "data", "split")
    """Where the class folders with images are stored."""

    @property
    def n_imgs(self) -> int:
        """Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires walking the directory structure. If you need to reference this number very often, cache it in a local variable."""
        n = 0
        for _, _, files in os.walk(self.root):
            n += len(files)
        return n


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Ade20kDataset:
    """ """

    root: str = os.path.join(".", "data", "ade20k")
    """Where the class folders with images are stored."""
    split: typing.Literal["training", "validation"] = "training"
    """Data split."""

    @property
    def n_imgs(self) -> int:
        if self.split == "validation":
            return 2000
        else:
            return 20210


DatasetConfig = ImagenetDataset | ImageFolderDataset | Ade20kDataset


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Activations:
    """
    Configuration for calculating and saving ViT activations.
    """

    data: DatasetConfig = dataclasses.field(default_factory=ImagenetDataset)
    """Which dataset to use."""
    dump_to: str = os.path.join(".", "shards")
    """Where to write shards."""
    vit_family: typing.Literal["clip", "siglip", "dinov2", "moondream2"] = "clip"
    """Which model family."""
    vit_ckpt: str = "ViT-L-14/openai"
    """Specific model checkpoint."""
    vit_batch_size: int = 1024
    """Batch size for ViT inference."""
    n_workers: int = 8
    """Number of dataloader workers."""
    d_vit: int = 1024
    """Dimension of the ViT activations (depends on model)."""
    vit_layers: list[int] = dataclasses.field(default_factory=lambda: [-2])
    """Which layers to save. By default, the second-to-last layer."""
    n_patches_per_img: int = 256
    """Number of ViT patches per image (depends on model)."""
    cls_token: bool = True
    """Whether the model has a [CLS] token."""
    n_patches_per_shard: int = 2_400_000
    """Number of activations per shard; 2.4M is approximately 10GB for 1024-dimensional 4-byte activations."""

    seed: int = 42
    """Random seed."""
    ssl: bool = True
    """Whether to use SSL."""

    # Hardware
    device: str = "cuda"
    """Which device to use."""
    slurm: bool = False
    """Whether to use `submitit` to run jobs on a Slurm cluster."""
    slurm_acct: str = "PAS2136"
    """Slurm account string."""
    log_to: str = "./logs"
    """Where to log Slurm job stdout/stderr."""


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class DataLoad:
    """
    Configuration for loading activation data from disk.
    """

    shard_root: str = os.path.join(".", "shards")
    """Directory with .bin shards and a metadata.json file."""
    patches: typing.Literal["cls", "patches", "meanpool"] = "patches"
    """Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'patches' indicates it will return all patches. 'meanpool' returns the mean of all image patches."""
    layer: int | typing.Literal["all", "meanpool"] = -2
    """.. todo: document this field."""
    clamp: float = 1e5
    """Maximum value for activations; activations will be clamped to within [-clamp, clamp]`."""
    n_random_samples: int = 2**19
    """Number of random samples used to calculate approximate dataset means at startup."""
    scale_mean: bool | str = True
    """Whether to subtract approximate dataset means from examples. If a string, manually load from the filepath."""
    scale_norm: bool | str = True
    """Whether to scale average dataset norm to sqrt(d_vit). If a string, manually load from the filepath."""


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class SparseAutoencoder:
    d_vit: int = 1024
    exp_factor: int = 16
    """Expansion factor for SAE."""
    sparsity_coeff: float = 4e-4
    """How much to weight sparsity loss term."""
    n_reinit_samples: int = 1024 * 16 * 32
    """Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean."""
    ghost_grads: bool = False
    """Whether to use ghost grads."""
    remove_parallel_grads: bool = True
    """Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic."""
    normalize_w_dec: bool = True
    """Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation."""
    seed: int = 0
    """Random seed."""

    @property
    def d_sae(self) -> int:
        return self.d_vit * self.exp_factor


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Train:
    """
    Configuration for training a sparse autoencoder on a vision transformer.
    """

    data: DataLoad = dataclasses.field(default_factory=DataLoad)
    """Data configuration"""
    n_workers: int = 32
    """Number of dataloader workers."""
    n_patches: int = 100_000_000
    """Number of SAE training examples."""
    sae: SparseAutoencoder = dataclasses.field(default_factory=SparseAutoencoder)
    """SAE configuration."""
    n_sparsity_warmup: int = 0
    """Number of sparsity coefficient warmup steps."""
    lr: float = 0.0004
    """Learning rate."""
    n_lr_warmup: int = 500
    """Number of learning rate warmup steps."""
    sae_batch_size: int = 1024 * 16
    """Batch size for SAE training."""

    # Logging
    track: bool = True
    """Whether to track with WandB."""
    wandb_project: str = "saev"
    """WandB project name."""
    tag: str = ""
    """Tag to add to WandB run."""
    log_every: int = 25
    """How often to log to WandB."""
    ckpt_path: str = os.path.join(".", "checkpoints")
    """Where to save checkpoints."""

    device: typing.Literal["cuda", "cpu"] = "cuda"
    """Hardware device."""
    seed: int = 42
    """Random seed."""
    slurm: bool = False
    """Whether to use `submitit` to run jobs on a Slurm cluster."""
    slurm_acct: str = "PAS2136"
    """Slurm account string."""
    log_to: str = os.path.join(".", "logs")
    """Where to log Slurm job stdout/stderr."""


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Visuals:
    """Configuration for generating visuals from trained SAEs."""

    ckpt: str = os.path.join(".", "checkpoints", "sae.pt")
    """Path to the sae.pt file."""
    data: DataLoad = dataclasses.field(default_factory=DataLoad)
    """Data configuration."""
    images: DatasetConfig = dataclasses.field(default_factory=ImagenetDataset)
    """Which images to use."""
    top_k: int = 128
    """How many images per SAE feature to store."""
    n_workers: int = 16
    """Number of dataloader workers."""
    topk_batch_size: int = 1024 * 16
    """Number of examples to apply top-k op to."""
    sae_batch_size: int = 1024 * 16
    """Batch size for SAE inference."""
    epsilon: float = 1e-9
    """Value to add to avoid log(0)."""
    sort_by: typing.Literal["cls", "img", "patch"] = "patch"
    """How to find the top k images. 'cls' picks images where the SAE latents of the ViT's [CLS] token are maximized without any patch highligting. 'img' picks images that maximize the sum of an SAE latent over all patches in the image, highlighting the patches. 'patch' pickes images that maximize an SAE latent over all patches (not summed), highlighting the patches and only showing unique images."""
    device: str = "cuda"
    """Which accelerator to use."""
    dump_to: str = os.path.join(".", "data")
    """Where to save data."""
    log_freq_range: tuple[float, float] = (-6.0, -2.0)
    """Log10 frequency range for which to save images."""
    log_value_range: tuple[float, float] = (-1.0, 1.0)
    """Log10 frequency range for which to save images."""
    include_latents: list[int] = dataclasses.field(default_factory=list)
    """Latents to always include, no matter what."""
    n_distributions: int = 25
    """Number of features to save distributions for."""
    percentile: int = 99
    """Percentile to estimate for outlier detection."""
    n_latents: int = 400
    """Maximum number of latents to save images for."""
    seed: int = 42
    """Random seed."""

    @property
    def root(self) -> str:
        return os.path.join(self.dump_to, f"sort_by_{self.sort_by}")

    @property
    def top_values_fpath(self) -> str:
        return os.path.join(self.root, "top_values.pt")

    @property
    def top_img_i_fpath(self) -> str:
        return os.path.join(self.root, "top_img_i.pt")

    @property
    def top_patch_i_fpath(self) -> str:
        return os.path.join(self.root, "top_patch_i.pt")

    @property
    def mean_values_fpath(self) -> str:
        return os.path.join(self.root, "mean_values.pt")

    @property
    def sparsity_fpath(self) -> str:
        return os.path.join(self.root, "sparsity.pt")

    @property
    def distributions_fpath(self) -> str:
        return os.path.join(self.root, "distributions.pt")

    @property
    def percentiles_fpath(self) -> str:
        return os.path.join(self.root, f"percentiles_p{self.percentile}.pt")


##########
# SWEEPS #
##########


@beartype.beartype
def grid(cfg: Train, sweep_dct: dict[str, object]) -> tuple[list[Train], list[str]]:
    cfgs, errs = [], []
    for d, dct in enumerate(expand(sweep_dct)):
        # .sae is a nested field that cannot be naively expanded.
        sae_dct = dct.pop("sae")
        if sae_dct:
            sae_dct["seed"] = sae_dct.pop("seed", cfg.sae.seed) + cfg.seed + d
            dct["sae"] = dataclasses.replace(cfg.sae, **sae_dct)

        # .data is a nested field that cannot be naively expanded.
        data_dct = dct.pop("data")
        if data_dct:
            dct["data"] = dataclasses.replace(cfg.data, **data_dct)

        try:
            cfgs.append(dataclasses.replace(cfg, **dct, seed=cfg.seed + d))
        except Exception as err:
            errs.append(str(err))

    return cfgs, errs


@beartype.beartype
def expand(config: dict[str, object]) -> collections.abc.Iterator[dict[str, object]]:
    """
    Expands dicts with (nested) lists into a list of (nested) dicts.
    """
    yield from _expand_discrete(config)


@beartype.beartype
def _expand_discrete(
    config: dict[str, object],
) -> collections.abc.Iterator[dict[str, object]]:
    """
    Expands any (possibly nested) list values in `config`
    """
    if not config:
        yield config
        return

    key, value = config.popitem()

    if isinstance(value, list):
        # Expand
        for c in _expand_discrete(config):
            for v in value:
                yield {**c, key: v}
    elif isinstance(value, dict):
        # Expand
        for c, v in itertools.product(
            _expand_discrete(config), _expand_discrete(value)
        ):
            yield {**c, key: v}
    else:
        for c in _expand_discrete(config):
            yield {**c, key: value}

```

# saev/activations.py

```python
"""
To save lots of activations, we want to do things in parallel, with lots of slurm jobs, and save multiple files, rather than just one.

This module handles that additional complexity.

Conceptually, activations are either thought of as

1. A single [n_imgs x n_layers x (n_patches + 1), d_vit] tensor. This is a *dataset*
2. Multiple [n_imgs_per_shard, n_layers, (n_patches + 1), d_vit] tensors. This is a set of sharded activations.
"""

import dataclasses
import hashlib
import json
import logging
import math
import os
import typing
from collections.abc import Callable

import beartype
import numpy as np
import torch
import torchvision.datasets
from jaxtyping import Float, jaxtyped
from PIL import Image
from torch import Tensor

from . import config, helpers

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger(__name__)


#######################
# VISION TRANSFORMERS #
#######################


@jaxtyped(typechecker=beartype.beartype)
class RecordedVisionTransformer(torch.nn.Module):
    _storage: Float[Tensor, "batch n_layers all_patches dim"] | None
    _i: int

    def __init__(
        self,
        vit: torch.nn.Module,
        n_patches_per_img: int,
        cls_token: bool,
        layers: list[int],
    ):
        super().__init__()

        self.vit = vit

        self.n_patches_per_img = n_patches_per_img
        self.cls_token = cls_token
        self.layers = layers

        self.patches = vit.get_patches(n_patches_per_img)

        self._storage = None
        self._i = 0

        self.logger = logging.getLogger(f"recorder({vit.name})")

        for i in self.layers:
            self.vit.get_residuals()[i].register_forward_hook(self.hook)

    def hook(
        self, module, args: tuple, output: Float[Tensor, "batch n_layers dim"]
    ) -> None:
        if self._storage is None:
            batch, _, dim = output.shape
            self._storage = self._empty_storage(batch, dim, output.device)

        if self._storage[:, self._i, 0, :].shape != output[:, 0, :].shape:
            batch, _, dim = output.shape

            old_batch, _, _, old_dim = self._storage.shape
            msg = "Output shape does not match storage shape: (batch) %d != %d or (dim) %d != %d"
            self.logger.warning(msg, old_batch, batch, old_dim, dim)

            self._storage = self._empty_storage(batch, dim, output.device)

        self._storage[:, self._i] = output[:, self.patches, :].detach()
        self._i += 1

    def _empty_storage(self, batch: int, dim: int, device: torch.device):
        n_patches_per_img = self.n_patches_per_img
        if self.cls_token:
            n_patches_per_img += 1

        return torch.zeros(
            (batch, len(self.layers), n_patches_per_img, dim), device=device
        )

    def reset(self):
        self._i = 0

    @property
    def activations(self) -> Float[Tensor, "batch n_layers all_patches dim"]:
        if self._storage is None:
            raise RuntimeError("First call forward()")
        return self._storage.cpu()

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> tuple[
        Float[Tensor, "batch patches dim"],
        Float[Tensor, "batch n_layers all_patches dim"],
    ]:
        self.reset()
        result = self.vit(batch)
        return result, self.activations


@jaxtyped(typechecker=beartype.beartype)
class Clip(torch.nn.Module):
    def __init__(self, vit_ckpt: str):
        super().__init__()

        import open_clip

        if vit_ckpt.startswith("hf-hub:"):
            clip, _ = open_clip.create_model_from_pretrained(
                vit_ckpt, cache_dir=helpers.get_cache_dir()
            )
        else:
            arch, ckpt = vit_ckpt.split("/")
            clip, _ = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()
            )

        model = clip.visual
        model.proj = None
        model.output_tokens = True  # type: ignore
        self.model = model.eval()

        assert not isinstance(self.model, open_clip.timm_model.TimmModel)

        self.name = f"clip/{vit_ckpt}"

    def get_residuals(self) -> list[torch.nn.Module]:
        return self.model.transformer.resblocks

    def get_patches(self, cfg: config.Activations) -> slice:
        return slice(None, None, None)

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches dim"]:
        cls, patches = self.model(batch)
        return {"cls": cls, "patches": patches}


@jaxtyped(typechecker=beartype.beartype)
class Siglip(torch.nn.Module):
    def __init__(self, vit_ckpt: str):
        super().__init__()

        import open_clip

        if vit_ckpt.startswith("hf-hub:"):
            clip, _ = open_clip.create_model_from_pretrained(
                vit_ckpt, cache_dir=helpers.get_cache_dir()
            )
        else:
            arch, ckpt = vit_ckpt.split("/")
            clip, _ = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()
            )

        model = clip.visual
        model.proj = None
        model.output_tokens = True  # type: ignore
        self.model = model

        assert isinstance(self.model, open_clip.timm_model.TimmModel)

    def get_residuals(self) -> list[torch.nn.Module]:
        return self.model.trunk.blocks

    def get_patches(self, cfg: config.Activations) -> slice:
        return slice(None, None, None)

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches dim"]:
        result = self.model(batch)
        return result


@jaxtyped(typechecker=beartype.beartype)
class DinoV2(torch.nn.Module):
    def __init__(self, vit_ckpt: str):
        super().__init__()

        self.model = torch.hub.load("facebookresearch/dinov2", vit_ckpt)
        self.name = f"dinov2/{vit_ckpt}"

    def get_residuals(self) -> list[torch.nn.Module]:
        return self.model.blocks

    def get_patches(self, n_patches_per_img: int) -> slice:
        n_reg = self.model.num_register_tokens
        patches = torch.cat((
            torch.tensor([0]),  # CLS token
            torch.arange(n_reg + 1, n_reg + 1 + n_patches_per_img),  # patches
        ))
        return patches

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches dim"]:
        dct = self.model.forward_features(batch)

        features = torch.cat(
            (dct["x_norm_clstoken"][:, None, :], dct["x_norm_patchtokens"]), axis=1
        )
        return features


@jaxtyped(typechecker=beartype.beartype)
class Moondream2(torch.nn.Module):
    """
    Moondream2 has 14x14 pixel patches. For a 378x378 image (as we use here), this is 27x27 patches for a total of 729, with no [CLS] token.
    """

    def __init__(self, vit_ckpt: str):
        super().__init__()

        import transformers

        vit_id, revision = vit_ckpt.split(":")

        mllm = transformers.AutoModelForCausalLM.from_pretrained(
            vit_id, revision=revision, trust_remote_code=True
        )
        self.model = mllm.vision_encoder.encoder.model.visual

    def get_patches(self, cfg: config.Activations) -> slice:
        return slice(None, None, None)

    def get_residuals(self) -> list[torch.nn.Module]:
        return self.model.blocks

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches dim"]:
        features = self.model(batch)
        return features


@beartype.beartype
def make_vit(vit_family: str, vit_ckpt: str):
    if vit_family == "clip":
        return Clip(vit_ckpt)
    elif vit_family == "siglip":
        return Siglip(vit_ckpt)
    elif vit_family == "dinov2":
        return DinoV2(vit_ckpt)
    elif vit_family == "moondream2":
        return Moondream2(vit_ckpt)
    else:
        typing.assert_never(vit_family)


@beartype.beartype
def make_img_transform(vit_family: str, vit_ckpt: str) -> Callable:
    if vit_family == "clip" or vit_family == "siglip":
        import open_clip

        if vit_ckpt.startswith("hf-hub:"):
            _, img_transform = open_clip.create_model_from_pretrained(
                vit_ckpt, cache_dir=helpers.get_cache_dir()
            )
        else:
            arch, ckpt = vit_ckpt.split("/")
            _, img_transform = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()
            )
        return img_transform

    elif vit_family == "dinov2":
        from torchvision.transforms import v2

        return v2.Compose([
            # TODO: I bet this should be 256, 256, which is causing localization issues in non-square images.
            v2.Resize(size=256),
            v2.CenterCrop(size=(224, 224)),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),
        ])

    elif vit_family == "moondream2":
        from torchvision.transforms import v2

        # Assume fixed image ratio, 378x378
        return v2.Compose([
            v2.Resize(size=(378, 378), interpolation=v2.InterpolationMode.BICUBIC),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        ])
    else:
        typing.assert_never(vit_family)


###############
# ACTIVATIONS #
###############


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    """
    Dataset of activations from disk.
    """

    class Example(typing.TypedDict):
        """Individual example."""

        act: Float[Tensor, " d_vit"]
        image_i: int
        patch_i: int

    cfg: config.DataLoad
    """Configuration; set via CLI args."""
    metadata: "Metadata"
    """Activations metadata; automatically loaded from disk."""
    layer_index: int
    """Layer index into the shards if we are choosing a specific layer."""
    scalar: float
    """Normalizing scalar such that ||x / scalar ||_2 ~= sqrt(d_vit)."""
    act_mean: Float[Tensor, " d_vit"]
    """Mean activation."""

    def __init__(self, cfg: config.DataLoad):
        self.cfg = cfg
        if not os.path.isdir(self.cfg.shard_root):
            raise RuntimeError(f"Activations are not saved at '{self.cfg.shard_root}'.")

        metadata_fpath = os.path.join(self.cfg.shard_root, "metadata.json")
        self.metadata = Metadata.load(metadata_fpath)

        # Pick a really big number so that if you accidentally use this when you shouldn't, you get an out of bounds IndexError.
        self.layer_index = 1_000_000
        if isinstance(self.cfg.layer, int):
            err_msg = f"Non-exact matches for .layer field not supported; {self.cfg.layer} not in {self.metadata.layers}."
            assert self.cfg.layer in self.metadata.layers, err_msg
            self.layer_index = self.metadata.layers.index(self.cfg.layer)

        # Premptively set these values so that preprocess() doesn't freak out.
        self.scalar = 1.0
        self.act_mean = torch.zeros(self.d_vit)

        # If either of these are true, we must do this work.
        if self.cfg.scale_mean is True or self.cfg.scale_norm is True:
            # Load a random subset of samples to calculate the mean activation and mean L2 norm.
            perm = np.random.default_rng(seed=42).permutation(len(self))
            perm = perm[: cfg.n_random_samples]

            samples = [
                self[p.item()]
                for p in helpers.progress(
                    perm, every=25_000, desc="examples to calc means"
                )
            ]
            samples = torch.stack([sample["act"] for sample in samples])
            if samples.abs().max() > 1e3:
                raise ValueError(
                    "You found an abnormally large activation {example.abs().max().item():.5f} that will mess up your L2 mean."
                )

            # Activation mean
            if self.cfg.scale_mean:
                self.act_mean = samples.mean(axis=0)
                if (self.act_mean > 1e3).any():
                    raise ValueError(
                        "You found an abnormally large activation that is messing up your activation mean."
                    )

            # Norm
            if self.cfg.scale_norm:
                l2_mean = torch.linalg.norm(samples - self.act_mean, axis=1).mean()
                if l2_mean > 1e3:
                    raise ValueError(
                        "You found an abnormally large activation that is messing up your L2 mean."
                    )

                self.scalar = l2_mean / math.sqrt(self.d_vit)
        elif isinstance(self.cfg.scale_mean, str):
            # Load mean activations from disk
            self.act_mean = torch.load(self.cfg.scale_mean)
        elif isinstance(self.cfg.scale_norm, str):
            # Load scalar normalization from disk
            self.scalar = torch.load(self.cfg.scale_norm).item()

    def transform(self, act: Float[np.ndarray, " d_vit"]) -> Float[Tensor, " d_vit"]:
        """
        Apply a scalar normalization so the mean squared L2 norm is same as d_vit. This is from 'Scaling Monosemanticity':

        > As a preprocessing step we apply a scalar normalization to the model activations so their average squared L2 norm is the residual stream dimension

        So we divide by self.scalar which is the datasets (approximate) L2 mean before normalization divided by sqrt(d_vit).
        """
        act = torch.from_numpy(act.copy())
        act = act.clamp(-self.cfg.clamp, self.cfg.clamp)
        return (act - self.act_mean) / self.scalar

    @property
    def d_vit(self) -> int:
        """Dimension of the underlying vision transformer's embedding space."""
        return self.metadata.d_vit

    @jaxtyped(typechecker=beartype.beartype)
    def __getitem__(self, i: int) -> Example:
        match (self.cfg.patches, self.cfg.layer):
            case ("cls", int()):
                img_act = self.get_img_patches(i)
                # Select layer's cls token.
                act = img_act[self.layer_index, 0, :]
                return self.Example(act=self.transform(act), image_i=i, patch_i=-1)
            case ("cls", "meanpool"):
                img_act = self.get_img_patches(i)
                # Select cls tokens from across all layers
                cls_act = img_act[:, 0, :]
                # Meanpool over the layers
                act = cls_act.mean(axis=0)
                return self.Example(act=self.transform(act), image_i=i, patch_i=-1)
            case ("meanpool", int()):
                img_act = self.get_img_patches(i)
                # Select layer's patches.
                layer_act = img_act[self.layer_index, 1:, :]
                # Meanpool over the patches
                act = layer_act.mean(axis=0)
                return self.Example(act=self.transform(act), image_i=i, patch_i=-1)
            case ("meanpool", "meanpool"):
                img_act = self.get_img_patches(i)
                # Select all layer's patches.
                act = img_act[:, 1:, :]
                # Meanpool over the layers and patches
                act = act.mean(axis=(0, 1))
                return self.Example(act=self.transform(act), image_i=i, patch_i=-1)
            case ("patches", int()):
                n_imgs_per_shard = (
                    self.metadata.n_patches_per_shard
                    // len(self.metadata.layers)
                    // (self.metadata.n_patches_per_img + 1)
                )
                n_examples_per_shard = (
                    n_imgs_per_shard * self.metadata.n_patches_per_img
                )

                shard = i // n_examples_per_shard
                pos = i % n_examples_per_shard

                acts_fpath = os.path.join(self.cfg.shard_root, f"acts{shard:06}.bin")
                shape = (
                    n_imgs_per_shard,
                    len(self.metadata.layers),
                    self.metadata.n_patches_per_img + 1,
                    self.metadata.d_vit,
                )
                acts = np.memmap(acts_fpath, mode="c", dtype=np.float32, shape=shape)
                # Choose the layer and the non-CLS tokens.
                acts = acts[:, self.layer_index, 1:]

                # Choose a patch among n and the patches.
                act = acts[
                    pos // self.metadata.n_patches_per_img,
                    pos % self.metadata.n_patches_per_img,
                ]
                return self.Example(
                    act=self.transform(act),
                    # What image is this?
                    image_i=i // self.metadata.n_patches_per_img,
                    patch_i=i % self.metadata.n_patches_per_img,
                )
            case _:
                print((self.cfg.patches, self.cfg.layer))
                typing.assert_never((self.cfg.patches, self.cfg.layer))

    def get_shard_patches(self):
        raise NotImplementedError()

    def get_img_patches(
        self, i: int
    ) -> Float[np.ndarray, "n_layers all_patches d_vit"]:
        n_imgs_per_shard = (
            self.metadata.n_patches_per_shard
            // len(self.metadata.layers)
            // (self.metadata.n_patches_per_img + 1)
        )
        shard = i // n_imgs_per_shard
        pos = i % n_imgs_per_shard
        acts_fpath = os.path.join(self.cfg.shard_root, f"acts{shard:06}.bin")
        shape = (
            n_imgs_per_shard,
            len(self.metadata.layers),
            self.metadata.n_patches_per_img + 1,
            self.metadata.d_vit,
        )
        acts = np.memmap(acts_fpath, mode="c", dtype=np.float32, shape=shape)
        # Note that this is not yet copied!
        return acts[pos]

    def __len__(self) -> int:
        """
        Dataset length depends on `patches` and `layer`.
        """
        match (self.cfg.patches, self.cfg.layer):
            case ("cls", "all"):
                # Return a CLS token from a random image and random layer.
                return self.metadata.n_imgs * len(self.metadata.layers)
            case ("cls", int()):
                # Return a CLS token from a random image and fixed layer.
                return self.metadata.n_imgs
            case ("cls", "meanpool"):
                # Return a CLS token from a random image and meanpool over all layers.
                return self.metadata.n_imgs
            case ("meanpool", "all"):
                # Return the meanpool of all patches from a random image and random layer.
                return self.metadata.n_imgs * len(self.metadata.layers)
            case ("meanpool", int()):
                # Return the meanpool of all patches from a random image and fixed layer.
                return self.metadata.n_imgs
            case ("meanpool", "meanpool"):
                # Return the meanpool of all patches from a random image and meanpool over all layers.
                return self.metadata.n_imgs
            case ("patches", int()):
                # Return a patch from a random image, fixed layer, and random patch.
                return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
            case ("patches", "meanpool"):
                # Return a patch from a random image, meanpooled over all layers, and a random patch.
                return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
            case ("patches", "all"):
                # Return a patch from a random image, random layer and random patch.
                return (
                    self.metadata.n_imgs
                    * len(self.metadata.layers)
                    * self.metadata.n_patches_per_img
                )
            case _:
                typing.assert_never((self.cfg.patches, self.cfg.layer))


##########
# IMAGES #
##########


@beartype.beartype
def setup(cfg: config.Activations):
    """
    Run dataset-specific setup. These setup functions can assume they are the only job running, but they should be idempotent; they should be safe (and ideally cheap) to run multiple times in a row.
    """
    if isinstance(cfg.data, config.ImagenetDataset):
        setup_imagenet(cfg)
    elif isinstance(cfg.data, config.ImageFolderDataset):
        setup_imagefolder(cfg)
    elif isinstance(cfg.data, config.Ade20kDataset):
        setup_ade20k(cfg)
    else:
        typing.assert_never(cfg.data)


@beartype.beartype
def setup_imagenet(cfg: config.Activations):
    assert isinstance(cfg.data, config.ImagenetDataset)


@beartype.beartype
def setup_imagefolder(cfg: config.Activations):
    assert isinstance(cfg.data, config.ImageFolderDataset)
    logger.info("No dataset-specific setup for ImageFolder.")


@beartype.beartype
def setup_ade20k(cfg: config.Activations):
    assert isinstance(cfg.data, config.Ade20kDataset)

    # url = "http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip"
    # breakpoint()

    # 1. Check


@beartype.beartype
def get_dataset(cfg: config.DatasetConfig, *, img_transform):
    """
    Gets the dataset for the current experiment; delegates construction to dataset-specific functions.

    Args:
        cfg: Experiment config.
        img_transform: Image transform to be applied to each image.

    Returns:
        A dataset that has dictionaries with `'image'`, `'index'`, `'target'`, and `'label'` keys containing examples.
    """
    if isinstance(cfg, config.ImagenetDataset):
        return Imagenet(cfg, img_transform=img_transform)
    elif isinstance(cfg, config.Ade20kDataset):
        return Ade20k(cfg, img_transform=img_transform)
    elif isinstance(cfg, config.ImageFolderDataset):
        return ImageFolder(cfg.root, transform=img_transform)
    else:
        typing.assert_never(cfg)


@beartype.beartype
def get_dataloader(cfg: config.Activations, *, img_transform=None):
    """
    Gets the dataloader for the current experiment; delegates dataloader construction to dataset-specific functions.

    Args:
        cfg: Experiment config.
        img_transform: Image transform to be applied to each image.

    Returns:
        A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches.
    """
    if isinstance(
        cfg.data,
        (config.ImagenetDataset, config.ImageFolderDataset, config.Ade20kDataset),
    ):
        dataloader = get_default_dataloader(cfg, img_transform=img_transform)
    else:
        typing.assert_never(cfg.data)

    return dataloader


@beartype.beartype
def get_default_dataloader(
    cfg: config.Activations, *, img_transform: Callable
) -> torch.utils.data.DataLoader:
    """
    Get a dataloader for a default map-style dataset.

    Args:
        cfg: Config.
        img_transform: Image transform to be applied to each image.

    Returns:
        A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches, `'index'` keys containing original dataset indices and `'label'` keys containing label batches.
    """
    dataset = get_dataset(cfg.data, img_transform=img_transform)

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=cfg.vit_batch_size,
        drop_last=False,
        num_workers=cfg.n_workers,
        persistent_workers=cfg.n_workers > 0,
        shuffle=False,
        pin_memory=False,
    )
    return dataloader


@beartype.beartype
class Imagenet(torch.utils.data.Dataset):
    def __init__(self, cfg: config.ImagenetDataset, *, img_transform=None):
        import datasets

        self.hf_dataset = datasets.load_dataset(
            cfg.name, split=cfg.split, trust_remote_code=True
        )

        self.img_transform = img_transform
        self.labels = self.hf_dataset.info.features["label"].names

    def __getitem__(self, i):
        example = self.hf_dataset[i]
        example["index"] = i

        example["image"] = example["image"].convert("RGB")
        if self.img_transform:
            example["image"] = self.img_transform(example["image"])
        example["target"] = example.pop("label")
        example["label"] = self.labels[example["target"]]

        return example

    def __len__(self) -> int:
        return len(self.hf_dataset)


@beartype.beartype
class ImageFolder(torchvision.datasets.ImageFolder):
    def __getitem__(self, index: int) -> dict[str, object]:
        """
        Args:
            index: Index

        Returns:
            dict with keys 'image', 'index', 'target' and 'label'.
        """
        path, target = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return {
            "image": sample,
            "target": target,
            "label": self.classes[target],
            "index": index,
        }


@beartype.beartype
class Ade20k(torch.utils.data.Dataset):
    @beartype.beartype
    @dataclasses.dataclass(frozen=True)
    class Sample:
        img_path: str
        seg_path: str
        label: str
        target: int

    samples: list[Sample]

    def __init__(
        self,
        cfg: config.Ade20kDataset,
        *,
        img_transform: Callable | None = None,
        seg_transform: Callable | None = lambda x: None,
    ):
        self.logger = logging.getLogger("ade20k")
        self.cfg = cfg
        self.img_dir = os.path.join(cfg.root, "images")
        self.seg_dir = os.path.join(cfg.root, "annotations")
        self.img_transform = img_transform
        self.seg_transform = seg_transform

        # Check that we have the right path.
        for subdir in ("images", "annotations"):
            if not os.path.isdir(os.path.join(cfg.root, subdir)):
                # Something is missing.
                if os.path.realpath(cfg.root).endswith(subdir):
                    self.logger.warning(
                        "The ADE20K root should contain 'images/' and 'annotations/' directories."
                    )
                raise ValueError(f"Can't find path '{os.path.join(cfg.root, subdir)}'.")

        _, split_mapping = torchvision.datasets.folder.find_classes(self.img_dir)
        split_lookup: dict[int, str] = {
            value: key for key, value in split_mapping.items()
        }
        self.loader = torchvision.datasets.folder.default_loader

        assert cfg.split in set(split_lookup.values())

        # Load all the image paths.
        imgs: list[str] = [
            path
            for path, s in torchvision.datasets.folder.make_dataset(
                self.img_dir,
                split_mapping,
                extensions=torchvision.datasets.folder.IMG_EXTENSIONS,
            )
            if split_lookup[s] == cfg.split
        ]

        segs: list[str] = [
            path
            for path, s in torchvision.datasets.folder.make_dataset(
                self.seg_dir,
                split_mapping,
                extensions=torchvision.datasets.folder.IMG_EXTENSIONS,
            )
            if split_lookup[s] == cfg.split
        ]

        # Load all the targets, classes and mappings
        with open(os.path.join(cfg.root, "sceneCategories.txt")) as fd:
            img_labels: list[str] = [line.split()[1] for line in fd.readlines()]

        label_set = sorted(set(img_labels))
        label_to_idx = {label: i for i, label in enumerate(label_set)}

        self.samples = [
            self.Sample(img_path, seg_path, label, label_to_idx[label])
            for img_path, seg_path, label in zip(imgs, segs, img_labels)
        ]

    def __getitem__(self, index: int) -> dict[str, object]:
        # Convert to dict.
        sample = dataclasses.asdict(self.samples[index])

        sample["image"] = self.loader(sample.pop("img_path"))
        if self.img_transform is not None:
            image = self.img_transform(sample.pop("image"))
            if image is not None:
                sample["image"] = image

        sample["segmentation"] = Image.open(sample.pop("seg_path")).convert("L")
        if self.seg_transform is not None:
            segmentation = self.seg_transform(sample.pop("segmentation"))
            if segmentation is not None:
                sample["segmentation"] = segmentation

        sample["index"] = index

        return sample

    def __len__(self) -> int:
        return len(self.samples)


########
# MAIN #
########


@beartype.beartype
def main(cfg: config.Activations):
    """
    Args:
        cfg: Config for activations.
    """
    logger = logging.getLogger("dump")

    if not cfg.ssl:
        logger.warning("Ignoring SSL certs. Try not to do this!")
        # https://github.com/openai/whisper/discussions/734#discussioncomment-4491761
        # Ideally we don't have to disable SSL but we are only downloading weights.
        import ssl

        ssl._create_default_https_context = ssl._create_unverified_context

    # Run any setup steps.
    setup(cfg)

    # Actually record activations.
    if cfg.slurm:
        import submitit

        executor = submitit.SlurmExecutor(folder=cfg.log_to)
        executor.update_parameters(
            time=24 * 60,
            partition="gpu",
            gpus_per_node=1,
            cpus_per_task=cfg.n_workers + 4,
            stderr_to_stdout=True,
            account=cfg.slurm_acct,
        )

        job = executor.submit(worker_fn, cfg)
        logger.info("Running job '%s'.", job.job_id)
        job.result()

    else:
        worker_fn(cfg)


@beartype.beartype
def worker_fn(cfg: config.Activations):
    """
    Args:
        cfg: Config for activations.
    """

    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than
        # float16 and almost as accurate as float32
        # This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    logger = logging.getLogger("dump")

    vit = make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)
    vit = RecordedVisionTransformer(
        vit, cfg.n_patches_per_img, cfg.cls_token, cfg.vit_layers
    )
    img_transform = make_img_transform(cfg.vit_family, cfg.vit_ckpt)
    dataloader = get_dataloader(cfg, img_transform=img_transform)

    writer = ShardWriter(cfg)

    n_batches = cfg.data.n_imgs // cfg.vit_batch_size + 1
    logger.info("Dumping %d batches of %d examples.", n_batches, cfg.vit_batch_size)

    if cfg.device == "cuda" and not torch.cuda.is_available():
        logger.warning("No CUDA device available, using CPU.")
        cfg = dataclasses.replace(cfg, device="cpu")

    vit = vit.to(cfg.device)
    # vit = torch.compile(vit)

    i = 0
    # Calculate and write ViT activations.
    with torch.inference_mode():
        for batch in helpers.progress(dataloader, total=n_batches):
            images = batch.pop("image").to(cfg.device)
            # cache has shape [batch size, n layers, n patches + 1, d vit]
            out, cache = vit(images)
            del out

            writer[i : i + len(cache)] = cache
            i += len(cache)

    writer.flush()


@beartype.beartype
class ShardWriter:
    """
    ShardWriter is a stateful object that handles sharded activation writing to disk.
    """

    root: str
    shape: tuple[int, int, int, int]
    shard: int
    acts_path: str
    acts: Float[np.ndarray, "n_imgs_per_shard n_layers all_patches d_vit"] | None
    filled: int

    def __init__(self, cfg: config.Activations):
        self.logger = logging.getLogger("shard-writer")

        self.root = get_acts_dir(cfg)

        n_patches_per_img = cfg.n_patches_per_img
        if cfg.cls_token:
            n_patches_per_img += 1
        self.n_imgs_per_shard = (
            cfg.n_patches_per_shard // len(cfg.vit_layers) // n_patches_per_img
        )
        self.shape = (
            self.n_imgs_per_shard,
            len(cfg.vit_layers),
            n_patches_per_img,
            cfg.d_vit,
        )

        self.shard = -1
        self.acts = None
        self.next_shard()

    @jaxtyped(typechecker=beartype.beartype)
    def __setitem__(
        self, i: slice, val: Float[Tensor, "_ n_layers all_patches d_vit"]
    ) -> None:
        assert i.step is None
        a, b = i.start, i.stop
        assert len(val) == b - a

        offset = self.n_imgs_per_shard * self.shard

        if b >= offset + self.n_imgs_per_shard:
            # We have run out of space in this mmap'ed file. Let's fill it as much as we can.
            n_fit = offset + self.n_imgs_per_shard - a
            self.acts[a - offset : a - offset + n_fit] = val[:n_fit]
            self.filled = a - offset + n_fit

            self.next_shard()

            # Recursively call __setitem__ in case we need *another* shard
            self[a + n_fit : b] = val[n_fit:]
        else:
            msg = f"0 <= {a} - {offset} <= {offset} + {self.n_imgs_per_shard}"
            assert 0 <= a - offset <= offset + self.n_imgs_per_shard, msg
            msg = f"0 <= {b} - {offset} <= {offset} + {self.n_imgs_per_shard}"
            assert 0 <= b - offset <= offset + self.n_imgs_per_shard, msg
            self.acts[a - offset : b - offset] = val
            self.filled = b - offset

    def flush(self) -> None:
        if self.acts is not None:
            self.acts.flush()

        self.acts = None

    def next_shard(self) -> None:
        self.flush()

        self.shard += 1
        self._count = 0
        self.acts_path = os.path.join(self.root, f"acts{self.shard:06}.bin")
        self.acts = np.memmap(
            self.acts_path, mode="w+", dtype=np.float32, shape=self.shape
        )
        self.filled = 0

        self.logger.info("Opened shard '%s'.", self.acts_path)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Metadata:
    vit_family: str
    vit_ckpt: str
    layers: tuple[int, ...]
    n_patches_per_img: int
    cls_token: bool
    d_vit: int
    seed: int
    n_imgs: int
    n_patches_per_shard: int
    data: str

    @classmethod
    def from_cfg(cls, cfg: config.Activations) -> "Metadata":
        return cls(
            cfg.vit_family,
            cfg.vit_ckpt,
            tuple(cfg.vit_layers),
            cfg.n_patches_per_img,
            cfg.cls_token,
            cfg.d_vit,
            cfg.seed,
            cfg.data.n_imgs,
            cfg.n_patches_per_shard,
            str(cfg.data),
        )

    @classmethod
    def load(cls, fpath) -> "Metadata":
        with open(fpath) as fd:
            dct = json.load(fd)
        dct["layers"] = tuple(dct.pop("layers"))
        return cls(**dct)

    def dump(self, fpath):
        with open(fpath, "w") as fd:
            json.dump(dataclasses.asdict(self), fd, indent=4)

    @property
    def hash(self) -> str:
        cfg_str = json.dumps(dataclasses.asdict(self), sort_keys=True)
        return hashlib.sha256(cfg_str.encode("utf-8")).hexdigest()


@beartype.beartype
def get_acts_dir(cfg: config.Activations) -> str:
    """
    Return the activations directory based on the relevant values of a config.
    Also saves a metadata.json file to that directory for human reference.

    Args:
        cfg: Config for experiment.

    Returns:
        Directory to where activations should be dumped/loaded from.
    """
    metadata = Metadata.from_cfg(cfg)

    acts_dir = os.path.join(cfg.dump_to, metadata.hash)
    os.makedirs(acts_dir, exist_ok=True)

    metadata.dump(os.path.join(acts_dir, "metadata.json"))

    return acts_dir

```

# saev/colors.py

```python
# https://coolors.co/palette/001219-005f73-0a9396-94d2bd-e9d8a6-ee9b00-ca6702-bb3e03-ae2012-9b2226

BLACK_HEX = "001219"
BLACK_RGB = (0, 18, 25)
BLACK_RGB01 = tuple(c / 256 for c in BLACK_RGB)


BLUE_HEX = "005f73"
BLUE_RGB = (0, 95, 115)
BLUE_RGB01 = tuple(c / 256 for c in BLUE_RGB)


CYAN_HEX = "0a9396"
CYAN_RGB = (10, 147, 150)
CYAN_RGB01 = tuple(c / 256 for c in CYAN_RGB)

SEA_HEX = "94d2bd"
SEA_RGB = (148, 210, 189)
SEA_RGB01 = tuple(c / 256 for c in SEA_RGB)

CREAM_HEX = "e9d8a6"
CREAM_RGB = (233, 216, 166)
CREAM_RGB01 = tuple(c / 256 for c in CREAM_RGB)

GOLD_HEX = "ee9b00"
GOLD_RGB = (238, 155, 0)
GOLD_RGB01 = tuple(c / 256 for c in GOLD_RGB)

ORANGE_HEX = "ca6702"
ORANGE_RGB = (202, 103, 2)
ORANGE_RGB01 = tuple(c / 256 for c in ORANGE_RGB)

RUST_HEX = "bb3e03"
RUST_RGB = (187, 62, 3)
RUST_RGB01 = tuple(c / 256 for c in RUST_RGB)

SCARLET_HEX = "ae2012"
SCARLET_RGB = (174, 32, 18)
SCARLET_RGB01 = tuple(c / 256 for c in SCARLET_RGB)

RED_HEX = "9b2226"
RED_RGB = (155, 34, 38)
RED_RGB01 = tuple(c / 256 for c in RED_RGB)


ALL_HEX = [
    BLACK_HEX,
    BLUE_HEX,
    CYAN_HEX,
    SEA_HEX,
    CREAM_HEX,
    GOLD_HEX,
    ORANGE_HEX,
    RUST_HEX,
    SCARLET_HEX,
    RED_HEX,
]
ALL_RGB01 = [
    BLACK_RGB01,
    BLUE_RGB01,
    CYAN_RGB01,
    SEA_RGB01,
    CREAM_RGB01,
    GOLD_RGB01,
    ORANGE_RGB01,
    RUST_RGB01,
    SCARLET_RGB01,
    RED_RGB01,
]

```

# saev/test_activations.py

```python
"""
Test that the cached activations are actually correct.
These tests are quite slow
"""

import tempfile

import pytest
import torch

from . import activations, config


@pytest.mark.slow
def test_dataloader_batches():
    cfg = config.Activations(
        vit_ckpt="ViT-B-32/openai",
        d_vit=768,
        vit_layers=[-2, -1],
        n_patches_per_img=49,
        vit_batch_size=8,
    )
    dataloader = activations.get_dataloader(
        cfg, img_transform=activations.make_img_transform(cfg.vit_family, cfg.vit_ckpt)
    )
    batch = next(iter(dataloader))

    assert isinstance(batch, dict)
    assert "image" in batch
    assert "index" in batch

    torch.testing.assert_close(batch["index"], torch.arange(8))
    assert batch["image"].shape == (8, 3, 224, 224)


@pytest.mark.slow
def test_shard_writer_and_dataset_e2e():
    with tempfile.TemporaryDirectory() as tmpdir:
        cfg = config.Activations(
            vit_family="timm",
            vit_ckpt="hf_hub:timm/test_vit3.r160_in1k",
            d_vit=96,
            n_patches_per_img=100,
            vit_layers=[-2, -1],
            vit_batch_size=8,
            n_workers=8,
            dump_to=tmpdir,
        )
        vit = activations.make_vit(cfg.vit_family, cfg.vit_ckpt)
        dataloader = activations.get_dataloader(
            cfg,
            img_transform=activations.make_img_transform(cfg.vit_family, cfg.vit_ckpt),
        )
        writer = activations.ShardWriter(cfg)
        dataset = activations.Dataset(
            config.DataLoad(
                shard_root=activations.get_acts_dir(cfg),
                patches="cls",
                layer=-1,
                scale_mean=False,
                scale_norm=False,
            )
        )

        i = 0
        for b, batch in zip(range(100), dataloader):
            # Don't care about the forward pass.
            _, cache = vit(batch["image"])
            writer[i : i + len(cache)] = cache
            i += len(cache)
            assert cache.shape == (cfg.vit_batch_size, len(cfg.layers), 101, 96)

            acts, _, _ = zip(*[dataset[i.item()] for i in batch["index"]])
            from_dataset = torch.stack(acts)
            torch.testing.assert_close(cache[:, -1, 0], from_dataset)
            print(f"Batch {b} matched.")

```

# saev/app/data.py

```python
import base64
import functools
import logging
import typing

import beartype
import pyvips
import torch
import torchvision.datasets
from PIL import Image

from .. import activations, config

logger = logging.getLogger("app.data")


class VipsImageFolder(torchvision.datasets.ImageFolder):
    """
    Clone of ImageFolder that returns pyvips.Image instead of PIL.Image.Image.
    """

    def __init__(
        self,
        root: str,
        transform: typing.Callable | None = None,
        target_transform: typing.Callable | None = None,
    ):
        super().__init__(
            root,
            transform=transform,
            target_transform=target_transform,
            loader=self._vips_loader,
        )

    @staticmethod
    def _vips_loader(path: str) -> torch.Tensor:
        """Load and convert image to tensor using pyvips."""
        image = pyvips.Image.new_from_file(path, access="random")
        return image

    def __getitem__(self, index: int) -> dict[str, object]:
        """
        Args:
            index: Index

        Returns:
            dict with keys 'image', 'index', 'target' and 'label'.
        """
        path, target = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return {
            "image": sample,
            "target": target,
            "label": self.classes[target],
            "index": index,
        }


@beartype.beartype
class VipsImagenet(activations.Imagenet):
    def __getitem__(self, i):
        example = self.hf_dataset[i]
        example["index"] = i

        example["image"] = example["image"].convert("RGB")
        # Convert to pyvips
        example["image"] = pyvips.Image.new_from_memory(
            example["image"].tobytes(),
            example["image"].width,
            example["image"].height,
            3,  # bands (RGB)
            "uchar",
        )
        if self.img_transform:
            example["image"] = self.img_transform(example["image"])
        example["target"] = example.pop("label")
        example["label"] = self.labels[example["target"]]

        return example


@functools.cache
def get_datasets():
    datasets = {
        "inat21__train_mini": VipsImageFolder(
            root="/research/nfs_su_809/workspace/stevens.994/datasets/inat21/train_mini/"
        ),
        "imagenet__train": VipsImagenet(config.ImagenetDataset()),
    }
    logger.info("Loaded datasets.")
    return datasets


@beartype.beartype
def get_img_v_raw(key: str, i: int) -> tuple[pyvips.Image, str]:
    """
    Get raw image and processed label from dataset.

    Returns:
        Tuple of pyvips.Image and classname.
    """
    dataset = get_datasets()[key]
    sample = dataset[i]
    # iNat21 specific: Remove taxonomy prefix
    label = " ".join(sample["label"].split("_")[1:])
    return sample["image"], label


def to_sized(
    img_v_raw: pyvips.Image, min_px: int, crop_px: tuple[int, int]
) -> pyvips.Image:
    """Convert raw vips image to standard model input size (resize + crop)."""
    # Calculate scale factor to make smallest dimension = min_px
    scale = min_px / min(img_v_raw.width, img_v_raw.height)

    # Resize maintaining aspect ratio
    img_v_raw = img_v_raw.resize(scale)
    assert min(img_v_raw.width, img_v_raw.height) == min_px

    # Calculate crop coordinates to center crop
    left = (img_v_raw.width - crop_px[0]) // 2
    top = (img_v_raw.height - crop_px[1]) // 2

    # Crop to final size
    return img_v_raw.crop(left, top, crop_px[0], crop_px[1])


@beartype.beartype
def pil_to_vips(img_p: Image.Image) -> pyvips.Image:
    """Convert a PIL Image to a pyvips Image."""
    # Convert PIL image to bytes
    img_bytes = img_p.tobytes()
    # Create new pyvips image from memory buffer
    return pyvips.Image.new_from_memory(
        img_bytes,
        img_p.width,
        img_p.height,
        len(img_p.getbands()),  # Number of bands (channels)
        "uchar",  # 8-bit unsigned data
    )


@beartype.beartype
def vips_to_base64(img_v: pyvips.Image) -> str:
    buf = img_v.write_to_buffer(".webp")
    b64 = base64.b64encode(buf)
    s64 = b64.decode("utf8")
    return "data:image/webp;base64," + s64

```

# saev/app/modeling.py

```python
import dataclasses
import pathlib

import beartype

from .. import config


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    """Configuration for a Vision Transformer (ViT) and Sparse Autoencoder (SAE) model pair.

    Stores paths and configuration needed to load and run a specific ViT+SAE combination.
    """

    key: str
    """The lookup key."""

    vit_family: str
    """The family of ViT model, e.g. 'clip' for CLIP models."""

    vit_ckpt: str
    """Checkpoint identifier for the ViT model, either as HuggingFace path or model/checkpoint pair."""

    sae_ckpt: str
    """Identifier for the SAE checkpoint to load."""

    tensor_dpath: pathlib.Path
    """Directory containing precomputed tensors for this model combination."""

    dataset_name: str
    """Which dataset to use."""

    acts_cfg: config.DataLoad
    """Which activations to load for normalizing."""

    @property
    def wrapped_cfg(self) -> config.Activations:
        n_patches = 196
        if self.vit_family == "dinov2":
            n_patches = 256

        return config.Activations(
            vit_family=self.vit_family,
            vit_ckpt=self.vit_ckpt,
            vit_layers=[-2],
            n_patches_per_img=n_patches,
        )


def get_model_lookup() -> dict[str, Config]:
    cfgs = [
        Config(
            "bioclip/inat21",
            "clip",
            "hf-hub:imageomics/bioclip",
            "gpnn7x3p",
            pathlib.Path(
                "/research/nfs_su_809/workspace/stevens.994/saev/features/gpnn7x3p-high-freq/sort_by_patch/"
            ),
            "inat21__train_mini",
            config.DataLoad(
                shard_root="/local/scratch/stevens.994/cache/saev/50149a5a12c70d378dc38f1976d676239839b591cadbfc9af5c84268ac30a868",
                n_random_samples=2**16,
            ),
        ),
        Config(
            "clip/inat21",
            "clip",
            "ViT-B-16/openai",
            "rscsjxgd",
            pathlib.Path(
                "/research/nfs_su_809/workspace/stevens.994/saev/features/rscsjxgd-high-freq/sort_by_patch/"
            ),
            "inat21__train_mini",
            config.DataLoad(
                shard_root="/local/scratch/stevens.994/cache/saev/07aed612e3f70b93ecff46e5a3beea7b8a779f0376dcd3bddf1d5a6ffb4c8f76",
                n_random_samples=2**16,
            ),
        ),
        Config(
            "clip/imagenet",
            "clip",
            "ViT-B-16/openai",
            "usvhngx4",
            pathlib.Path(
                "/research/nfs_su_809/workspace/stevens.994/saev/features/usvhngx4-high-freq/sort_by_patch/"
            ),
            "imagenet__train",
            config.DataLoad(
                shard_root="/local/scratch/stevens.994/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8",
                n_random_samples=2**16,
            ),
        ),
        Config(
            "dinov2/imagenet",
            "dinov2",
            "dinov2_vitb14_reg",
            "oebd6e6i",
            pathlib.Path(
                "/research/nfs_su_809/workspace/stevens.994/saev/features/oebd6e6i/sort_by_patch/"
            ),
            "imagenet__train",
            config.DataLoad(
                shard_root="/local/scratch/stevens.994/cache/saev/724b1b7be995ef7212d64640fec2885737a706a33b8e5a18f7f323223bd43af1",
                n_random_samples=2**16,
            ),
        ),
    ]
    # TODO: figure out how to normalize the activations from the ViT using the same mean/scalar as in the sorted data.
    return {cfg.key: cfg for cfg in cfgs}

```

# saev/app/__main__.py

```python
import base64
import concurrent.futures
import functools
import json
import logging
import math
import pathlib
import time
import typing

import beartype
import einops.layers.torch
import gradio as gr
import matplotlib
import numpy as np
import PIL.Image
import pyvips
import torch
from jaxtyping import Float, Int, jaxtyped
from torch import Tensor

from .. import activations, nn
from . import data, modeling

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger("app")
# Disable pyvips info logging
logging.getLogger("pyvips").setLevel(logging.WARNING)


###########
# Globals #
###########


RESIZE_SIZE = 512
"""Resize shorter size to this size in pixels."""

CROP_SIZE = (448, 448)
"""Crop size in pixels."""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
"""Hardware accelerator, if any."""

CWD = pathlib.Path(".")

MODEL_LOOKUP = modeling.get_model_lookup()

COLORMAP = matplotlib.colormaps.get_cmap("plasma")

logger.info("Set global constants.")


##########
# Models #
##########


@functools.cache
def load_vit(
    model_cfg: modeling.Config,
) -> tuple[
    activations.WrappedVisionTransformer,
    typing.Callable,
    float,
    Float[Tensor, " d_vit"],
]:
    """
    Returns the wrapped ViT, the vit transform, the activation scalar and the activation mean to normalize the activations.
    """
    vit = activations.WrappedVisionTransformer(model_cfg.wrapped_cfg).to(DEVICE).eval()
    vit_transform = activations.make_img_transform(
        model_cfg.vit_family, model_cfg.vit_ckpt
    )
    logger.info("Loaded ViT: %s.", model_cfg.key)

    try:
        # Normalizing constants
        acts_dataset = activations.Dataset(model_cfg.acts_cfg)
        logger.info("Loaded dataset norms: %s.", model_cfg.key)
    except RuntimeError as err:
        logger.warning("Error loading ViT: %s", err)
        return None, None, None, None

    return vit, vit_transform, acts_dataset.scalar.item(), acts_dataset.act_mean


@beartype.beartype
@functools.cache
def load_sae(model_cfg: modeling.Config) -> nn.SparseAutoencoder:
    sae_ckpt_fpath = CWD / "checkpoints" / model_cfg.sae_ckpt / "sae.pt"
    sae = nn.load(sae_ckpt_fpath.as_posix())
    sae.to(DEVICE).eval()
    logger.info("Loaded SAE: %s.", model_cfg.sae_ckpt)
    return sae


############
# Datasets #
############


@beartype.beartype
def load_tensor(path: str | pathlib.Path) -> Tensor:
    return torch.load(path, weights_only=True, map_location="cpu")


@beartype.beartype
@functools.cache
def load_tensors(
    model_cfg: modeling.Config,
) -> tuple[Int[Tensor, "d_sae top_k"], Float[Tensor, "d_sae top_k n_patches"]]:
    top_img_i = load_tensor(model_cfg.tensor_dpath / "top_img_i.pt")
    # TODO: For some reason, the top_values are about 4 times larger.
    top_values = load_tensor(model_cfg.tensor_dpath / "top_values.pt") / 4
    return top_img_i, top_values


@beartype.beartype
def get_image(example_id: str) -> list[str]:
    dataset, split, i_str = example_id.split("__")
    i = int(i_str)
    img_v_raw, label = data.get_img_v_raw(f"{dataset}__{split}", i)
    img_v_sized = data.to_sized(img_v_raw, RESIZE_SIZE, CROP_SIZE)

    return [data.vips_to_base64(img_v_sized), label]


@jaxtyped(typechecker=beartype.beartype)
def add_highlights(
    img_v_sized: pyvips.Image,
    patches: np.ndarray,
    *,
    upper: float | None = None,
    opacity: float = 0.9,
) -> pyvips.Image:
    """Add colored highlights to an image based on patch activation values.

    Overlays a colored highlight on each patch of the image, with intensity proportional
    to the activation value for that patch. Used to visualize which parts of an image
    most strongly activated a particular SAE latent.

    Args:
        img: The base image to highlight
        patches: Array of activation values, one per patch
        upper: Optional maximum value to normalize activations against
        opacity: Opacity of the highlight overlay (0-1)

    Returns:
        A new image with colored highlights overlaid on the original
    """
    if not len(patches):
        return img_v_sized

    # Calculate patch grid dimensions
    grid_w = grid_h = int(math.sqrt(len(patches)))
    assert grid_w * grid_h == len(patches)

    patch_w = img_v_sized.width // grid_w
    patch_h = img_v_sized.height // grid_h
    assert patch_w == patch_h

    patches = np.clip(patches, a_min=0.0, a_max=upper + 1e-9)

    assert upper is not None
    colors = (COLORMAP(patches / (upper + 1e-9))[:, :3] * 256).astype(np.uint8)

    # Create overlay by processing each patch
    overlay = np.zeros((img_v_sized.width, img_v_sized.height, 4), dtype=np.uint8)
    for idx, (val, color) in enumerate(zip(patches, colors)):
        val = val / (upper + 1e-9)

        x = (idx % grid_w) * patch_w
        y = (idx // grid_w) * patch_h

        # Create patch overlay
        patch = np.zeros((patch_w, patch_h, 4), dtype=np.uint8)
        patch[:, :, 0:3] = color
        patch[:, :, 3] = int(256 * val * opacity)
        overlay[y : y + patch_h, x : x + patch_w, :] = patch
    overlay = pyvips.Image.new_from_array(overlay).copy(interpretation="srgb")
    return img_v_sized.addalpha().composite(overlay, "over")


@beartype.beartype
class Example(typing.TypedDict):
    """Represents an example image and its associated label.

    Used to store examples of SAE latent activations for visualization.
    """

    orig_url: str
    """The URL or path to access the original example image."""
    highlighted_url: str
    """The URL or path to access the SAE-highlighted image."""
    label: str
    """The class label or description associated with this example."""
    example_id: str
    """Unique ID to idenfify the original dataset instance."""


@beartype.beartype
class SaeActivation(typing.TypedDict):
    """Represents the activation pattern of a single SAE latent across patches.

    This captures how strongly a particular SAE latent fires on different patches of an input image.
    """

    model_cfg: modeling.Config
    """The model config."""

    latent: int
    """The index of the SAE latent being measured."""

    activations: list[float]
    """The activation values of this latent across different patches. Each value represents how strongly this latent fired on a particular patch."""

    highlighted_url: str
    """The image with the colormaps applied."""

    examples: list[Example]
    """Top examples for this latent."""


@beartype.beartype
def pil_to_vips(pil_img: PIL.Image.Image) -> pyvips.Image:
    # Convert to numpy array
    np_array = np.asarray(pil_img)
    # Handle different formats
    if np_array.ndim == 2:  # Grayscale
        return pyvips.Image.new_from_memory(
            np_array.tobytes(),
            np_array.shape[1],  # width
            np_array.shape[0],  # height
            1,  # bands
            "uchar",
        )
    else:  # RGB/RGBA
        return pyvips.Image.new_from_memory(
            np_array.tobytes(),
            np_array.shape[1],  # width
            np_array.shape[0],  # height
            np_array.shape[2],  # bands
            "uchar",
        )


@beartype.beartype
def vips_to_pil(vips_img: PIL.Image.Image) -> PIL.Image.Image:
    # Convert to numpy array
    np_array = vips_img.numpy()
    # Convert numpy array to PIL Image
    return PIL.Image.fromarray(np_array)


@beartype.beartype
class BufferInfo(typing.NamedTuple):
    buffer: bytes
    width: int
    height: int
    bands: int
    format: object

    @classmethod
    def from_img_v(cls, img_v: pyvips.Image) -> "BufferInfo":
        return cls(
            img_v.write_to_memory(),
            img_v.width,
            img_v.height,
            img_v.bands,
            img_v.format,
        )


@beartype.beartype
def bufferinfo_to_base64(bufferinfo: BufferInfo) -> str:
    img_v = pyvips.Image.new_from_memory(*bufferinfo)
    buf = img_v.write_to_buffer(".webp")
    b64 = base64.b64encode(buf)
    s64 = b64.decode("utf8")
    return "data:image/webp;base64," + s64


@jaxtyped(typechecker=beartype.beartype)
def make_sae_activation(
    model_cfg: modeling.Config,
    latent: int,
    acts: Float[np.ndarray, " n_patches"],
    img_v: pyvips.Image,
    top_img_i: list[int],
    top_values: Float[Tensor, "top_k n_patches"],
    pool: concurrent.futures.Executor,
) -> SaeActivation:
    raw_examples: list[tuple[int, pyvips.Image, Float[np.ndarray, "..."], str]] = []
    seen_i_im = set()
    for i_im, values_p in zip(top_img_i, top_values):
        if i_im in seen_i_im:
            continue

        ex_img_v_raw, ex_label = data.get_img_v_raw(model_cfg.dataset_name, i_im)
        ex_img_v_sized = data.to_sized(ex_img_v_raw, RESIZE_SIZE, CROP_SIZE)
        raw_examples.append((i_im, ex_img_v_sized, values_p.numpy(), ex_label))

        seen_i_im.add(i_im)

        # Only need 4 example images per latent.
        if len(seen_i_im) >= 4:
            break

    upper = top_values.max().item()

    futures = []
    for i_im, ex_img, values_p, ex_label in raw_examples:
        highlighted_img = add_highlights(ex_img, values_p, upper=upper)
        # Submit both conversions to the thread pool
        orig_future = pool.submit(data.vips_to_base64, ex_img)
        highlight_future = pool.submit(data.vips_to_base64, highlighted_img)
        futures.append((i_im, orig_future, highlight_future, ex_label))

    # Wait for all conversions to complete and build examples
    examples = []
    for i_im, orig_future, highlight_future, ex_label in futures:
        example = Example(
            orig_url=orig_future.result(),
            highlighted_url=highlight_future.result(),
            label=ex_label,
            example_id=f"{model_cfg.dataset_name}__{i_im}",
        )
        examples.append(example)

    print(model_cfg.key, latent, top_values.max(), acts.max())

    # Highlight the original image.
    img_sized_v = data.to_sized(img_v, RESIZE_SIZE, CROP_SIZE)
    highlighted_img = add_highlights(img_sized_v, acts, upper=upper)
    highlighted_url = data.vips_to_base64(highlighted_img)

    return SaeActivation(
        model_cfg=model_cfg,
        latent=latent,
        activations=acts.tolist(),
        highlighted_url=highlighted_url,
        examples=examples,
    )


@beartype.beartype
@torch.inference_mode
def get_sae_activations(
    img_p: PIL.Image.Image, latents: dict[str, list[int]]
) -> dict[str, list[SaeActivation]]:
    """
    Args:
        image: Image to get SAE activations for.
        latents: A lookup from model name (string) to a list of latents to report latents for (integers).

    Returns:
        A lookup from model name (string) to a list of SaeActivations, one for each latent in the `latents` argument.
    """
    logger.info("latents: %s", json.dumps(latents))

    response = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as pool:
        for model_name, requested_latents in latents.items():
            sae_activations = []
            if not requested_latents:
                logger.warning(
                    "Skipping ViT '%s' with no requested latents.", model_name
                )
                response[model_name] = sae_activations
                continue

            model_cfg = MODEL_LOOKUP[model_name]
            vit, vit_transform, scalar, mean = load_vit(model_cfg)
            if vit is None:
                logger.warning("Skipping ViT '%s'", model_name)
                continue
            sae = load_sae(model_cfg)

            mean = mean.to(DEVICE)
            x = vit_transform(img_p)[None, ...].to(DEVICE)

            _, vit_acts_BLPD = vit(x)
            vit_acts_PD = (
                vit_acts_BLPD[0, 0, 1:].to(DEVICE).clamp(-1e-5, 1e5) - mean
            ) / scalar

            _, f_x_PS, _ = sae(vit_acts_PD)
            # Ignore [CLS] token and get just the requested latents.
            acts_SP = einops.rearrange(f_x_PS, "patches n_latents -> n_latents patches")
            logger.info("Got SAE activations for '%s'.", model_name)
            top_img_i, top_values = load_tensors(model_cfg)
            logger.info("Loaded top SAE activations for '%s'.", model_name)

            for latent in requested_latents:
                sae_activations.append(
                    make_sae_activation(
                        model_cfg,
                        latent,
                        acts_SP[latent].cpu().numpy(),
                        data.pil_to_vips(img_p),
                        top_img_i[latent].tolist(),
                        top_values[latent],
                        pool,
                    )
                )
            response[model_name] = sae_activations
    return response


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress", total: int = 0):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
            total: If non-zero, how long the iterable is.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)
        self.total = total

    def __iter__(self):
        start = time.time()

        try:
            total = len(self)
        except TypeError:
            total = None

        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if total is not None:
                    pred_min = (total - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        total,
                        (i + 1) / total * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        if self.total > 0:
            return self.total

        # Will throw exception.
        return len(self.it)


#############
# Interface #
#############


with gr.Blocks() as demo:
    example_id_text = gr.Text(label="Test Example")

    input_image_base64 = gr.Text(label="Image in Base64")
    input_image_label = gr.Text(label="Image Label")
    get_input_image_btn = gr.Button(value="Get Input Image")
    get_input_image_btn.click(
        get_image,
        inputs=[example_id_text],
        outputs=[input_image_base64, input_image_label],
        api_name="get-image",
        postprocess=False,
    )

    latents_json = gr.JSON(label="Latents", value={})
    activations_json = gr.JSON(label="Activations", value={})

    input_image = gr.Image(
        label="Input Image",
        sources=["upload", "clipboard"],
        type="pil",
        interactive=True,
    )
    get_sae_activations_btn = gr.Button(value="Get SAE Activations")
    get_sae_activations_btn.click(
        get_sae_activations,
        inputs=[input_image, latents_json],
        outputs=[activations_json],
        api_name="get-sae-activations",
    )


if __name__ == "__main__":
    demo.launch()

```

# saev/interactive/metrics.py

```python
import marimo

__generated_with = "0.9.32"
app = marimo.App(
    width="medium",
    css_file="/home/stevens.994/.config/marimo/custom.css",
)


@app.cell
def __():
    import json
    import os

    import altair as alt
    import beartype
    import marimo as mo
    import matplotlib.pyplot as plt
    import numpy as np
    import polars as pl
    from jaxtyping import Float, jaxtyped

    import wandb

    return Float, alt, beartype, jaxtyped, json, mo, np, os, pl, plt, wandb


@app.cell
def __(mo):
    tag_input = mo.ui.text(value="classification-v1.0", label="Sweep Tag:")
    return (tag_input,)


@app.cell
def __(mo, tag_input):
    mo.vstack([
        mo.md(
            "Look at [WandB](https://wandb.ai/samuelstevens/saev/table) to pick your tag."
        ),
        tag_input,
    ])
    return


@app.cell
def __(alt, df, mo, pl):
    chart = mo.ui.altair_chart(
        alt.Chart(
            df.filter(
                pl.col("config/data/shard_root")
                == "/local/scratch/stevens.994/cache/saev/724b1b7be995ef7212d64640fec2885737a706a33b8e5a18f7f323223bd43af1/"
            ).select(
                "summary/eval/l0",
                "summary/losses/mse",
                "id",
                "config/sae/sparsity_coeff",
                "config/lr",
                "config/sae/d_sae",
                "model_key",
            )
        )
        .mark_point()
        .encode(
            x=alt.X("summary/eval/l0"),
            y=alt.Y("summary/losses/mse"),
            tooltip=["id", "config/lr"],
            color="config/lr:Q",
            # shape="config/sae/sparsity_coeff:N",
            shape="config/sae/d_sae:N",
            # shape="model_key",
        )
    )
    chart
    return (chart,)


@app.cell
def __(chart, df, mo, np, plot_dist, plt):
    mo.stop(
        len(chart.value) < 2,
        mo.md(
            "Select two or more points. Exactly one point is not supported because of a [Polars bug](https://github.com/pola-rs/polars/issues/19855)."
        ),
    )

    sub_df = (
        df.select(
            "id",
            "summary/eval/freqs",
            "summary/eval/mean_values",
            "summary/eval/l0",
        )
        .join(chart.value.select("id"), on="id", how="inner")
        .sort(by="summary/eval/l0")
        .head(4)
    )

    scatter_fig, scatter_axes = plt.subplots(
        ncols=len(sub_df), figsize=(12, 3), squeeze=False, sharey=True, sharex=True
    )

    hist_fig, hist_axes = plt.subplots(
        ncols=len(sub_df),
        nrows=2,
        figsize=(12, 6),
        squeeze=False,
        sharey=True,
        sharex=True,
    )

    # Always one row
    scatter_axes = scatter_axes.reshape(-1)
    hist_axes = hist_axes.T

    for (id, freqs, values, _), scatter_ax, (freq_hist_ax, values_hist_ax) in zip(
        sub_df.iter_rows(), scatter_axes, hist_axes
    ):
        plot_dist(
            freqs.astype(float),
            (-1.0, 1.0),
            values.astype(float),
            (-2.0, 2.0),
            scatter_ax,
        )
        # ax.scatter(freqs, values, marker=".", alpha=0.03)
        # ax.set_yscale("log")
        # ax.set_xscale("log")
        scatter_ax.set_title(id)

        # Plot feature
        bins = np.linspace(-6, 1, 100)
        freq_hist_ax.hist(np.log10(freqs.astype(float)), bins=bins)
        freq_hist_ax.set_title(f"{id} Feat. Freq. Dist.")

        values_hist_ax.hist(np.log10(values.astype(float)), bins=bins)
        values_hist_ax.set_title(f"{id} Mean Val. Distribution")

    scatter_fig.tight_layout()
    hist_fig.tight_layout()
    return (
        bins,
        freq_hist_ax,
        freqs,
        hist_axes,
        hist_fig,
        id,
        scatter_ax,
        scatter_axes,
        scatter_fig,
        sub_df,
        values,
        values_hist_ax,
    )


@app.cell
def __(scatter_fig):
    scatter_fig
    return


@app.cell
def __(hist_fig):
    hist_fig
    return


@app.cell
def __(chart, df, pl):
    df.join(chart.value.select("id"), on="id", how="inner").sort(
        by="summary/eval/l0"
    ).select("id", pl.selectors.starts_with("config/"))
    return


@app.cell
def __(Float, beartype, jaxtyped, np):
    @jaxtyped(typechecker=beartype.beartype)
    def plot_dist(
        freqs: Float[np.ndarray, " d_sae"],
        freqs_log_range: tuple[float, float],
        values: Float[np.ndarray, " d_sae"],
        values_log_range: tuple[float, float],
        ax,
    ):
        log_sparsity = np.log10(freqs + 1e-9)
        log_values = np.log10(values + 1e-9)

        mask = np.ones(len(log_sparsity)).astype(bool)
        min_log_freq, max_log_freq = freqs_log_range
        mask[log_sparsity < min_log_freq] = False
        mask[log_sparsity > max_log_freq] = False
        min_log_value, max_log_value = values_log_range
        mask[log_values < min_log_value] = False
        mask[log_values > max_log_value] = False

        n_shown = mask.sum()
        ax.scatter(
            log_sparsity[mask],
            log_values[mask],
            marker=".",
            alpha=0.1,
            color="tab:blue",
            label=f"Shown ({n_shown})",
        )
        n_filtered = (~mask).sum()
        ax.scatter(
            log_sparsity[~mask],
            log_values[~mask],
            marker=".",
            alpha=0.1,
            color="tab:red",
            label=f"Filtered ({n_filtered})",
        )

        ax.axvline(min_log_freq, linewidth=0.5, color="tab:red")
        ax.axvline(max_log_freq, linewidth=0.5, color="tab:red")
        ax.axhline(min_log_value, linewidth=0.5, color="tab:red")
        ax.axhline(max_log_value, linewidth=0.5, color="tab:red")

        ax.set_xlabel("Feature Frequency (log10)")
        ax.set_ylabel("Mean Activation Value (log10)")

    return (plot_dist,)


@app.cell
def __(
    beartype,
    get_data_key,
    get_model_key,
    json,
    load_freqs,
    load_mean_values,
    mo,
    os,
    pl,
    tag_input,
    wandb,
):
    @beartype.beartype
    def make_df(tag: str):
        runs = wandb.Api().runs(path="samuelstevens/saev", filters={"config.tag": tag})

        rows = []
        for run in mo.status.progress_bar(
            runs,
            remove_on_exit=True,
            title="Loading",
            subtitle="Parsing runs from WandB",
        ):
            row = {}
            row["id"] = run.id

            row.update(**{
                f"summary/{key}": value for key, value in run.summary.items()
            })
            try:
                row["summary/eval/freqs"] = load_freqs(run)
            except ValueError:
                print(f"Run {run.id} did not log eval/freqs.")
                continue
            except RuntimeError:
                print(f"Wandb blew up on run {run.id}.")
                continue
            try:
                row["summary/eval/mean_values"] = load_mean_values(run)
            except ValueError:
                print(f"Run {run.id} did not log eval/mean_values.")
                continue
            except RuntimeError:
                print(f"Wandb blew up on run {run.id}.")
                continue

            # config
            row.update(**{
                f"config/data/{key}": value
                for key, value in run.config.pop("data").items()
            })
            row.update(**{
                f"config/sae/{key}": value
                for key, value in run.config.pop("sae").items()
            })

            row.update(**{f"config/{key}": value for key, value in run.config.items()})
            try:
                with open(
                    os.path.join(row["config/data/shard_root"], "metadata.json")
                ) as fd:
                    metadata = json.load(fd)
            except FileNotFoundError:
                print(f"Bad run {run.id}: missing metadata.json")
                continue

            row["model_key"] = get_model_key(metadata)

            data_key = get_data_key(metadata)
            if data_key is None:
                print(f"Bad run {run.id}: unknown data.")
                continue
            row["data_key"] = data_key

            row["config/d_vit"] = metadata["d_vit"]
            rows.append(row)

        if not rows:
            raise ValueError("No runs found.")

        df = pl.DataFrame(rows).with_columns(
            (pl.col("config/sae/d_vit") * pl.col("config/sae/exp_factor")).alias(
                "config/sae/d_sae"
            )
        )
        return df

    df = make_df(tag_input.value)
    return df, make_df


@app.cell
def __(beartype):
    @beartype.beartype
    def get_model_key(metadata: dict[str, object]) -> str | None:
        family, ckpt = metadata["vit_family"], metadata["vit_ckpt"]
        if family == "dinov2" and ckpt == "dinov2_vitb14_reg":
            return "DINOv2 ViT-B/14"
        if family == "clip" and ckpt == "ViT-B-16/openai":
            return "CLIP ViT-B/16"
        if family == "clip" and ckpt == "hf-hub:imageomics/bioclip":
            return "BioCLIP ViT-B/16"

        print(f"Unknown model: {(family, ckpt)}")
        return None

    @beartype.beartype
    def get_data_key(metadata: dict[str, object]) -> str | None:
        if (
            "train_mini" in metadata["data"]
            and "ImageFolderDataset" in metadata["data"]
        ):
            return "iNat21"

        if "train" in metadata["data"] and "Imagenet" in metadata["data"]:
            return "ImageNet-1K"

        print(f"Unknown data: {metadata['data']}")
        return None

    return get_data_key, get_model_key


@app.cell
def __(Float, json, np, os):
    def load_freqs(run) -> Float[np.ndarray, " d_sae"]:
        try:
            for artifact in run.logged_artifacts():
                if "evalfreqs" not in artifact.name:
                    continue

                dpath = artifact.download()
                fpath = os.path.join(dpath, "eval", "freqs.table.json")
                print(fpath)
                with open(fpath) as fd:
                    raw = json.load(fd)
                return np.array(raw["data"]).reshape(-1)
        except Exception as err:
            raise RuntimeError("Wandb sucks.") from err

        raise ValueError(f"freqs not found in run '{run.id}'")

    def load_mean_values(run) -> Float[np.ndarray, " d_sae"]:
        try:
            for artifact in run.logged_artifacts():
                if "evalmean_values" not in artifact.name:
                    continue

                dpath = artifact.download()
                fpath = os.path.join(dpath, "eval", "mean_values.table.json")
                print(fpath)
                with open(fpath) as fd:
                    raw = json.load(fd)
                return np.array(raw["data"]).reshape(-1)
        except Exception as err:
            raise RuntimeError("Wandb sucks.") from err

        raise ValueError(f"mean_values not found in run '{run.id}'")

    return load_freqs, load_mean_values


@app.cell
def __(df):
    df.drop(
        "config/log_every",
        "config/slurm_acct",
        "config/device",
        "config/n_workers",
        "config/wandb_project",
        "config/track",
        "config/slurm",
        "config/log_to",
        "config/ckpt_path",
        "config/sae/ghost_grads",
    )
    return


if __name__ == "__main__":
    app.run()

```

# saev/interactive/features.py

```python
import marimo

__generated_with = "0.9.32"
app = marimo.App(width="full")


@app.cell
def __():
    import json
    import os
    import random

    import marimo as mo
    import matplotlib.pyplot as plt
    import numpy as np
    import polars as pl
    import torch
    import tqdm

    return json, mo, np, os, pl, plt, random, torch, tqdm


@app.cell
def __(mo, os):
    def make_ckpt_dropdown():
        try:
            choices = sorted(
                os.listdir("/research/nfs_su_809/workspace/stevens.994/saev/features")
            )

        except FileNotFoundError:
            choices = []

        return mo.ui.dropdown(choices, label="Checkpoint:")

    ckpt_dropdown = make_ckpt_dropdown()
    return ckpt_dropdown, make_ckpt_dropdown


@app.cell
def __(ckpt_dropdown, mo):
    mo.hstack([ckpt_dropdown], justify="start")
    return


@app.cell
def __(ckpt_dropdown, mo):
    mo.stop(
        ckpt_dropdown.value is None,
        mo.md(
            "Run `uv run main.py webapp --help` to fill out at least one checkpoint."
        ),
    )

    webapp_dir = f"/research/nfs_su_809/workspace/stevens.994/saev/features/{ckpt_dropdown.value}/sort_by_patch"

    get_i, set_i = mo.state(0)
    return get_i, set_i, webapp_dir


@app.cell
def __(mo):
    sort_by_freq_btn = mo.ui.run_button(label="Sort by frequency")

    sort_by_value_btn = mo.ui.run_button(label="Sort by value")

    sort_by_latent_btn = mo.ui.run_button(label="Sort by latent")
    return sort_by_freq_btn, sort_by_latent_btn, sort_by_value_btn


@app.cell
def __(mo, sort_by_freq_btn, sort_by_latent_btn, sort_by_value_btn):
    mo.hstack(
        [sort_by_freq_btn, sort_by_value_btn, sort_by_latent_btn], justify="start"
    )
    return


@app.cell
def __(
    json,
    mo,
    os,
    sort_by_freq_btn,
    sort_by_latent_btn,
    sort_by_value_btn,
    tqdm,
    webapp_dir,
):
    def get_neurons() -> list[dict]:
        rows = []
        for name in tqdm.tqdm(list(os.listdir(f"{webapp_dir}/neurons"))):
            if not name.isdigit():
                continue
            try:
                with open(f"{webapp_dir}/neurons/{name}/metadata.json") as fd:
                    rows.append(json.load(fd))
            except FileNotFoundError:
                print(f"Missing metadata.json for neuron {name}.")
                continue
            # rows.append({"neuron": int(name)})
        return rows

    neurons = get_neurons()

    if sort_by_latent_btn.value:
        neurons = sorted(neurons, key=lambda dct: dct["neuron"])
    elif sort_by_freq_btn.value:
        neurons = sorted(neurons, key=lambda dct: dct["log10_freq"])
    elif sort_by_value_btn.value:
        neurons = sorted(neurons, key=lambda dct: dct["log10_value"], reverse=True)

    mo.md(f"Found {len(neurons)} saved neurons.")
    return get_neurons, neurons


@app.cell
def __(mo, neurons, set_i):
    next_button = mo.ui.button(
        label="Next",
        on_change=lambda _: set_i(lambda v: (v + 1) % len(neurons)),
    )

    prev_button = mo.ui.button(
        label="Previous",
        on_change=lambda _: set_i(lambda v: (v - 1) % len(neurons)),
    )
    return next_button, prev_button


@app.cell
def __(get_i, mo, neurons, set_i):
    neuron_slider = mo.ui.slider(
        0,
        len(neurons),
        value=get_i(),
        on_change=lambda i: set_i(i),
        full_width=True,
    )
    return (neuron_slider,)


@app.cell
def __():
    return


@app.cell
def __(
    display_info,
    get_i,
    mo,
    neuron_slider,
    neurons,
    next_button,
    prev_button,
):
    # label = f"Neuron {neurons[get_i()]} ({get_i()}/{len(neurons)}; {get_i() / len(neurons) * 100:.2f}%)"
    # , display_info(**neurons[get_i()])
    mo.md(f"""
    {mo.hstack([prev_button, next_button, display_info(**neurons[get_i()])], justify="start")}
    {neuron_slider}
    """)
    return


@app.cell
def __():
    return


@app.cell
def __(get_i, mo, neurons):
    def display_info(log10_freq: float, log10_value: float, neuron: int):
        return mo.md(
            f"Neuron {neuron} ({get_i()}/{len(neurons)}; {get_i() / len(neurons) * 100:.1f}%) | Frequency: {10**log10_freq * 100:.3f}% of inputs | Mean Value: {10**log10_value:.3f}"
        )

    return (display_info,)


@app.cell
def __(mo, webapp_dir):
    def show_img(n: int, i: int):
        label = "No label found."
        try:
            label = open(f"{webapp_dir}/neurons/{n}/{i}.txt").read().strip()
            label = " ".join(label.split("_"))
        except FileNotFoundError:
            return mo.md(f"*Missing image {i + 1}*")

        return mo.vstack([mo.image(f"{webapp_dir}/neurons/{n}/{i}.png"), mo.md(label)])

    return (show_img,)


@app.cell
def __(get_i, mo, neurons, show_img):
    n = neurons[get_i()]["neuron"]

    mo.vstack([
        mo.hstack(
            [
                show_img(n, 0),
                show_img(n, 1),
                show_img(n, 2),
                show_img(n, 3),
                show_img(n, 4),
            ],
            widths="equal",
        ),
        mo.hstack(
            [
                show_img(n, 5),
                show_img(n, 6),
                show_img(n, 7),
                show_img(n, 8),
                show_img(n, 9),
            ],
            widths="equal",
        ),
        mo.hstack(
            [
                show_img(n, 10),
                show_img(n, 11),
                show_img(n, 12),
                show_img(n, 13),
                show_img(n, 14),
            ],
            widths="equal",
        ),
        mo.hstack(
            [
                show_img(n, 15),
                show_img(n, 16),
                show_img(n, 17),
                show_img(n, 18),
                show_img(n, 19),
            ],
            widths="equal",
        ),
        mo.hstack(
            [
                show_img(n, 20),
                show_img(n, 21),
                show_img(n, 22),
                show_img(n, 23),
                show_img(n, 24),
            ],
            widths="equal",
        ),
    ])
    return (n,)


@app.cell
def __(os, torch, webapp_dir):
    sparsity_fpath = os.path.join(webapp_dir, "sparsity.pt")
    sparsity = torch.load(sparsity_fpath, weights_only=True, map_location="cpu")

    values_fpath = os.path.join(webapp_dir, "mean_values.pt")
    values = torch.load(values_fpath, weights_only=True, map_location="cpu")
    return sparsity, sparsity_fpath, values, values_fpath


@app.cell
def __(mo, np, plt, sparsity):
    def plot_hist(counts):
        fig, ax = plt.subplots()
        ax.hist(np.log10(counts.numpy() + 1e-9), bins=100)
        return fig

    mo.md(f"""
    Sparsity Log10

    {mo.as_html(plot_hist(sparsity))}
    """)
    return (plot_hist,)


@app.cell
def __(mo, plot_hist, values):
    mo.md(f"""
    Mean Value Log10

    {mo.as_html(plot_hist(values))}
    """)
    return


@app.cell
def __(np, plt, sparsity, values):
    def plot_dist(
        min_log_sparsity: float,
        max_log_sparsity: float,
        min_log_value: float,
        max_log_value: float,
    ):
        fig, ax = plt.subplots()

        log_sparsity = np.log10(sparsity.numpy() + 1e-9)
        log_values = np.log10(values.numpy() + 1e-9)

        mask = np.ones(len(log_sparsity)).astype(bool)
        mask[log_sparsity < min_log_sparsity] = False
        mask[log_sparsity > max_log_sparsity] = False
        mask[log_values < min_log_value] = False
        mask[log_values > max_log_value] = False

        n_shown = mask.sum()
        ax.scatter(
            log_sparsity[mask],
            log_values[mask],
            marker=".",
            alpha=0.1,
            color="tab:blue",
            label=f"Shown ({n_shown})",
        )
        n_filtered = (~mask).sum()
        ax.scatter(
            log_sparsity[~mask],
            log_values[~mask],
            marker=".",
            alpha=0.1,
            color="tab:red",
            label=f"Filtered ({n_filtered})",
        )

        ax.axvline(min_log_sparsity, linewidth=0.5, color="tab:red")
        ax.axvline(max_log_sparsity, linewidth=0.5, color="tab:red")
        ax.axhline(min_log_value, linewidth=0.5, color="tab:red")
        ax.axhline(max_log_value, linewidth=0.5, color="tab:red")

        ax.set_xlabel("Feature Frequency (log10)")
        ax.set_ylabel("Mean Activation Value (log10)")
        ax.legend(loc="upper right")

        return fig

    return (plot_dist,)


@app.cell
def __(mo, plot_dist, sparsity_slider, value_slider):
    mo.md(f"""
    Log Sparsity Range: {sparsity_slider}
    {sparsity_slider.value}

    Log Value Range: {value_slider}
    {value_slider.value}

    {mo.as_html(plot_dist(sparsity_slider.value[0], sparsity_slider.value[1], value_slider.value[0], value_slider.value[1]))}
    """)
    return


@app.cell
def __(mo):
    sparsity_slider = mo.ui.range_slider(start=-8, stop=0, step=0.1, value=[-6, -1])
    return (sparsity_slider,)


@app.cell
def __(mo):
    value_slider = mo.ui.range_slider(start=-3, stop=1, step=0.1, value=[-0.75, 1.0])
    return (value_slider,)


@app.cell
def __():
    return


@app.cell
def __():
    return


@app.cell
def __():
    return


@app.cell
def __():
    return


@app.cell
def __():
    return


if __name__ == "__main__":
    app.run()

```

# contrib/__init__.py

```python

```

# contrib/semseg/visuals.py

```python
"""
Propose features for manual verification.
"""

import beartype
import einops
import numpy as np
import torch
from jaxtyping import Int, Shaped, jaxtyped

import saev.helpers
import saev.nn

from . import config, training


@beartype.beartype
@torch.no_grad
def main(cfg: config.Visuals):
    sae = saev.nn.load(cfg.sae_ckpt)
    sae = sae.to(cfg.device)

    dataset = training.Dataset(cfg.acts, cfg.imgs)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        shuffle=False,
        persistent_workers=(cfg.n_workers > 0),
    )

    tp = torch.zeros((sae.cfg.d_sae,), dtype=int, device=cfg.device)
    fp = torch.zeros((sae.cfg.d_sae,), dtype=int, device=cfg.device)
    fn = torch.zeros((sae.cfg.d_sae,), dtype=int, device=cfg.device)

    for batch in saev.helpers.progress(dataloader):
        pixel_labels = einops.rearrange(
            batch["pixel_labels"],
            "batch (w pw) (h ph) -> batch w h (pw ph)",
            # TODO: change from hard-coded values
            pw=16,
            ph=16,
        )
        unique, counts = axis_unique(pixel_labels.numpy(), null_value=0)

        # TODO: change from hard-coded values
        # 256 is 16x16
        idx = counts[:, :, :, 0] > (256 * cfg.label_threshold)
        acts = batch["acts"][idx].to(cfg.device)
        labels = unique[idx][:, 0]

        _, f_x, _ = sae(acts)

        pred = f_x > 0
        true = torch.from_numpy(labels == cfg.ade20k_cls).view(-1, 1).to(cfg.device)

        tp += (pred & true).sum(axis=0)
        fp += (pred & ~true).sum(axis=0)
        fn += (~pred & true).sum(axis=0)

    f1 = (2 * tp) / (2 * tp + fp + fn)
    latents = " ".join(str(i) for i in f1.topk(cfg.k).indices.tolist())

    scale_mean_flag = (
        "--data.scale-mean" if cfg.acts.scale_mean else "--data.no-scale-mean"
    )
    scale_norm_flag = (
        "--data.scale-norm" if cfg.acts.scale_norm else "--data.no-scale-norm"
    )

    print("Run this command to save best images:")
    print()
    print(
        f"  uv run python -m saev visuals --ckpt {cfg.sae_ckpt} --include-latents {latents} --data.shard-root {cfg.acts.shard_root} {scale_mean_flag} {scale_norm_flag} images:ade20k-dataset --images.root {cfg.imgs.root} --images.split {cfg.imgs.split}"
    )
    print()
    print("Be sure to add --dump-to to this command.")


@jaxtyped(typechecker=beartype.beartype)
def axis_unique(
    a: Shaped[np.ndarray, "*axes"],
    axis: int = -1,
    return_counts: bool = True,
    *,
    null_value: int = -1,
) -> (
    Shaped[np.ndarray, "*axes"]
    | tuple[Shaped[np.ndarray, "*axes"], Int[np.ndarray, "*axes"]]
):
    """
    Calculate unique values and their counts along any axis of a matrix.

    Arguments:
        a: Input array
        axis: The axis along which to find unique values.
        return_counts: If true, also return the count of each unique value

    Returns:
        unique: Array of unique values, with zeros replacing duplicates
        counts: (optional) Count of each unique value (only if return_counts=True)
    """
    assert isinstance(axis, int)

    # Move the target axis to the end for consistent processing
    a_transformed = np.moveaxis(a, axis, -1)

    # Sort along the last axis
    sorted_a = np.sort(a_transformed, axis=-1)

    # Find duplicates
    duplicates = sorted_a[..., 1:] == sorted_a[..., :-1]

    # Create output array
    unique = sorted_a.copy()
    unique[..., 1:][duplicates] = null_value

    if not return_counts:
        # Move axis back to original position
        return np.moveaxis(unique, -1, axis)

    # Calculate counts
    shape = list(a_transformed.shape)
    count_matrix = np.zeros(shape, dtype=int)

    # Process each slice along other dimensions
    for idx in np.ndindex(*shape[:-1]):
        slice_unique = unique[idx]
        idxs = np.flatnonzero(slice_unique)
        if len(idxs) > 0:
            # Calculate counts using diff for intermediate positions
            counts = np.diff(idxs)
            count_matrix[idx][idxs[:-1]] = counts
            # Handle the last unique value
            count_matrix[idx][idxs[-1]] = shape[-1] - idxs[-1]

    # Move axes back to original positions
    unique = np.moveaxis(unique, -1, axis)
    count_matrix = np.moveaxis(count_matrix, -1, axis)

    return unique, count_matrix

```

# contrib/semseg/config.py

```python
"""
Configs for all the different subscripts in `contrib.semseg`.

Imports must be fast in this file, as described in `saev.config`.
So do not import torch, numpy, etc.
"""

import dataclasses
import os.path
import typing

import beartype

import saev.config


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Train:
    learning_rate: float = 1e-4
    """Linear layer learning rate."""
    weight_decay: float = 1e-3
    """Weight decay  for AdamW."""
    n_epochs: int = 400
    """Number of training epochs for linear layer."""
    batch_size: int = 1024
    """Training batch size for linear layer."""
    n_workers: int = 32
    """Number of dataloader workers."""
    imgs: saev.config.Ade20kDataset = dataclasses.field(
        default_factory=saev.config.Ade20kDataset
    )
    """Configuration for the ADE20K dataset."""
    eval_every: int = 100
    """How many epochs between evaluations."""
    device: str = "cuda"
    "Hardware to train on."
    ckpt_path: str = os.path.join(".", "checkpoints", "contrib", "semseg")
    seed: int = 42
    """Random seed."""
    log_to: str = os.path.join(".", "logs", "contrib", "semseg")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Visuals:
    sae_ckpt: str = os.path.join(".", "checkpoints", "sae.pt")
    """Path to the sae.pt file."""
    ade20k_cls: int = 29
    """ADE20K class to probe for."""
    k: int = 32
    """Top K features to save."""
    acts: saev.config.DataLoad = dataclasses.field(default_factory=saev.config.DataLoad)
    """Configuration for the saved ADE20K training ViT activations."""
    imgs: saev.config.Ade20kDataset = dataclasses.field(
        default_factory=lambda: saev.config.Ade20kDataset(split="training")
    )
    """Configuration for the ADE20K training dataset."""
    batch_size: int = 128
    """Batch size for calculating F1 scores."""
    n_workers: int = 32
    """Number of dataloader workers."""
    label_threshold: float = 0.9
    device: str = "cuda"
    "Hardware for SAE inference."


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Validation:
    ckpt_root: str = os.path.join(".", "checkpoints", "contrib", "semseg")
    """Root to all checkpoints to evaluate."""
    dump_to: str = os.path.join(".", "logs", "contrib", "semseg")
    """Directory to dump results to."""
    imgs: saev.config.Ade20kDataset = dataclasses.field(
        default_factory=lambda: saev.config.Ade20kDataset(split="validation")
    )
    """Configuration for the ADE20K validation dataset."""
    batch_size: int = 128
    """Batch size for calculating F1 scores."""
    n_workers: int = 32
    """Number of dataloader workers."""
    device: str = "cuda"
    "Hardware for linear probe inference."


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Quantitative:
    sae_ckpt: str = os.path.join(".", "checkpoints", "sae.pt")
    """Path to trained SAE checkpoint."""
    seg_ckpt: str = os.path.join(".", "checkpoints", "contrib", "semseg", "best.pt")
    """Path to trained segmentation head."""
    top_values: str = os.path.join(".", "data", "sort_by_patch", "top_values.pt")
    """Path to top_values.pt file generated by `saev visuals`."""
    sparsity: str = os.path.join(".", "data", "sort_by_patch", "sparsity.pt")
    """Path to sparsity.pt file generated by `saev visuals`."""
    max_freq = 3e-2
    """Maximum frequency. Any feature that fires more than this is ignored."""
    act_mean: str = os.path.join(
        ".", "data", "contrib", "semseg", "dinov2_imagenet1k_mean.pt"
    )
    """Where to load activation mean from."""
    act_norm: float = 2.0181241035461426
    """How much to scale activations such that average dataset norm is approximately sqrt(d_vit)."""

    label_threshold: float = 0.9
    """Proportion of pixels that must have the same label to consider a given patch when calculating F1."""

    vit_family: typing.Literal["clip", "siglip", "dinov2", "moondream2"] = "dinov2"
    """Which ViT family."""
    vit_ckpt: str = "dinov2_vitb14_reg"
    """Specific ViT checkpoint."""
    vit_layer: int = 11
    """Vit layer to read/modify."""
    patch_size_px: tuple[int, int] = (14, 14)
    """ViT patch size."""

    n_patches_per_img: int = 256
    """Number of ViT patches per image (depends on model)."""
    cls_token: bool = True
    """Whether the model has a [CLS] token."""

    imgs: saev.config.Ade20kDataset = dataclasses.field(
        default_factory=lambda: saev.config.Ade20kDataset(split="validation")
    )
    """Data configuration for ADE20K dataset."""

    batch_size: int = 128
    """Batch size for inference."""
    n_workers: int = 32
    """Number of dataloader workers."""

    scale: float = -2.0
    """Intervention scale. Likely needs to be larger for random-vector."""
    top_k: int = 3
    """Number of latents to show."""

    device: str = "cuda"
    """Hardware for inference."""
    dump_to: str = os.path.join(".", "logs", "contrib", "semseg", "quantitative")
    """Directory to save results to."""
    seed: int = 42
    """Random seed."""


@beartype.beartype
def grid(cfg: Train, sweep_dct: dict[str, object]) -> tuple[list[Train], list[str]]:
    cfgs, errs = [], []
    for d, dct in enumerate(saev.config.expand(sweep_dct)):
        try:
            cfgs.append(dataclasses.replace(cfg, **dct, seed=cfg.seed + d))
        except Exception as err:
            errs.append(str(err))

    return cfgs, errs

```

# contrib/semseg/interactive.py

```python
import marimo

__generated_with = "0.9.32"
app = marimo.App(
    width="full",
    css_file="/home/stevens.994/.config/marimo/custom.css",
)


@app.cell
def __():
    n_images_per_feature = 5
    n_features = 3

    pw_px, ph_px = (14, 14)

    colors = [
        "#1f78b4",
        "#33a02c",
        "#e31a1c",
        "#ff7f00",
        "#a6cee3",
        "#b2df8a",
        "#fb9a99",
        "#fdbf6f",
    ]
    return colors, n_features, n_images_per_feature, ph_px, pw_px


@app.cell(hide_code=True)
def __(mo):
    mo.md(
        r"""
        This is a dashboard to explore how you can use sparse autoencoders to manipulate *semantic segmentations*. It presents a couple different ways to automatically propose features that you might want to manipulate.

        How do we propose features?

        1. Pick the 3 features that maximally activate on a target class's patches. This demonstrates that we can influence decisions for/against a class.
        3. Pick 3 random features. This demonstrates that arbitrary, unrelated directions don't really matter for downstream predictions (precision).

        For all of these features, show the `n_images_per_feature` highest activating examples from ImageNet-1K.
        """
    )
    return


@app.cell
def __(contrib, saev):
    head_ckpt_fpath = "/home/stevens.994/projects/saev/checkpoints/contrib/semseg/lr_0_001__wd_0_001/model_step8000.pt"
    head = contrib.semseg.training.load(head_ckpt_fpath)
    head.eval()

    sae_ckpt_fpath = "checkpoints/oebd6e6i/sae.pt"
    sae = saev.nn.load(sae_ckpt_fpath)
    sae.eval()
    return head, head_ckpt_fpath, sae, sae_ckpt_fpath


@app.cell
def __(einops, saev, v2):
    acts_cfg = saev.config.DataLoad(
        shard_root="/local/scratch/stevens.994/cache/saev/1864947033ca8b8a171a482644a948a6e6489e3249469373c78dfeeb0a75bcd4",
        scale_mean=True,
        scale_norm=True,
    )

    acts_dataset = saev.activations.Dataset(acts_cfg)

    imgs_cfg = saev.config.Ade20kDataset(
        root="/research/nfs_su_809/workspace/stevens.994/datasets/ade20k/"
    )

    to_array = v2.Compose([
        v2.Resize(256, interpolation=v2.InterpolationMode.NEAREST),
        v2.CenterCrop((224, 224)),
        v2.ToImage(),
        einops.layers.torch.Rearrange("channels width height -> width height channels"),
    ])

    imgs_dataset = saev.activations.Ade20k(
        imgs_cfg, img_transform=to_array, seg_transform=to_array
    )

    assert len(imgs_dataset) * acts_dataset.metadata.n_patches_per_img == len(
        acts_dataset
    )
    return acts_cfg, acts_dataset, imgs_cfg, imgs_dataset, to_array


@app.cell
def __(os, torch):
    sae_data_root = "/research/nfs_su_809/workspace/stevens.994/saev/features/oebd6e6i/sort_by_patch"

    top_img_i = torch.load(
        os.path.join(sae_data_root, "top_img_i.pt"),
        weights_only=True,
        map_location="cpu",
    )
    top_values = torch.load(
        os.path.join(sae_data_root, "top_values.pt"),
        weights_only=True,
        map_location="cpu",
    )
    sparsity = torch.load(
        os.path.join(sae_data_root, "sparsity.pt"),
        weights_only=True,
        map_location="cpu",
    )
    percentiles = torch.load(
        os.path.join(sae_data_root, "percentiles_p99.pt"),
        weights_only=True,
        map_location="cpu",
    )
    return percentiles, sae_data_root, sparsity, top_img_i, top_values


@app.cell
def __(mo):
    get_example_idx, set_example_idx = mo.state(1)
    return get_example_idx, set_example_idx


@app.cell
def __(imgs_dataset):
    len(imgs_dataset)
    return


@app.cell
def __(imgs_dataset, mo, random, set_example_idx):
    random_example_btn = mo.ui.button(
        label="Random Example",
        on_change=lambda _: set_example_idx(random.randrange(len(imgs_dataset)) + 1),
    )
    return (random_example_btn,)


@app.cell
def __(get_example_idx, imgs_dataset, mo, set_example_idx):
    example_num = mo.ui.number(
        start=1,
        stop=len(imgs_dataset),
        step=1,
        value=get_example_idx(),
        label="Example:",
        on_change=set_example_idx,
    )
    return (example_num,)


@app.cell
def __(cls_dropdowns, mo, n_features):
    getter, setter = mo.state([0.0] * n_features * len(cls_dropdowns.value))
    return getter, setter


@app.cell
def __(beartype, getter, setter):
    @beartype.beartype
    def indexed_setter(i: int, v: float):
        setter(getter()[:i] + [v] + getter()[i + 1 :])

    return (indexed_setter,)


@app.cell
def __(features, functools, getter, indexed_setter, mo):
    sliders = [
        mo.ui.slider(
            start=-10,
            stop=10,
            step=0.1,
            value=getter()[i],
            on_change=functools.partial(indexed_setter, i),
        )
        for i, _ in enumerate(features)
    ]
    sliders = mo.ui.array(sliders)
    return (sliders,)


@app.cell
def __(example_num, mo, random_example_btn):
    mo.hstack([
        mo.hstack(
            [
                random_example_btn,
                example_num,
                # mo.md(f"'{classnames[target]}'"),
            ],
            justify="start",
        ),
        # cls_dropdown,
    ])
    return


@app.cell
def __(classnames, mo):
    def make_cls_dropdowns(n: int = 3):
        i_name_pairs = sorted(
            [(j, ", ".join(names[:2])) for j, names in classnames.items()],
            key=lambda pair: pair[1],
        )
        cls_dropdowns = []
        for i in range(n):
            dropdown = mo.ui.dropdown(
                options={name: j for j, name in i_name_pairs},
                label=f"Class {i + 1}:",
                value=i_name_pairs[i][1],
            )
            cls_dropdowns.append(dropdown)
        return mo.ui.array(cls_dropdowns)

    cls_dropdowns = make_cls_dropdowns()
    return cls_dropdowns, make_cls_dropdowns


@app.cell
def __(cls_dropdowns, mo):
    mo.vstack(cls_dropdowns)
    return


@app.cell
def __(sae, sparsity, torch):
    mask = torch.ones((sae.cfg.d_sae), dtype=bool)
    mask = mask & (sparsity < 1e-2)
    return (mask,)


@app.cell
def __(acts_dataset, einops, example_num, torch):
    acts_PD = torch.stack([
        acts_dataset[
            (example_num.value - 1) * acts_dataset.metadata.n_patches_per_img + i
        ]["act"]
        for i in range(acts_dataset.metadata.n_patches_per_img)
    ])
    acts_WHD = einops.rearrange(acts_PD, "(w h) d -> w h d", w=16, h=16)
    return acts_PD, acts_WHD


@app.cell
def __(acts_WHD, head, modify, torch):
    with torch.inference_mode():
        logits_WHC = head(acts_WHD)

        modified_acts_WHD = modify(acts_WHD)
        modified_logits_WHC = head(modified_acts_WHD)
    return logits_WHC, modified_acts_WHD, modified_logits_WHC


@app.cell
def __(cls_idxs, get_aggregate_features, mask):
    features = []
    for idxs in cls_idxs:
        features += get_aggregate_features(idxs, mask=mask)
    # features += get_random_features()
    return features, idxs


@app.cell
def __(
    classnames,
    cls_dropdowns,
    features,
    in1k_dataset,
    mo,
    n_features,
    n_images_per_feature,
    sliders,
    top_img_i,
):
    def make_sliders_ui():
        rows = []
        for slider, feature in zip(sliders, features):
            imgs, seen = [], set()
            for img_i in top_img_i[feature.latent].tolist():
                if img_i in seen:
                    continue
                imgs.append(in1k_dataset[img_i]["image"])
                seen.add(img_i)

                if len(seen) >= n_images_per_feature:
                    break

            row = [
                mo.hstack(
                    [slider, f"{slider.value:.3f}", f"Latent 12K/{feature.latent}"],
                    justify="start",
                ),
                mo.hstack(imgs, justify="start", gap=0.1),
            ]

            rows.append(mo.vstack(row))

        err_msg = f"len(rows) == {len(rows)} != n_features * len(cls_dropdowns.value) == {n_features * len(cls_dropdowns.value)}"
        assert len(rows) == n_features * len(cls_dropdowns.value), err_msg

        return mo.hstack(
            [
                mo.vstack(
                    [mo.md(f"Features for '{classnames[value]}'")]
                    + rows[n_features * i : n_features * (i + 1)]
                )
                for i, value in enumerate(cls_dropdowns.value)
            ],
            justify="start",
            gap=1.0,
        )

    make_sliders_ui()
    return (make_sliders_ui,)


@app.cell
def __(
    Image,
    example_num,
    imgs_dataset,
    logits_WHC,
    make_interpolated_pred,
    make_upsampled_pred,
    modified_logits_WHC,
    seg_to_img,
):
    display = {
        "Original Image": Image.fromarray(
            imgs_dataset[example_num.value - 1]["image"].numpy()
        ),
        "True Labels": seg_to_img(imgs_dataset[example_num.value - 1]["segmentation"]),
        "Predicted Labels (Upsampled)": seg_to_img(make_upsampled_pred(logits_WHC)),
        "Predicted Labels (Interpolated)": seg_to_img(
            make_interpolated_pred(logits_WHC)
        ),
        "Predicted Labels After Manipulation (Upsampled)": seg_to_img(
            make_upsampled_pred(modified_logits_WHC)
        ),
        "Predicted Labels After Manipulation (Interpolated)": seg_to_img(
            make_interpolated_pred(modified_logits_WHC)
        ),
    }
    return (display,)


@app.cell
def __(display, mo):
    mo.hstack(
        [
            mo.vstack([img, caption], align="center")
            for caption, img in list(display.items())
        ],
        widths="equal",
    )
    return


@app.cell
def __(beartype, dataclasses):
    @beartype.beartype
    @dataclasses.dataclass(frozen=True)
    class Feature:
        latent: int
        max_obs: float
        raw_obs: float

        @property
        def default(self) -> float:
            """Return raw_obs, scaled from [-10 * max_obs, 10 * max_obs] to [-1, 1]."""
            return self.scaled(self.raw_obs)

        def unscaled(self, x: float) -> float:
            """Scale from [-10, 10] to [10 * -max_obs, 10 * max_obs]."""

            return self.map_range(
                x,
                (-10, 10),
                (-10 * self.max_obs, 10 * self.max_obs),
            )

        def scaled(self, x: float) -> float:
            """Return x, scaled from [-10 * max_obs, 10 * max_obs] to [-10, 10]."""
            return self.map_range(
                x,
                (-10 * self.max_obs, 10 * self.max_obs),
                (-10, 10),
            )

        @staticmethod
        def map_range(
            x: float,
            domain: tuple[float | int, float | int],
            range: tuple[float | int, float | int],
        ):
            a, b = domain
            c, d = range
            if not (a <= x <= b):
                raise ValueError(f"x={x:.3f} must be in {[a, b]}.")
            return c + (x - a) * (d - c) / (b - a)

    return (Feature,)


@app.cell
def __(Int, Tensor, beartype, jaxtyped, torch):
    @jaxtyped(typechecker=beartype.beartype)
    def make_patch_lookup(
        *,
        patch_size_px: tuple[int, int],
        im_size_px: tuple[int, int] = (224, 224),
    ) -> Int[Tensor, "w_px h_px"]:
        im_w_px, im_h_px = im_size_px
        p_w_px, p_h_px = patch_size_px
        xv, yv = torch.meshgrid(torch.arange(im_w_px), torch.arange(im_h_px))
        patch_lookup = (xv // p_w_px) + (yv // p_h_px) * (im_h_px // p_h_px)
        return patch_lookup

    patch_lookup = make_patch_lookup(patch_size_px=(14, 14), im_size_px=(224, 224))
    return make_patch_lookup, patch_lookup


@app.cell
def __(
    acts_dataset,
    classnames,
    cls_dropdowns,
    imgs_dataset,
    mo,
    patch_lookup,
    torch,
):
    imgs_dataloader = torch.utils.data.DataLoader(
        imgs_dataset,
        num_workers=16,
        shuffle=False,
        batch_size=512,
        persistent_workers=True,
    )

    def get_cls_act_idx(cls: int):
        idxs = []
        for batch in mo.status.progress_bar(
            imgs_dataloader,
            remove_on_exit=True,
            title="Loading",
            subtitle=f"Getting class '{classnames[cls]}' segmentations.",
        ):
            batch_i, w_i, h_i, _ = (batch["segmentation"] == cls).nonzero(as_tuple=True)

            patch_i, image_i = torch.stack([
                patch_lookup[w_i, h_i],
                batch["index"][batch_i],
            ]).unique(dim=1)
            acts_i = image_i * acts_dataset.metadata.n_patches_per_img + patch_i

            idxs.append(acts_i)

        return torch.cat(idxs)

    cls_idxs = [get_cls_act_idx(v) for v in cls_dropdowns.value]
    return cls_idxs, get_cls_act_idx, imgs_dataloader


@app.cell
def __(
    Float,
    Tensor,
    beartype,
    einops,
    features,
    jaxtyped,
    sae,
    sliders,
    torch,
):
    @jaxtyped(typechecker=beartype.beartype)
    @torch.inference_mode
    def modify(
        acts_WHD: Float[Tensor, "width height d_vit"],
    ) -> Float[Tensor, "width height d_vit"]:
        x_hat_WHD, f_x_WHS, _ = sae(acts_WHD)

        err_WHD = acts_WHD - x_hat_WHD

        latents = [f.latent for f in features]
        values = torch.tensor([
            f.unscaled(sliders.value[i]) for i, f in enumerate(features)
        ])
        modified_f_x_WHS = f_x_WHS.clone()
        modified_f_x_WHS[..., latents] = values

        # Reproduce the SAE forward pass after f_x
        modified_x_hat_WHD = (
            einops.einsum(
                modified_f_x_WHS,
                sae.W_dec,
                "width height d_sae, d_sae d_vit -> width height d_vit",
            )
            + sae.b_dec
        )
        modified_WHD = err_WHD + modified_x_hat_WHD

        return modified_WHD

    return (modify,)


@app.cell
def __(
    Bool,
    Feature,
    Int,
    Tensor,
    acts_dataset,
    beartype,
    jaxtyped,
    mo,
    n_features,
    sae,
    top_values,
    torch,
):
    @jaxtyped(typechecker=beartype.beartype)
    @torch.inference_mode
    def get_aggregate_features(
        cls_act_idx: Int[Tensor, " n"],
        mask: Bool[Tensor, " d_sae"] = torch.ones(sae.cfg.d_sae, dtype=bool),
    ) -> list[Feature]:
        acts_MD = torch.stack([
            acts_dataset[i.item()]["act"]
            for i in mo.status.progress_bar(
                cls_act_idx,
                remove_on_exit=True,
                title="Loading",
                subtitle="Getting class-specific ViT acts",
            )
        ])

        max_examples = 1024 * 8

        # Shuffle
        acts_MD = acts_MD[torch.randperm(len(acts_MD))]
        # Select
        acts_MD = acts_MD[:max_examples]

        _, f_x_MS, _ = sae(acts_MD)

        f_x_S = f_x_MS.sum(axis=0)
        latents = torch.argsort(f_x_S, descending=True).cpu()

        latents = latents[mask[latents]][:n_features]

        max_obs = top_values[latents, 0]

        return [
            Feature(latent.item(), max.max().item(), 0.0)
            for latent, max in zip(latents, max_obs)
        ]

    return (get_aggregate_features,)


@app.cell
def __(csv, imgs_cfg, os):
    classnames = {}
    with open(os.path.join(imgs_cfg.root, "objectInfo150.txt")) as fd:
        for row in csv.DictReader(fd, delimiter="\t"):
            names = [name.strip() for name in row["Name"].split(",")]
            classnames[int(row["Idx"])] = names[:2]

    # [names for idx, names in sorted(classnames.items(), key=lambda pair: pair[0])]
    return classnames, fd, names, row


@app.cell
def __(Float, Tensor, Uint8, beartype, jaxtyped, torch):
    @jaxtyped(typechecker=beartype.beartype)
    @torch.inference_mode
    def make_upsampled_pred(
        logits_WHC: Float[Tensor, "width height classes"],
    ) -> Uint8[Tensor, "width height"]:
        return (
            torch.nn.functional.interpolate(
                logits_WHC.max(axis=-1).indices.view((1, 1, 16, 16)).float(),
                scale_factor=14,
            )
            .view((224, 224))
            .type(torch.uint8)
        )

    return (make_upsampled_pred,)


@app.cell
def __(Float, Tensor, Uint8, beartype, einops, jaxtyped, torch):
    @jaxtyped(typechecker=beartype.beartype)
    @torch.inference_mode
    def make_interpolated_pred(
        logits_WHC: Float[Tensor, "width height classes"],
    ) -> Uint8[Tensor, "width height"]:
        logits_CWH = einops.rearrange(
            logits_WHC, "width height classes -> classes width height"
        )
        upsampled_CWH = torch.nn.functional.interpolate(
            logits_CWH.contiguous().unsqueeze(0), size=(224, 224), mode="bilinear"
        )[0]
        pred_WH = upsampled_CWH.argmax(axis=0).cpu().type(torch.uint8)
        return pred_WH

    return (make_interpolated_pred,)


@app.cell
def __(Image, Tensor, UInt8, beartype, einops, jaxtyped, np, random):
    @jaxtyped(typechecker=beartype.beartype)
    def make_colors(seed: int = 42) -> UInt8[np.ndarray, "n 3"]:
        values = (0, 51, 102, 153, 204, 255)
        colors = []
        for r in values:
            for g in values:
                for b in values:
                    colors.append((r, g, b))
        random.Random(seed).shuffle(colors)
        colors = np.array(colors, dtype=np.uint8)

        return colors

    @jaxtyped(typechecker=beartype.beartype)
    def seg_to_img(map: UInt8[Tensor, "width height *channel"]) -> Image.Image:
        map = map.numpy()
        if map.ndim == 3:
            map = einops.rearrange(map, "w h () -> w h")
        colored = np.zeros((224, 224, 3), dtype=np.uint8)
        for i, color in enumerate(make_colors()):
            colored[map == i + 1, :] = color

        return Image.fromarray(colored)

    return make_colors, seg_to_img


@app.cell
def __(saev, v2):
    in1k_dataset = saev.activations.get_dataset(
        saev.config.ImagenetDataset(),
        img_transform=v2.Compose([
            v2.Resize(size=(128, 128)),
            v2.CenterCrop(size=(112, 112)),
        ]),
    )
    return (in1k_dataset,)


@app.cell
def __():
    import sys

    pkg_root = "/home/stevens.994/projects/saev"
    if pkg_root not in sys.path:
        sys.path.append(pkg_root)

    import csv
    import dataclasses
    import functools
    import os.path
    import random

    import beartype
    import einops
    import einops.layers.torch
    import marimo as mo
    import numpy as np
    import torch
    from jaxtyping import Bool, Float, Int, UInt8, jaxtyped
    from PIL import Image
    from torchvision.transforms import v2

    import contrib.semseg.training
    import saev.activations
    import saev.config

    return (
        Bool,
        Float,
        Image,
        Int,
        UInt8,
        beartype,
        contrib,
        csv,
        dataclasses,
        einops,
        functools,
        jaxtyped,
        mo,
        np,
        os,
        pkg_root,
        random,
        saev,
        sys,
        torch,
        v2,
    )


if __name__ == "__main__":
    app.run()

```

# contrib/semseg/quantitative.py

```python
import csv
import dataclasses
import logging
import math
import os
import random
from collections.abc import Callable

import beartype
import einops
import numpy as np
import torch
from jaxtyping import Bool, Float, Int, UInt8, jaxtyped
from torch import Tensor

import saev.helpers
import saev.nn

from . import config, training

logger = logging.getLogger("contrib.semseg.quantitative")


@beartype.beartype
@torch.inference_mode
def main(cfg: config.Quantitative):
    """Main entry point for quantitative evaluation."""
    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than float16 and almost as accurate as float32. This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    # Load models
    sae = saev.nn.load(cfg.sae_ckpt).to(cfg.device)
    clf = training.load_latest(cfg.seg_ckpt, device=cfg.device)

    # Get validation data
    dataset = training.Dataset(cfg.imgs)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=cfg.batch_size, num_workers=cfg.n_workers, shuffle=False
    )

    # For each method (random vector, random feature, etc)
    reports = []
    for fn in (eval_auto_feat,):  # (eval_rand_vec, eval_rand_feat, eval_auto_feat):
        report = fn(cfg, sae, clf, dataloader)
        reports.append(report)

    # Save results
    save(reports, os.path.join(cfg.dump_to, "results.csv"))


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class ClassResults:
    """Results for a single class."""

    class_id: int
    """Numeric identifier for the class."""

    class_name: str
    """Human-readable name of the class."""

    n_orig_patches: int
    """Original patches that were this class."""

    n_changed_patches: int
    """After intervention, how many patches changed."""

    n_other_patches: int
    """Total patches that weren't this class."""

    n_other_changed: int
    """After intervention, how many of the other patches changed."""

    change_distribution: dict[int, int]
    """What classes did patches change to? Tracks how many times <value> a patch changed from self.class_id to <key>."""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Report:
    """Complete results from an intervention experiment."""

    method: str
    """Which intervention method was used."""

    class_results: list[ClassResults]
    """Per-class detailed results."""

    intervention_scale: float
    """Magnitude of intervention."""

    @property
    def mean_target_change(self) -> float:
        """Percentage of target patches that changed class."""
        total_target = sum(r.n_orig_patches for r in self.class_results)
        total_changed = sum(r.n_changed_patches for r in self.class_results)
        return total_changed / total_target if total_target > 0 else 0.0

    @property
    def mean_other_change(self) -> float:
        """Percentage of non-target patches that changed class."""
        total_other = sum(r.n_other_patches for r in self.class_results)
        total_changed = sum(r.n_other_changed for r in self.class_results)
        return total_changed / total_other if total_other > 0 else 0.0

    @property
    def target_change_std(self) -> float:
        """Standard deviation of change percentage across classes."""
        per_class_target_changes = np.array([
            r.n_changed_patches / r.n_orig_patches if r.n_orig_patches > 0 else 0.0
            for r in self.class_results
        ])
        return float(np.std(per_class_target_changes))

    @property
    def other_change_std(self) -> float:
        """Standard deviation of non-target patch changes across classes."""
        per_class_other_changes = np.array([
            r.n_other_changed / r.n_other_patches if r.n_other_patches > 0 else 0.0
            for r in self.class_results
        ])
        return float(np.std(per_class_other_changes))

    def to_csv_row(self) -> dict[str, float]:
        """Convert to a row for the summary CSV."""
        return {
            "method": self.method,
            "target_change": self.mean_target_change,
            "other_change": self.mean_other_change,
            "target_std": self.target_change_std,
            "other_std": self.other_change_std,
        }


@beartype.beartype
def save(results: list[Report], fpath: str) -> None:
    """
    Save evaluation results to a CSV file.

    Args:
        results: List of Report objects containing evaluation results
        dpath: Path to save the CSV file
    """

    os.makedirs(os.path.dirname(fpath), exist_ok=True)
    columns = ["method", "target_change", "target_std", "other_change", "other_std"]

    with open(fpath, "w") as fd:
        writer = csv.DictWriter(fd, fieldnames=columns)
        writer.writeheader()
        for result in results:
            writer.writerow(result.to_csv_row())


@beartype.beartype
def eval_rand_vec(
    cfg: config.Quantitative,
    sae: saev.nn.SparseAutoencoder,
    clf: torch.nn.Module,
    dataloader,
) -> Report:
    """
    Evaluates the effects of adding a random unit vector to the patches.

    Args:
        cfg: Configuration for quantitative evaluation
        sae: Trained sparse autoencoder model
        clf: Trained classifier model
        dataloader: DataLoader providing batches of images

    Returns:
        Report containing intervention results, including per-class changes
    """
    torch.manual_seed(cfg.seed)

    @jaxtyped(typechecker=beartype.beartype)
    def hook(
        x_BPD: Float[Tensor, "batch patches dim"],
    ) -> Float[Tensor, "batch patches dim"]:
        """
        Adds random unit vectors to patch activations.

        Args:
            x_BPD: Activation tensor with shape (batch_size, n_patches, d_vit) where d_vit is the ViT feature dimension

        Returns:
            Modified activation tensor with random unit vectors added
        """
        batch_size, n_patches, dim = x_BPD.shape
        rand_vecs = torch.randn((batch_size, n_patches, dim), device=cfg.device)
        # Normalize each vector to unit norm along the last dimension
        rand_vecs = rand_vecs / torch.norm(rand_vecs, dim=-1, keepdim=True)

        rand_vecs = rand_vecs * cfg.scale

        x_BPD += rand_vecs
        return x_BPD

    vit = saev.activations.make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)

    hooked_vit = saev.activations.make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)
    register_hook(hooked_vit, hook, cfg.vit_layer, cfg.n_patches_per_img)

    orig_preds, mod_preds = [], []
    for batch in dataloader:
        x_BCWH = batch["image"].to(cfg.device)

        orig_acts = vit(x_BCWH)
        orig_logits = clf(orig_acts[:, 1:, :])
        orig_preds.append(argmax_logits(orig_logits).cpu())

        # Create a custom hook for this batch
        def current_hook(module, inputs, outputs):
            x = outputs[:, patches, :]
            x = hook(x, current_batch=batch)
            outputs[:, patches, :] = x
            return outputs

        # Register the hook for this batch
        patches = hooked_vit.get_patches(cfg.n_patches_per_img)
        handle = hooked_vit.get_residuals()[cfg.vit_layer - 1].register_forward_hook(
            current_hook
        )

        mod_acts = hooked_vit(x_BCWH)
        mod_logits = clf(mod_acts[:, 1:, :])
        mod_preds.append(argmax_logits(mod_logits).cpu())

        # Remove the hook after use
        handle.remove()

    # Concatenate all predictions
    orig_preds = torch.cat(orig_preds, dim=0)
    mod_preds = torch.cat(mod_preds, dim=0)

    class_results = compute_class_results(orig_preds, mod_preds)

    return Report(
        method="random-vector",
        class_results=class_results,
        intervention_scale=cfg.scale,
    )


@beartype.beartype
def eval_rand_feat(
    cfg: config.Quantitative,
    sae: saev.nn.SparseAutoencoder,
    clf: torch.nn.Module,
    dataloader,
) -> Report:
    """
    Evaluates the effects of suppressing a random SAE feature.

    Args:
        cfg: Configuration for quantitative evaluation
        sae: Trained sparse autoencoder model
        clf: Trained classifier model
        dataloader: DataLoader providing batches of images

    Returns:
        Report containing intervention results, including per-class changes
    """
    torch.manual_seed(cfg.seed)
    random.seed(cfg.seed)

    top_values = torch.load(cfg.top_values, map_location="cpu", weights_only=True)

    @jaxtyped(typechecker=beartype.beartype)
    def hook(
        x_BPD: Float[Tensor, "batch patches dim"],
    ) -> Float[Tensor, "batch patches dim"]:
        latent = random.randrange(0, sae.cfg.d_sae)

        x_hat_BPD, f_x_BPS, _ = sae(x_BPD)

        err_BPD = x_BPD - x_hat_BPD

        value = unscaled(cfg.scale, top_values[latent].max().item())
        f_x_BPS[..., latent] = value

        # Reproduce the SAE forward pass after f_x
        mod_x_hat_BPD = sae.decode(f_x_BPS)
        mod_BPD = err_BPD + mod_x_hat_BPD
        return mod_BPD

    vit = saev.activations.make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)

    hooked_vit = saev.activations.make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)
    register_hook(hooked_vit, hook, cfg.vit_layer, cfg.n_patches_per_img)

    orig_preds, mod_preds = [], []
    for batch in dataloader:
        x_BCWH = batch["image"].to(cfg.device)

        orig_acts = vit(x_BCWH)
        orig_logits = clf(orig_acts[:, 1:, :])
        orig_preds.append(argmax_logits(orig_logits).cpu())

        mod_acts = hooked_vit(x_BCWH)
        mod_logits = clf(mod_acts[:, 1:, :])
        mod_preds.append(argmax_logits(mod_logits).cpu())

    # Concatenate all predictions
    orig_preds = torch.cat(orig_preds, dim=0)
    mod_preds = torch.cat(mod_preds, dim=0)

    class_results = compute_class_results(orig_preds, mod_preds)

    return Report(
        method="random-feature",
        class_results=class_results,
        intervention_scale=cfg.scale,
    )


@beartype.beartype
def eval_auto_feat(
    cfg: config.Quantitative,
    sae: saev.nn.SparseAutoencoder,
    clf: torch.nn.Module,
    dataloader,
) -> Report:
    torch.manual_seed(cfg.seed)
    random.seed(cfg.seed)

    top_values = torch.load(cfg.top_values, map_location="cpu", weights_only=True)

    # First, for each class, we need to pick an appropriate latent.
    # Then we can use that to lookup which latent to suppress for each patch.
    latent_lookup = get_latent_lookup(cfg, sae, dataloader)

    @jaxtyped(typechecker=beartype.beartype)
    def hook(
        x_BPD: Float[Tensor, "batch patches dim"],
        current_batch=None,
    ) -> Float[Tensor, "batch patches dim"]:
        batch_size, n_patches, _ = x_BPD.shape
        x_hat_BPD, f_x_BPS, _ = sae(x_BPD)

        err_BPD = x_BPD - x_hat_BPD

        # Get patch labels and reshape to match batch_size x patches
        patch_labels = (
            current_batch["patch_labels"]
            .view(batch_size, n_patches)
            .int()
            .to(cfg.device)
        )

        # For each patch, look up the latent to modify based on its class
        for b in range(batch_size):
            for p in range(n_patches):
                class_id = patch_labels[b, p].item()
                if class_id > 0 and class_id < 151:  # Valid class ID
                    latent = latent_lookup[class_id]
                    value = unscaled(cfg.scale, top_values[latent].max().item())
                    f_x_BPS[b, p, latent] = value

        # Reproduce the SAE forward pass after f_x
        mod_x_hat_BPD = sae.decode(f_x_BPS)
        mod_BPD = err_BPD + mod_x_hat_BPD
        return mod_BPD

    vit = saev.activations.make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)

    hooked_vit = saev.activations.make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)
    register_hook(hooked_vit, hook, cfg.vit_layer, cfg.n_patches_per_img)

    orig_preds, mod_preds = [], []
    for batch in dataloader:
        x_BCWH = batch["image"].to(cfg.device)

        orig_acts = vit(x_BCWH)
        orig_logits = clf(orig_acts[:, 1:, :])
        orig_preds.append(argmax_logits(orig_logits).cpu())

        mod_acts = hooked_vit(x_BCWH)
        mod_logits = clf(mod_acts[:, 1:, :])
        mod_preds.append(argmax_logits(mod_logits).cpu())

    # Concatenate all predictions
    orig_preds = torch.cat(orig_preds, dim=0)
    mod_preds = torch.cat(mod_preds, dim=0)

    class_results = compute_class_results(orig_preds, mod_preds)

    return Report(
        method="automatic-feature",
        class_results=class_results,
        intervention_scale=cfg.scale,
    )


@jaxtyped(typechecker=beartype.beartype)
def get_latent_lookup(
    cfg: config.Quantitative, sae: saev.nn.SparseAutoencoder, dataloader
) -> Int[Tensor, "151"]:
    """
    Dimension key:

    * B: batch dimension
    * P: patches per image
    * D: ViT hidden dimension
    * S: SAE feature dimension
    * T: threshold dimension
    * C: class dimension
    * L: layer dimension
    """
    act_mean = torch.load(cfg.act_mean, weights_only=True, map_location=cfg.device)

    latent_lookup = torch.zeros((151,), dtype=int)
    thresholds_T = torch.tensor([0, 0.1, 0.3, 0.5, 0.7, 1.0], device=cfg.device)

    vit = saev.activations.make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)
    recorded_vit = saev.activations.RecordedVisionTransformer(
        vit, cfg.n_patches_per_img, cfg.cls_token, [cfg.vit_layer - 1]
    )

    tp_counts_CTS = torch.zeros(
        (151, len(thresholds_T), sae.cfg.d_sae), dtype=torch.int32, device=cfg.device
    )
    fp_counts_CTS = torch.zeros(
        (151, len(thresholds_T), sae.cfg.d_sae), dtype=torch.int32, device=cfg.device
    )
    fn_counts_CTS = torch.zeros(
        (151, len(thresholds_T), sae.cfg.d_sae), dtype=torch.int32, device=cfg.device
    )

    # Load sparsity and set up frequency mask.
    sparsity_S = torch.load(cfg.sparsity, weights_only=True, map_location="cpu")
    mask_S = (sparsity_S < cfg.max_freq).to(cfg.device)

    for batch in saev.helpers.progress(dataloader, every=1):
        x_BCWH = batch["image"].to(cfg.device)
        _, vit_acts_BLPD = recorded_vit(x_BCWH)

        # Normalize activations
        vit_acts_BPD = (
            vit_acts_BLPD[:, 0, 1:, :].to(cfg.device).clamp(-1e-5, 1e5) - act_mean
        ) / cfg.act_norm
        _, sae_acts_BPS, _ = sae(vit_acts_BPD)

        # Merge batch and patches dimensions for easier processing
        sae_acts_BS = einops.rearrange(
            sae_acts_BPS, "batch patches d_sae -> (batch patches) d_sae"
        )

        pw, ph = cfg.patch_size_px
        patch_labels_B = batch["patch_labels"].to(cfg.device).reshape(-1)
        pixel_labels_BP = einops.rearrange(
            batch["pixel_labels"].to(cfg.device),
            "batch (w pw) (h ph) -> (batch w h) (pw ph)",
            pw=pw,
            ph=ph,
        )

        # Create mask for patches that meet the threshold
        valid_mask = get_patch_mask(pixel_labels_BP, cfg.label_threshold)
        logger.info(
            "Ignoring %d (%.1f%%) patches.",
            (~valid_mask).sum(),
            (~valid_mask).float().mean() * 100,
        )

        # Filter patch labels to only include those meeting the threshold
        patch_labels_B = patch_labels_B[valid_mask]
        sae_acts_BS = sae_acts_BS[valid_mask]

        unique_classes = torch.unique(patch_labels_B)

        for class_id in unique_classes:
            if class_id == 0:  # Skip background/null class if needed
                continue

            # if class_id == 3:
            #     breakpoint()

            class_mask_B = patch_labels_B == class_id

            # Skip if no patches of this class
            if not torch.any(class_mask_B):
                continue

            # Process all thresholds at once
            # Create binary activation masks for all thresholds
            binary_activations_TBS = (
                sae_acts_BS[None, :, :] > thresholds_T[:, None, None]
            )

            # Compute TP, FP, FN for all thresholds and features at once
            # Each has shape: [thresholds, features] = [T, S]

            # True Positives: activation is 1 AND patch is this class
            tp_TS = torch.sum(
                binary_activations_TBS & class_mask_B[None, :, None], dim=1
            )
            tp_counts_CTS[class_id] += tp_TS

            # False Positives: activation is 1 BUT patch is NOT this class
            fp_TS = torch.sum(
                binary_activations_TBS & ~class_mask_B[None, :, None], dim=1
            )
            fp_counts_CTS[class_id] += fp_TS

            # False Negatives: activation is 0 BUT patch IS this class
            fn_TS = torch.sum(
                (~binary_activations_TBS) & class_mask_B[None, :, None], dim=1
            )
            fn_counts_CTS[class_id] += fn_TS

    class_lookup = {}
    with open(os.path.join(cfg.imgs.root, "objectInfo150.txt")) as fd:
        for row in csv.DictReader(fd, delimiter="\t"):
            class_lookup[int(row["Idx"])] = row["Name"]

    for class_id, class_name in saev.helpers.progress(class_lookup.items()):
        # Compute F1 scores: 2*TP / (2*TP + FP + FN)
        tp_TS = tp_counts_CTS[class_id]
        fp_TS = fp_counts_CTS[class_id]
        fn_TS = fn_counts_CTS[class_id]

        # Add small epsilon to avoid division by zero
        f1_TS = (2 * tp_TS) / (2 * tp_TS + fp_TS + fn_TS + 1e-10)

        # Calculate precision and recall for each threshold
        # precision_TS = tp_TS / (tp_TS + fp_TS + 1e-10)  # TP / (TP + FP)
        # recall_TS = tp_TS / (tp_TS + fn_TS + 1e-10)  # TP / (TP + FN)
        breakpoint()

        f1_S, best_thresh_i_S = f1_TS.max(dim=0)
        f1_S = torch.where(mask_S, f1_S, torch.tensor(-1.0, device=f1_S.device))
        best_thresholds_S = thresholds_T[best_thresh_i_S]

        # Get top performing features
        topk_scores, topk_latents = torch.topk(f1_S, k=cfg.top_k)

        print(f"Top {cfg.top_k} features for {class_name}:")
        for score, latent in zip(topk_scores, topk_latents):
            print(f"{latent:>6} >{best_thresholds_S[latent]}: {score:.3f}")

        latent_lookup[class_id] = topk_latents[0].item()

    return latent_lookup


@jaxtyped(typechecker=beartype.beartype)
def get_patch_mask(
    pixel_labels_NP: UInt8[Tensor, "n patch_px"], threshold: float
) -> Bool[Tensor, " n"]:
    """
    Create a mask for patches where at least threshold proportion of pixels have the same label.

    Args:
        pixel_labels_NP: Tensor of shape [n, patch_pixels] with pixel labels
        threshold: Minimum proportion of pixels with same label

    Returns:
        Tensor of shape [n] with True for patches that pass the threshold
    """
    # For each patch, count occurrences of each unique label
    _, patch_pixels = pixel_labels_NP.shape

    mode_N = pixel_labels_NP.mode(axis=-1).values

    # Count occurrences of the mode value in each patch using vectorized operations
    counts_N = (pixel_labels_NP == mode_N[:, None]).sum(dim=1)

    # Calculate proportion and create mask
    mask_N = (counts_N / patch_pixels) >= threshold
    return mask_N


@jaxtyped(typechecker=beartype.beartype)
def register_hook(
    vit: torch.nn.Module,
    hook: Callable[[Float[Tensor, "..."]], Float[Tensor, "..."]],
    layer: int,
    n_patches_per_img: int,
):
    patches = vit.get_patches(n_patches_per_img)

    @jaxtyped(typechecker=beartype.beartype)
    def _hook(
        block: torch.nn.Module,
        inputs: tuple,
        outputs: Float[Tensor, "batch patches dim"],
    ) -> Float[Tensor, "batch patches dim"]:
        x = outputs[:, patches, :]
        x = hook(x)
        outputs[:, patches, :] = x
        return outputs

    return vit.get_residuals()[layer].register_forward_hook(_hook)


@jaxtyped(typechecker=beartype.beartype)
def compute_class_results(
    orig_preds: Int[Tensor, "n_imgs patches"], mod_preds: Int[Tensor, "n_imgs patches"]
) -> list[ClassResults]:
    class_results = []
    for class_id in range(1, 151):
        # Count original patches of this class
        orig_mask = orig_preds == class_id
        n_orig = orig_mask.sum().item()

        # Count how many changed
        changed_mask = (mod_preds != orig_preds) & orig_mask
        n_changed = changed_mask.sum().item()

        # Count changes in other patches
        other_mask = ~orig_mask
        n_other = other_mask.sum().item()
        n_other_changed = ((mod_preds != orig_preds) & other_mask).sum().item()

        # Track what classes patches changed to
        changes = {}
        for new_class in range(1, 151):
            if new_class != class_id:
                count = ((mod_preds == new_class) & changed_mask).sum().item()
                if count > 0:
                    changes[new_class] = count

        class_results.append(
            ClassResults(
                class_id=class_id,
                class_name="TODO",  # class_names[class_id],
                n_orig_patches=n_orig,
                n_changed_patches=n_changed,
                n_other_patches=n_other,
                n_other_changed=n_other_changed,
                change_distribution=changes,
            )
        )

    return class_results


@jaxtyped(typechecker=beartype.beartype)
def argmax_logits(
    logits_BPC: Float[Tensor, "batch patches channels_with_null"],
) -> Int[Tensor, "batch patches"]:
    return logits_BPC[:, :, 1:].argmax(axis=-1) + 1


@jaxtyped(typechecker=beartype.beartype)
def unscaled(
    x: Float[Tensor, "*batch"], max_obs: float | int
) -> Float[Tensor, "*batch"]:
    """Scale from [-10, 10] to [10 * -max_obs, 10 * max_obs]."""
    return map_range(x, (-10.0, 10.0), (-10.0 * max_obs, 10.0 * max_obs))


@jaxtyped(typechecker=beartype.beartype)
def map_range(
    x: Float[Tensor, "*batch"],
    domain: tuple[float | int, float | int],
    range: tuple[float | int, float | int],
) -> Float[Tensor, "*batch"]:
    a, b = domain
    c, d = range
    if not (a <= x <= b):
        raise ValueError(f"x={x:.3f} must be in {[a, b]}.")
    return c + (x - a) * (d - c) / (b - a)


@jaxtyped(typechecker=beartype.beartype)
def get_patch_i(
    i: Int[Tensor, "batch width height"], n_patches_per_img: int
) -> Int[Tensor, "batch width height"]:
    w = h = int(math.sqrt(n_patches_per_img))

    i = i * n_patches_per_img
    i = i + torch.arange(n_patches_per_img).view(w, h)
    return i

```

# contrib/semseg/__init__.py

```python
"""
Interpret and manipulate semantic segmentation models using SAEs.

.. include:: ./reproduce.md

The main entry point is `contrib/semseg/__main__.py`.
Run `uv run python -m contrib.semseg --help` to see all options.
"""

```

# contrib/semseg/training.py

```python
"""
Trains multiple linear probes in parallel on DINOv2's ADE20K activations.
"""

import collections
import dataclasses
import io
import json
import logging
import os.path
import re

import beartype
import einops
import torch
from jaxtyping import Float, Int, jaxtyped
from torch import Tensor
from torchvision.transforms import v2

import saev.activations
import saev.config
import saev.helpers
import saev.training

from . import config

logger = logging.getLogger(__name__)

n_classes = 151


@beartype.beartype
def main(cfgs: list[config.Train]):
    check_cfgs(cfgs)

    cfg = cfgs[0]
    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than float16 and almost as accurate as float32. This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    os.makedirs(cfg.ckpt_path, exist_ok=True)

    train_dataloader = get_dataloader(cfg, is_train=True)
    val_dataloader = get_dataloader(cfg, is_train=False)

    vit = DinoV2()
    vit = vit.to(cfg.device)
    models, params = make_models(cfgs, vit.d_resid)
    models = models.to(cfg.device)

    optim = torch.optim.AdamW(
        params, lr=cfg.learning_rate, weight_decay=cfg.weight_decay
    )

    global_step = 0

    for epoch in range(cfg.n_epochs):
        models.train()
        for batch in train_dataloader:
            imgs_BWHC = batch["image"].to(cfg.device)
            with torch.inference_mode():
                vit_acts = vit(imgs_BWHC)
                acts_BWHD = einops.rearrange(
                    vit_acts,
                    "batch (width height) dim -> batch width height dim",
                    width=16,
                    height=16,
                )

            patch_labels_BWH = batch["patch_labels"].to(cfg.device)
            patch_labels_MBWH = patch_labels_BWH.expand(len(cfgs), -1, -1, -1)
            logits_MBWHC = torch.stack([model(acts_BWHD) for model in models])
            loss = torch.nn.functional.cross_entropy(
                logits_MBWHC.view(-1, n_classes), patch_labels_MBWH.reshape(-1)
            )
            loss.backward()
            optim.step()
            optim.zero_grad()

            global_step += 1

        # Show last batch's loss and acc.
        acc_M = einops.reduce(
            (logits_MBWHC.argmax(axis=-1) == patch_labels_MBWH).float(),
            "models batch width height -> models",
            "mean",
        )
        logger.info(
            "epoch: %d, step: %d, mean train loss: %.5f, max train acc: %.3f",
            epoch,
            global_step,
            loss.item(),
            acc_M.max().item() * 100,
        )

        if epoch % cfg.eval_every == 0 or epoch + 1 == cfg.n_epochs:
            with torch.inference_mode():
                pred_label_list, true_label_list = [], []
                for batch in val_dataloader:
                    imgs_BWHC = batch["image"].to(cfg.device)
                    with torch.inference_mode():
                        vit_acts = vit(imgs_BWHC)
                        acts_BWHD = einops.rearrange(
                            vit_acts,
                            "batch (width height) dim -> batch width height dim",
                            width=16,
                            height=16,
                        )

                    pixel_labels_BWH = batch["pixel_labels"]
                    true_label_list.append(pixel_labels_BWH)

                    logits_MBWHC = torch.stack([model(acts_BWHD) for model in models])
                    logits_MB_CWH = einops.rearrange(
                        logits_MBWHC,
                        "models batch width height classes -> (models batch) classes width height",
                    )

                    pred_MB_WH = batched_upsample_and_pred(
                        logits_MB_CWH, size=(224, 224), mode="bilinear"
                    )
                    del logits_MB_CWH

                    pred_MBWH = einops.rearrange(
                        pred_MB_WH,
                        "(models batch) width height -> models batch width height",
                        models=len(models),
                    )
                    pred_label_list.append(pred_MBWH)

                pred_labels_MNWH = torch.cat(pred_label_list, dim=1).int()
                true_labels_MNWH = (
                    torch.cat(true_label_list).int().expand(len(models), -1, -1, -1)
                )

                logger.info("Evaluated all validation batchs.")
                class_ious_MC = get_class_ious(
                    pred_labels_MNWH,
                    true_labels_MNWH.expand(len(models), -1, -1, -1),
                    n_classes,
                )
                mean_ious_M = einops.reduce(
                    class_ious_MC, "models classes -> models", "mean"
                )
                acc_M = einops.reduce(
                    (pred_labels_MNWH == true_labels_MNWH).float(),
                    "models n width height -> models",
                    "mean",
                )

            logger.info(
                "epoch: %d, step: %d, max val miou: %.5f, max val acc: %.3f",
                epoch,
                global_step,
                mean_ious_M.max().item(),
                acc_M.max().item() * 100,
            )

            for cfg, model in zip(cfgs, models):
                dump(cfg, model, step=global_step)


@beartype.beartype
def dump(
    cfg: config.Train,
    model: torch.nn.Module,
    *,
    step: int | None = None,
):
    """
    Save a model checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).
    """
    dpath = os.path.join(
        cfg.ckpt_path,
        f"lr_{cfg.learning_rate}__wd_{cfg.weight_decay}".replace(".", "_"),
    )
    os.makedirs(dpath, exist_ok=True)

    kwargs = dict(in_features=model.in_features, out_features=model.out_features)

    fname = "model"
    if step is not None:
        fname += f"_step{step}"

    fpath = os.path.join(dpath, f"{fname}.pt")
    with open(fpath, "wb") as fd:
        kwargs_str = json.dumps(kwargs)
        fd.write((kwargs_str + "\n").encode("utf-8"))
        torch.save(model.state_dict(), fd)

    fpath = os.path.join(dpath, "cfg.json")
    with open(fpath, "w") as fd:
        json.dump(dataclasses.asdict(cfg), fd)


@beartype.beartype
def load_latest(dpath: str, *, device: str = "cpu") -> torch.nn.Module:
    """
    Loads the latest checkpoint by picking out the checkpoint file in dpath with the largest _step{step} suffix.

    Arguments:
        dpath: Directory to search.
        device: optional torch device to pass to load.
    """
    if not os.path.exists(dpath):
        raise FileNotFoundError(f"Directory not found: {dpath}")

    # Find all .pt files
    pt_files = [f for f in os.listdir(dpath) if f.endswith(".pt")]
    if not pt_files:
        raise FileNotFoundError(f"No .pt files found in {dpath}")

    # Extract step numbers using regex
    step_pattern = re.compile(r"_step(\d+)\.pt$")
    latest_step = -1
    latest_file = None

    for fname in pt_files:
        match = step_pattern.search(fname)
        if match:
            step = int(match.group(1))
            if step > latest_step:
                latest_step = step
                latest_file = fname

    if latest_file is None:
        # If no files with _step found, just take the first .pt file
        latest_file = pt_files[0]
        logger.warning(f"No checkpoint files with _step found, using: {latest_file}")

    fpath = os.path.join(dpath, latest_file)
    ckpt = load(fpath, device=device)
    logger.info("Loaded checkpoint: %s", fpath)
    return ckpt


@beartype.beartype
def load(fpath: str, *, device: str = "cpu") -> torch.nn.Module:
    """
    Loads a sparse autoencoder from disk.
    """
    with open(fpath, "rb") as fd:
        kwargs = json.loads(fd.readline().decode())
        buffer = io.BytesIO(fd.read())

    model = torch.nn.Linear(**kwargs)
    state_dict = torch.load(buffer, weights_only=True, map_location=device)
    model.load_state_dict(state_dict)
    model = model.to(device)
    return model


CANNOT_PARALLELIZE = set([
    "n_epochs",
    "batch_size",
    "n_workers",
    "imgs",
    "eval_every",
    "device",
])


@beartype.beartype
def check_cfgs(cfgs: list[config.Train]):
    # Check that any differences in configs are supported by our parallel training run.
    seen = collections.defaultdict(list)
    for cfg in cfgs:
        for key, value in vars(cfg).items():
            seen[key].append(value)

    bad_keys = {}
    for key, values in seen.items():
        if key in CANNOT_PARALLELIZE and len(set(values)) != 1:
            bad_keys[key] = values

    if bad_keys:
        msg = ", ".join(f"'{key}': {values}" for key, values in bad_keys.items())
        raise ValueError(f"Cannot parallelize training over: {msg}")


@beartype.beartype
def make_models(
    cfgs: list[config.Train], d_vit: int
) -> tuple[torch.nn.ModuleList, list[dict[str, object]]]:
    param_groups = []
    models = []
    for cfg in cfgs:
        model = torch.nn.Linear(d_vit, n_classes)
        models.append(model)
        # Use an empty LR because our first step is warmup.
        param_groups.append({
            "params": model.parameters(),
            "lr": cfg.learning_rate,
            "weight_decay": cfg.weight_decay,
        })

    return torch.nn.ModuleList(models), param_groups


@jaxtyped(typechecker=beartype.beartype)
class DinoV2(torch.nn.Module):
    def __init__(self):
        super().__init__()

        self.model = torch.hub.load("facebookresearch/dinov2", "dinov2_vitb14_reg")
        self.d_resid = 768

    def forward(self, batch: Float[Tensor, "batch 3 width height"]):
        dct = self.model.forward_features(batch)

        return dct["x_norm_patchtokens"]


def get_dataloader(cfg: config.Train, *, is_train: bool):
    if is_train:
        shuffle = True
        dataset = Dataset(dataclasses.replace(cfg.imgs, split="training"))
    else:
        shuffle = False
        dataset = Dataset(dataclasses.replace(cfg.imgs, split="validation"))

    return torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        shuffle=shuffle,
        persistent_workers=(cfg.n_workers > 0),
    )


@beartype.beartype
class Dataset(torch.utils.data.Dataset):
    def __init__(self, imgs_cfg: saev.config.Ade20kDataset):
        img_transform = v2.Compose([
            v2.Resize(size=(256, 256)),
            v2.CenterCrop(size=(224, 224)),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),
        ])
        seg_transform = v2.Compose([
            v2.Resize(size=(256, 256), interpolation=v2.InterpolationMode.NEAREST),
            v2.CenterCrop((224, 224)),
            v2.ToImage(),
        ])
        self.samples = saev.activations.Ade20k(
            imgs_cfg, img_transform=img_transform, seg_transform=seg_transform
        )

        self.patch_size_px = (14, 14)

    def __getitem__(self, i: int) -> dict[str, object]:
        # Get patch and pixel level semantic labels.
        sample = self.samples[i]
        pixel_labels = sample["segmentation"].squeeze()

        pw, ph = self.patch_size_px
        patch_labels = (
            einops.rearrange(pixel_labels, "(w pw) (h ph) -> w h (pw ph)", pw=pw, ph=ph)
            .mode(axis=-1)
            .values
        )

        return {
            "index": i,
            "image": sample["image"],
            "pixel_labels": pixel_labels,
            "patch_labels": patch_labels,
        }

    def __len__(self) -> int:
        return len(self.samples)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_class_ious(
    y_pred: Int[Tensor, "models batch width height"],
    y_true: Int[Tensor, "models batch width height"],
    n_classes: int,
    ignore_class: int | None = 0,
) -> Float[Tensor, " models n_classes"]:
    """
    Calculate mean IoU for predicted masks.

    Arguments:
        y_pred:
        y_true:
        n_classes: Number of classes.

    Returns:
        Mean IoU as a float tensor.
    """

    mious = []
    for y_pred_, y_true_ in zip(y_pred, y_true):
        # Convert to one-hot encoded format
        pred_one_hot = torch.nn.functional.one_hot(y_pred_.long(), n_classes)
        true_one_hot = torch.nn.functional.one_hot(y_true_.long(), n_classes)

        if ignore_class is not None:
            if ignore_class == 0:
                pred_one_hot = pred_one_hot[..., 1:]
                true_one_hot = true_one_hot[..., 1:]
            else:
                pred_one_hot = torch.cat(
                    (
                        pred_one_hot[..., :ignore_class],
                        pred_one_hot[..., ignore_class + 1 :],
                    ),
                    axis=-1,
                )
                true_one_hot = torch.cat(
                    (
                        true_one_hot[..., :ignore_class],
                        true_one_hot[..., ignore_class + 1 :],
                    ),
                    axis=-1,
                )
        logger.info(
            "Got one-hot encodings for inputs (ignore_class='%s').", ignore_class
        )

        # Calculate intersection and union for all classes at once
        intersection = einops.reduce(
            torch.logical_and(pred_one_hot, true_one_hot), "n w h c -> c", "sum"
        )
        union = einops.reduce(
            torch.logical_or(pred_one_hot, true_one_hot), "n w h c -> c", "sum"
        )
        logger.info("Got intersection and union.")

        # Check for division by zero
        if (union == 0).any():
            logger.warning(
                "At least one class is not present in data: '%s'.",
                torch.nonzero(union == 0),
            )

        miou = intersection / union
        mious.append(miou)

    return torch.stack(mious)


@jaxtyped(typechecker=beartype.beartype)
def batched_upsample_and_pred(
    tensor: Float[Tensor, "n channels width height"],
    *,
    size: tuple[int, int],
    mode: str,
    batch_size: int = 128,
) -> Int[Tensor, "n {size[0]} {size[1]}"]:
    preds = []

    for start, end in batched_idx(len(tensor), batch_size):
        upsampled_BCWH = torch.nn.functional.interpolate(
            tensor[start:end].contiguous(), size=size, mode=mode
        )
        pred_BWH = upsampled_BCWH.argmax(axis=1).cpu()
        del upsampled_BCWH
        preds.append(pred_BWH)

    return torch.cat(preds)


@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -> collections.abc.Iterator[tuple[int, int]]:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop


@beartype.beartype
def count_patches(
    ade20k: saev.config.Ade20kDataset,
    patch_size_px: tuple[int, int] = (14, 14),
    threshold: float = 0.9,
    n_workers: int = 8,
):
    """
    Count the number of patches in the data that meets
    """
    torch.use_deterministic_algorithms(True)

    pw, ph = patch_size_px
    to_array = v2.Compose([
        v2.Resize(256, interpolation=v2.InterpolationMode.NEAREST),
        v2.CenterCrop((224, 224)),
        v2.ToImage(),
    ])
    dataset = saev.activations.Ade20k(
        ade20k, img_transform=to_array, seg_transform=to_array
    )
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=1024,
        num_workers=n_workers,
        shuffle=False,
    )
    n_class_patches = torch.zeros((n_classes,), dtype=int)
    for batch in saev.helpers.progress(dataloader):
        pixel_labels = einops.rearrange(
            batch["segmentation"],
            "batch () (w pw) (h ph) -> batch w h (pw ph)",
            pw=pw,
            ph=ph,
        )
        patch_labels = pixel_labels.mode(axis=-1).values
        matches = (
            einops.reduce(
                (pixel_labels == patch_labels[..., None]).float(),
                "batch w h patch -> batch w h",
                "mean",
            )
            > threshold
        )
        n_class_patches.scatter_add_(
            0, patch_labels.view(-1).to(int), matches.view(-1).to(int)
        )

    print(
        f"Mean: {n_class_patches.float().mean().item():.1f}, max: {n_class_patches.max().item()}, min: {n_class_patches.min().item()}"
    )


if __name__ == "__main__":
    import tyro

    tyro.cli(count_patches)

```

# contrib/semseg/validation.py

```python
"""
Checks which checkpoints have the best validation loss, mean IoU, class-specific IoU, validation accuracy, and qualitative results.

Writes results to CSV files and hparam graphs (in-progress).
"""

import csv
import json
import logging
import os

import altair as alt
import beartype
import einops
import polars as pl
import torch

import saev.helpers

from . import config, training

logger = logging.getLogger(__name__)


@beartype.beartype
def main(cfg: config.Validation):
    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than float16 and almost as accurate as float32. This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
    dataset = training.Dataset(cfg.imgs)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        shuffle=False,
        persistent_workers=(cfg.n_workers > 0),
    )

    ckpts = load_ckpts(cfg.ckpt_root, device=cfg.device)

    vit = training.DinoV2()
    vit = vit.to(cfg.device)
    pred_label_list, true_label_list = [], []
    for batch in saev.helpers.progress(dataloader, every=1):
        imgs_BWHC = batch["image"].to(cfg.device)
        with torch.inference_mode():
            vit_acts = vit(imgs_BWHC)
            acts_BWHD = einops.rearrange(
                vit_acts,
                "batch (width height) dim -> batch width height dim",
                width=16,
                height=16,
            )
        pixel_labels_BWH = batch["pixel_labels"]
        true_label_list.append(pixel_labels_BWH)

        logits_MBWHC = torch.stack([model(acts_BWHD) for _, model in ckpts])
        logits_MB_CWH = einops.rearrange(
            logits_MBWHC,
            "models batch width height classes -> (models batch) classes width height",
        )

        pred_MB_WH = training.batched_upsample_and_pred(
            logits_MB_CWH, size=(224, 224), mode="bilinear"
        )
        del logits_MB_CWH

        pred_MBWH = einops.rearrange(
            pred_MB_WH,
            "(models batch) width height -> models batch width height",
            models=len(ckpts),
        )
        pred_label_list.append(pred_MBWH)

    pred_labels_MNWH = torch.cat(pred_label_list, dim=1).int()
    true_labels_MNWH = torch.cat(true_label_list).int().expand(len(ckpts), -1, -1, -1)

    logger.info("Evaluated all validation batchs.")
    ious_MC = training.get_class_ious(
        pred_labels_MNWH,
        true_labels_MNWH.expand(len(ckpts), -1, -1, -1),
        training.n_classes,
    )
    acc_M = (pred_labels_MNWH == true_labels_MNWH).float().mean(dim=(1, 2, 3)) * 100

    lookup = {}
    with open(os.path.join(cfg.imgs.root, "objectInfo150.txt")) as fd:
        for row in csv.DictReader(fd, delimiter="\t"):
            lookup[int(row["Idx"])] = row["Name"]

    class_iou_headers = [name for _, name in sorted(lookup.items())]

    os.makedirs(cfg.dump_to, exist_ok=True)

    # Save CSV file
    csv_fpath = os.path.join(cfg.dump_to, "results.csv")
    with open(csv_fpath, "w") as fd:
        writer = csv.writer(fd)
        writer.writerow(
            ["learning_rate", "weight_decay", "acc", "mean_iou"] + class_iou_headers
        )
        for (c, _), acc, ious_C in zip(ckpts, acc_M, ious_MC):
            writer.writerow(
                [c.learning_rate, c.weight_decay, acc.item(), ious_C.mean().item()]
                + ious_C.tolist()
            )

    # Save hyperparameter sweep charts
    df = pl.read_csv(csv_fpath).with_columns(
        pl.col("weight_decay").add(1e-9).alias("weight_decay")
    )
    alt.Chart(df).mark_point().encode(
        alt.X(alt.repeat("column"), type="quantitative").scale(type="log"),
        alt.Y(alt.repeat("row"), type="quantitative").scale(zero=False),
    ).repeat(row=["acc", "mean_iou"], column=["learning_rate", "weight_decay"]).save(
        os.path.join(cfg.dump_to, "hparam-sweeps.png")
    )


@beartype.beartype
def load_ckpts(
    root: str, *, device: str = "cpu"
) -> list[tuple[config.Train, torch.nn.Module]]:
    """
    Loads the latest checkpoints for each directory within root.

    Arguments:
        root: directory containing other directories with cfg.json and model_step{step}.pt files.
        device: where to load models.

    Returns:
        List of cfg, model pairs.
    """
    if not os.path.exists(root):
        raise FileNotFoundError(f"Checkpoint root not found: {root}")

    results = []

    # Find all subdirectories that contain cfg.json
    for dname in os.listdir(root):
        dpath = os.path.join(root, dname)
        if not os.path.isdir(dpath):
            continue

        cfg_path = os.path.join(dpath, "cfg.json")
        if not os.path.exists(cfg_path):
            continue

        # Load config
        with open(cfg_path) as f:
            cfg_dict = json.load(f)
        # Handle the nested dataclasses.
        cfg_dict["imgs"] = saev.config.Ade20kDataset(**cfg_dict["imgs"])
        cfg_dict.pop("patch_size_px")
        cfg = config.Train(**cfg_dict)

        # Load latest model checkpoint
        model = training.load_latest(dpath, device=device)

        results.append((cfg, model))

    if not results:
        raise FileNotFoundError(f"No valid checkpoint directories found in: {root}")

    return results

```

# contrib/semseg/__main__.py

```python
import logging
import tomllib
import typing

import beartype
import tyro

from . import config

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)

logger = logging.getLogger("contrib.semseg")


@beartype.beartype
def train(
    cfg: typing.Annotated[config.Train, tyro.conf.arg(name="")],
    sweep: str | None = None,
):
    """
    Trains one or more linear probes in parallel on DINOv2 activations over ADE20K.
    """
    import submitit

    from . import training

    if sweep is not None:
        with open(sweep, "rb") as fd:
            cfgs, errs = config.grid(cfg, tomllib.load(fd))

        if errs:
            for err in errs:
                logger.warning("Error in config: %s", err)
            return

    else:
        cfgs = [cfg]

    logger.info("Running %d training jobs.", len(cfgs))

    executor = submitit.DebugExecutor(folder=cfg.log_to)

    job = executor.submit(training.main, cfgs)
    job.result()


@beartype.beartype
def visuals(cfg: typing.Annotated[config.Visuals, tyro.conf.arg(name="")]):
    from . import visuals

    visuals.main(cfg)


@beartype.beartype
def validate(cfg: typing.Annotated[config.Validation, tyro.conf.arg(name="")]):
    """
    Runs validation and reports best hyperparameters in the logs/contrib/semseg folder.
    """
    from . import validation

    validation.main(cfg)


@beartype.beartype
def quantify(cfg: typing.Annotated[config.Quantitative, tyro.conf.arg(name="")]):
    from . import quantitative

    quantitative.main(cfg)


if __name__ == "__main__":
    tyro.extras.subcommand_cli_from_dict({
        "train": train,
        "visuals": visuals,
        "validate": validate,
        "quantify": quantify,
    })

```

# contrib/semseg/interactive/feature_analysis.py

```python
import marimo

__generated_with = "0.9.20"
app = marimo.App(width="full")


@app.cell
def __():
    import einops
    import marimo as mo

    return einops, mo


@app.cell
def __():
    import contrib.semseg.training
    import saev.activations
    import saev.config

    return contrib, saev


@app.cell
def __():
    import beartype
    import torch
    from jaxtyping import Float, Int, jaxtyped

    return Float, Int, beartype, jaxtyped, torch


@app.cell
def __():
    import numpy as np

    return (np,)


@app.cell
def __(saev):
    sae_ckpt_fpath = "/home/stevens.994/projects/saev-live/checkpoints/ercgckr1/sae.pt"
    sae = saev.nn.load(sae_ckpt_fpath)
    return sae, sae_ckpt_fpath


@app.cell
def __(mo):
    mo.md(
        r"""Use this dashboard to find feature vectors for ADE20K classes (see the README.md for complete details)."""
    )
    return


@app.cell
def __():
    image_size_px = (224, 224)
    patch_size_px = (16, 16)
    threshold = 0.9
    return image_size_px, patch_size_px, threshold


@app.cell
def __(mo):
    mo.md(
        r"""
        For a given class $c$, we pick the SAE feature that activates on the most number of activations labeled $c$, while not activating on the most number of activations labeled $c_i \neq c$.

        We measure this with F1.
        """
    )
    return


@app.cell
def __(
    Float,
    Int,
    beartype,
    cls_lookup,
    cls_select,
    jaxtyped,
    np,
    sae_ckpt_fpath,
    saev,
    torch,
):
    @jaxtyped(typechecker=beartype.beartype)
    @torch.no_grad
    def get_feature_directions(
        acts: Float[np.ndarray, "n d_vit"], labels: Int[np.ndarray, " n"], *, k: int = 2
    ) -> tuple[Int[np.ndarray, "n_unique k"], Float[np.ndarray, "n_unique k d_vit"]]:
        """
        Get the k most meaningful features for each unique obj_cls.
        """
        sae = saev.nn.load(sae_ckpt_fpath)

        acts_pt = torch.from_numpy(acts)
        _, f_x, loss = sae(acts_pt)

        out_idxs, out_dirs = [], []
        for obj_cls in cls_select.value:
            i = labels == cls_lookup[obj_cls]
            vals, idxs = f_x[i].topk(32)
            idxs, counts = np.unique(idxs.numpy(), return_counts=True)
            top = list(reversed(np.argsort(counts)[-k:]))
            # print(len(top), idxs.shape, idxs[top])

            out_idxs.append(idxs[top])
            out_dirs.append(sae.W_dec[idxs[top]].numpy())
        # print(out_dirs)
        return np.array(out_idxs), np.array(out_dirs)

    # feature_idxs, feature_dirs = get_feature_directions(
    #     activations[without_outliers], labels[without_outliers]
    # )
    return (get_feature_directions,)


@app.cell
def __(contrib, saev, torch):
    data_cfg = saev.config.DataLoad(
        shard_root="/local/scratch/stevens.994/cache/saev/a860104bf29d6093dd18b8e2dccd2e7efdfcd9fac35dceb932795af05187cb9f",
        scale_mean=False,
        scale_norm=False,
    )
    imgs_cfg = saev.config.Ade20kDataset(
        root="/research/nfs_su_809/workspace/stevens.994/datasets/ade20k/"
    )
    dataset = contrib.semseg.training.Dataset(data_cfg, imgs_cfg)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=128, num_workers=8, shuffle=False, persistent_workers=False
    )
    return data_cfg, dataloader, dataset, imgs_cfg


@app.cell
def __(dataset):
    dataset.acts.metadata
    return


@app.cell
def __(dataloader):
    batch = next(iter(dataloader))
    return (batch,)


@app.cell
def __(unique):
    unique.shape
    return


@app.cell
def __(axis_unique, batch, einops, threshold):
    pixel_labels = einops.rearrange(
        batch["pixel_labels"],
        "batch (w pw) (h ph) -> batch w h (pw ph)",
        pw=16,
        ph=16,
    ).numpy()
    unique, counts = axis_unique(
        pixel_labels, axis=-1, return_counts=True, null_value=0
    )

    idx = counts[:, :, :, 0] > (256 * threshold)
    acts = batch["acts"][idx]
    labels = unique[idx][:, 0]
    return acts, counts, idx, labels, pixel_labels, unique


@app.cell
def __(acts):
    acts.shape
    return


@app.cell
def __(labels):
    labels.shape
    return


@app.cell
def __(acts, sae, torch):
    with torch.no_grad():
        _, f_x, loss = sae(acts)
    return f_x, loss


@app.cell
def __(f_x):
    f_x.shape
    return


@app.cell
def __(labels):
    labels[labels == 1].shape
    return


@app.cell
def __(f_x, labels):
    f_x[labels == 1].shape
    return


@app.cell
def __(f_x):
    pred_pos = f_x > 0
    return (pred_pos,)


@app.cell
def __(pred_pos):
    pred_pos.sum(axis=0)
    return


@app.cell
def __(labels):
    true_pos = (labels == 1).reshape(-1, 1)
    return (true_pos,)


@app.cell
def __(pred_pos, true_pos):
    tp = (pred_pos & true_pos).sum(axis=0)
    tp
    return (tp,)


@app.cell
def __(pred_pos, true_pos):
    fp = (pred_pos & ~true_pos).sum(axis=0)
    fp
    return (fp,)


@app.cell
def __(pred_pos, true_pos):
    fn = (~pred_pos & true_pos).sum(axis=0)
    fn
    return (fn,)


@app.cell
def __(true_pos):
    true_pos.sum()
    return


@app.cell
def __(fn, fp, tp):
    f1 = (2 * tp) / (2 * tp + fp + fn)
    f1.topk(5)
    return (f1,)


@app.cell
def __(loss):
    loss
    return


@app.cell
def __(Shaped, np):
    def axis_unique(
        a: Shaped[np.ndarray, "*axes"],
        axis: int | None = None,
        return_counts: bool = False,
        *,
        null_value: int = -1,
    ):
        """
        Calculate unique values and their counts along any axis of a matrix.

        Arguments:
            a: Input array
            axis: The axis along which to find unique values. If None, the flattened array is used.
            return_counts: If true, also return the count of each unique value

        Returns:
            unique: Array of unique values, with zeros replacing duplicates
            counts: (optional) Count of each unique value (only if return_counts=True)
        """
        if axis is None:
            # Handle flattened array case
            if return_counts:
                unique, counts = np.unique(a, return_counts=True)
                return unique, counts
            return np.unique(a)

        # Move the target axis to the end for consistent processing
        a_transformed = np.moveaxis(a, axis, -1)

        # Sort along the last axis
        sorted_a = np.sort(a_transformed, axis=-1)

        # Find duplicates
        duplicates = sorted_a[..., 1:] == sorted_a[..., :-1]

        # Create output array
        unique = sorted_a.copy()
        unique[..., 1:][duplicates] = null_value

        if not return_counts:
            # Move axis back to original position
            return np.moveaxis(unique, -1, axis)

        # Calculate counts
        shape = list(a_transformed.shape)
        count_matrix = np.zeros(shape, dtype=int)

        # Process each slice along other dimensions
        for idx in np.ndindex(*shape[:-1]):
            slice_unique = unique[idx]
            idxs = np.flatnonzero(slice_unique)
            if len(idxs) > 0:
                # Calculate counts using diff for intermediate positions
                counts = np.diff(idxs)
                count_matrix[idx][idxs[:-1]] = counts
                # Handle the last unique value
                count_matrix[idx][idxs[-1]] = shape[-1] - idxs[-1]

        # Move axes back to original positions
        unique = np.moveaxis(unique, -1, axis)
        count_matrix = np.moveaxis(count_matrix, -1, axis)

        return unique, count_matrix

    return (axis_unique,)


@app.cell
def __():
    return


if __name__ == "__main__":
    app.run()

```

# contrib/classification/__main__.py

```python
import logging
import tomllib
import typing

import beartype
import tyro

from . import config

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger("contrib.semseg")


@beartype.beartype
def train(
    cfg: typing.Annotated[config.Train, tyro.conf.arg(name="")],
    sweep: str | None = None,
):
    import submitit

    from . import training

    if sweep is not None:
        with open(sweep, "rb") as fd:
            cfgs, errs = config.grid(cfg, tomllib.load(fd))

        if errs:
            for err in errs:
                logger.warning("Error in config: %s", err)
            return

    else:
        cfgs = [cfg]

    logger.info("Running %d training jobs.", len(cfgs))

    executor = submitit.DebugExecutor(folder=cfg.log_to)

    job = executor.submit(training.main, cfgs)
    job.result()


if __name__ == "__main__":
    tyro.cli(train)

```

# contrib/classification/__init__.py

```python
"""
.. include:: ./reproduce.md
"""

```

# contrib/classification/config.py

```python
import dataclasses
import os

import beartype

import saev.config


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Train:
    learning_rate: float = 1e-4
    """Linear layer learning rate."""
    weight_decay: float = 1e-3
    """Weight decay  for AdamW."""
    n_epochs: int = 20
    """Number of training epochs for linear layer."""
    batch_size: int = 512
    """Training batch size."""
    n_workers: int = 32
    """Number of dataloader workers."""
    train_imgs: saev.config.ImageFolderDataset = dataclasses.field(
        default_factory=saev.config.ImageFolderDataset
    )
    """Configuration for the training images."""
    val_imgs: saev.config.ImageFolderDataset = dataclasses.field(
        default_factory=saev.config.ImageFolderDataset
    )
    """Configuration for the validation images."""
    device: str = "cuda"
    "Hardware to train on."
    ckpt_path: str = os.path.join(".", "checkpoints", "contrib", "classification")
    seed: int = 42
    """Random seed."""
    log_to: str = os.path.join(".", "logs", "contrib", "classification")


@beartype.beartype
def grid(cfg: Train, sweep_dct: dict[str, object]) -> tuple[list[Train], list[str]]:
    cfgs, errs = [], []
    for d, dct in enumerate(saev.config.expand(sweep_dct)):
        try:
            cfgs.append(dataclasses.replace(cfg, **dct, seed=cfg.seed + d))
        except Exception as err:
            errs.append(str(err))

    return cfgs, errs

```

# contrib/classification/training.py

```python
"""
Train a linear probe on [CLS] activations from a ViT.
"""

import collections
import csv
import dataclasses
import io
import json
import logging
import os

import altair as alt
import beartype
import einops
import polars as pl
import torch
from jaxtyping import Float, Int, jaxtyped
from torch import Tensor

import saev.activations

from . import config, transforms

logger = logging.getLogger(__name__)


model_ckpt = "ViT-B-16/openai"
d_repr = 768


@beartype.beartype
def main(cfgs: list[config.Train]):
    check_cfgs(cfgs)
    cfg = cfgs[0]

    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than
        # float16 and almost as accurate as float32
        # This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    os.makedirs(cfg.ckpt_path, exist_ok=True)

    train_dataloader = get_dataloader(cfg, is_train=True)
    val_dataloader = get_dataloader(cfg, is_train=False)

    assert len(train_dataloader.dataset.classes) == len(val_dataloader.dataset.classes)
    n_classes = len(train_dataloader.dataset.classes)

    vit = saev.activations.Clip(model_ckpt)
    vit = vit.to(cfg.device)

    models, params = make_models(cfgs, n_classes)
    models = models.to(cfg.device)
    optim = torch.optim.AdamW(
        params, lr=cfg.learning_rate, weight_decay=cfg.weight_decay
    )

    global_step = 0
    for epoch in range(cfg.n_epochs):
        models.train()
        for batch in train_dataloader:
            imgs_BWHC = batch["image"].to(cfg.device)
            with torch.no_grad():
                vit_acts = vit(imgs_BWHC)
                acts_BD = vit_acts["cls"]
            logits_MBC = torch.stack([model(acts_BD) for model in models])
            targets_B = batch["target"].to(cfg.device)
            loss = torch.nn.functional.cross_entropy(
                logits_MBC.view(-1, n_classes),
                targets_B.expand(len(cfgs), -1).reshape(-1),
            )
            loss.backward()
            optim.step()
            optim.zero_grad()
            global_step += 1
        train_acc_M = (
            (logits_MBC.argmax(axis=-1) == targets_B.expand(len(cfgs), -1))
            .float()
            .mean(axis=1)
        )

        models.eval()
        with torch.inference_mode():
            pred_tgt_list, true_tgt_list = [], []
            for batch in val_dataloader:
                imgs_BWHC = batch["image"].to(cfg.device)
                with torch.no_grad():
                    vit_acts = vit(imgs_BWHC)
                    acts_BD = vit_acts["cls"]
                logits_MBC = torch.stack([model(acts_BD) for model in models])
                pred_tgt_list.append(logits_MBC.argmax(axis=-1).cpu())
                true_tgt_list.append(batch["target"])

            pred_tgt_MN = torch.cat(pred_tgt_list, dim=1).int()
            true_tgt_MN = torch.cat(true_tgt_list).int().expand(len(models), -1)
            val_acc_M = einops.reduce(
                (pred_tgt_MN == true_tgt_MN).float(), "models n -> models", "mean"
            )

        logger.info(
            "epoch: %d, step: %d, max train acc: %.5f, max val acc: %.3f",
            epoch,
            global_step,
            train_acc_M.max().item() * 100,
            val_acc_M.max().item() * 100,
        )

    for cfg, model in zip(cfgs, models):
        dump_model(cfg, model)

    os.makedirs(cfg.log_to, exist_ok=True)

    # Save CSV file
    csv_fpath = os.path.join(cfg.log_to, "results.csv")
    with open(csv_fpath, "w") as fd:
        writer = csv.writer(fd)
        writer.writerow(["learning_rate", "weight_decay", "train_acc", "val_acc"])
        for c, train_acc, val_acc in zip(
            cfgs, train_acc_M.tolist(), val_acc_M.tolist()
        ):
            writer.writerow([c.learning_rate, c.weight_decay, train_acc, val_acc])

    # Save hyperparameter sweep charts
    df = pl.read_csv(csv_fpath).with_columns(
        pl.col("weight_decay").add(1e-9).alias("weight_decay")
    )
    alt.Chart(df).mark_point().encode(
        alt.X(alt.repeat("column"), type="quantitative").scale(type="log"),
        alt.Y(alt.repeat("row"), type="quantitative").scale(zero=False),
    ).repeat(
        row=["train_acc", "val_acc"], column=["learning_rate", "weight_decay"]
    ).save(os.path.join(cfg.log_to, "hparam-sweeps.png"))


@beartype.beartype
def dump_model(cfg: config.Train, model: torch.nn.Module):
    """
    Save a model checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).
    """
    dpath = os.path.join(
        cfg.ckpt_path,
        f"lr_{cfg.learning_rate}__wd_{cfg.weight_decay}".replace(".", "_"),
    )
    os.makedirs(dpath, exist_ok=True)

    kwargs = dict(in_features=model.in_features, out_features=model.out_features)

    fpath = os.path.join(dpath, "model.pt")
    with open(fpath, "wb") as fd:
        kwargs_str = json.dumps(kwargs)
        fd.write((kwargs_str + "\n").encode("utf-8"))
        torch.save(model.state_dict(), fd)

    fpath = os.path.join(dpath, "cfg.json")
    with open(fpath, "w") as fd:
        json.dump(dataclasses.asdict(cfg), fd)


@beartype.beartype
def load_model(fpath: str, *, device: str = "cpu") -> torch.nn.Module:
    """
    Loads a linear layer from disk.
    """
    with open(fpath, "rb") as fd:
        kwargs = json.loads(fd.readline().decode())
        buffer = io.BytesIO(fd.read())

    model = torch.nn.Linear(**kwargs)
    state_dict = torch.load(buffer, weights_only=True, map_location=device)
    model.load_state_dict(state_dict)
    model = model.to(device)
    return model


@jaxtyped(typechecker=beartype.beartype)
def load_acts(cfg: saev.config.DataLoad) -> Float[Tensor, "n d_vit"]:
    dataset = saev.activations.Dataset(cfg)
    acts = torch.stack([dataset[i]["act"] for i in range(len(dataset))])
    return acts


@jaxtyped(typechecker=beartype.beartype)
def load_targets(cfg: saev.config.ImageFolderDataset) -> Int[Tensor, " n"]:
    dataset = saev.activations.ImageFolder(cfg.root)
    targets = torch.tensor([tgt for sample, tgt in dataset.samples])
    return targets


@jaxtyped(typechecker=beartype.beartype)
def load_class_headers(cfg: saev.config.ImageFolderDataset) -> list[str]:
    dataset = saev.activations.ImageFolder(cfg.root)
    unique_targets = sorted(set([tgt for sample, tgt in dataset.samples]))
    labels = [dataset.classes[tgt] for tgt in unique_targets]
    return labels


@beartype.beartype
class Dataset(torch.utils.data.Dataset):
    def __init__(
        self, acts_cfg: saev.config.DataLoad, imgs_cfg: saev.config.ImageFolderDataset
    ):
        self.acts = saev.activations.Dataset(acts_cfg)

        img_dataset = saev.activations.ImageFolder(imgs_cfg.root)
        self.targets = [tgt for sample, tgt in img_dataset.samples]
        self.labels = [img_dataset.classes[tgt] for tgt in self.targets]

    @property
    def d_vit(self) -> int:
        return self.acts.metadata.d_vit

    @property
    def n_classes(self) -> int:
        return len(set(self.targets))

    def __getitem__(self, i: int) -> dict[str, object]:
        act_D = self.acts[i]["act"]
        label = self.labels[i]
        target = self.targets[i]

        return {"index": i, "acts": act_D, "labels": label, "targets": target}

    def __len__(self) -> int:
        assert len(self.acts) == len(self.targets)
        return len(self.targets)


CANNOT_PARALLELIZE = set([
    "n_epochs",
    "batch_size",
    "n_workers",
    "train_acts",
    "val_acts",
    "imgs",
    "eval_every",
    "device",
])


@beartype.beartype
def check_cfgs(cfgs: list[config.Train]):
    # Check that any differences in configs are supported by our parallel training run.
    seen = collections.defaultdict(list)
    for cfg in cfgs:
        for key, value in vars(cfg).items():
            seen[key].append(value)

    bad_keys = {}
    for key, values in seen.items():
        if key in CANNOT_PARALLELIZE and len(set(values)) != 1:
            bad_keys[key] = values

    if bad_keys:
        msg = ", ".join(f"'{key}': {values}" for key, values in bad_keys.items())
        raise ValueError(f"Cannot parallelize training over: {msg}")


@beartype.beartype
def make_models(
    cfgs: list[config.Train], d_out: int
) -> tuple[torch.nn.ModuleList, list[dict[str, object]]]:
    param_groups = []
    models = []
    for cfg in cfgs:
        model = torch.nn.Linear(d_repr, d_out)
        models.append(model)
        # Use an empty LR because our first step is warmup.
        param_groups.append({
            "params": model.parameters(),
            "lr": cfg.learning_rate,
            "weight_decay": cfg.weight_decay,
        })

    return torch.nn.ModuleList(models), param_groups


def get_dataloader(cfg: config.Train, *, is_train: bool):
    img_transform = transforms.for_training(model_ckpt)

    if is_train:
        shuffle = True

        dataset = saev.activations.ImageFolder(
            cfg.train_imgs.root, transform=img_transform
        )
    else:
        shuffle = False
        dataset = saev.activations.ImageFolder(
            cfg.val_imgs.root, transform=img_transform
        )

    return torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        shuffle=shuffle,
        persistent_workers=(cfg.n_workers > 0),
    )

```

# contrib/classification/transforms.py

```python
"""
Contains the transforms used in every spot:

* Training
* Making figures
* Web app

For both figures and the webapp, the transform is:

1. Resize the image so that the shortest size is 512 pixels.
2. Take the middle 448x448 as a crop

This gives us object-centric images that are 448x448 (fixed pixel size is important for the web app) and not distorted.
"""

from PIL import Image


def for_training(vit_ckpt: str):
    import saev.activations

    return saev.activations.make_img_transform("clip", vit_ckpt)


def for_figures():
    import einops.layers.torch
    from torchvision.transforms import v2

    return v2.Compose([
        v2.Resize(512, interpolation=v2.InterpolationMode.BICUBIC),
        v2.CenterCrop((448, 448)),
        v2.ToImage(),
        einops.layers.torch.Rearrange("channels width height -> width height channels"),
    ])


def for_webapp(img: Image.Image) -> Image.Image:
    w, h = img.size
    if w > h:
        resize_w = w * 512 / h
        resize_px = (resize_w, 512)

        margin_x = (resize_w - 448) / 2
        crop_px = (margin_x, 32, 448 + margin_x, 480)
    else:
        resize_h = h * 512 / w
        resize_px = (512, resize_h)
        margin_y = (resize_h - 448) / 2
        crop_px = (32, margin_y, 480, 448 + margin_y)

    return img.resize(resize_px, resample=Image.Resampling.BICUBIC).crop(crop_px)

```

# contrib/classification/download/download_caltech101.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "beartype",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the Caltech101 dataset for use as an saev.activations.ImageFolderDataset.

```sh
uv run contrib/classification/download_flowers.py --help
```
"""

import dataclasses
import os
import random
import shutil
import tarfile
import zipfile

import beartype
import requests
import tqdm
import tyro

url = "https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip"


IMG_EXTS = (".jpg", ".jpeg", ".png")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    seed: int = 42
    """Random seed used to generate split."""


def main(args: Args):
    """Download Caltech 101."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    zip_path = os.path.join(args.dir, "caltech-101.zip")
    tgz_path = os.path.join(args.dir, "caltech-101", "101_ObjectCategories.tar.gz")

    # Download dataset.
    r = requests.get(url, stream=True)
    r.raise_for_status()

    n_bytes = int(r.headers["content-length"])

    with open(zip_path, "wb") as fd:
        for chunk in tqdm.tqdm(
            r.iter_content(chunk_size=chunk_size),
            total=n_bytes / chunk_size,
            unit="b",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading dataset",
        ):
            fd.write(chunk)
    print(f"Downloaded dataset: {zip_path}.")

    zip = zipfile.ZipFile(zip_path)
    zip.extract("caltech-101/101_ObjectCategories.tar.gz", args.dir)
    print("Unzipped file.")

    with tarfile.open(tgz_path, "r") as tar:
        for member in tqdm.tqdm(tar, desc="Extracting images"):
            tar.extract(member, path=args.dir, filter="data")
    print("Extracted images.")

    # Clean up and organize files

    # Remove the temporary caltech-101 directory
    dpath = os.path.join(args.dir, "caltech-101")
    shutil.rmtree(dpath)
    print(f"Removed temporary directory: {dpath}")

    # Move 101_ObjectCategories to caltech-101
    os.rename(os.path.join(args.dir, "101_ObjectCategories"), dpath)
    print(f"Moved dataset to: {dpath}")

    # Remove the BACKGROUND_Google folder
    shutil.rmtree(os.path.join(dpath, "BACKGROUND_Google"))
    print("Removed BACKGROUND_Google folder")

    # Create train/test split
    random.seed(args.seed)

    # Create output directories
    train_dir = os.path.join(args.dir, "train")
    test_dir = os.path.join(args.dir, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    # Process each class directory
    for class_name in sorted(os.listdir(dpath)):
        class_dpath = os.path.join(dpath, class_name)
        if not os.path.isdir(class_dpath):
            print(f"Skippping {class_dpath} because it is not a directory.")
            continue

        # Get all image files
        image_files = [
            f for f in sorted(os.listdir(class_dpath)) if f.endswith(IMG_EXTS)
        ]
        random.shuffle(image_files)

        # Take first 30 for training
        train_images = image_files[:30]
        # Take up to next 50 for testing
        test_images = image_files[30:80]

        # Create class directories in train and test
        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)

        # Move training images
        for img in train_images:
            src = os.path.join(class_dpath, img)
            dst = os.path.join(train_dir, class_name, img)
            shutil.copy2(src, dst)

        # Move test images
        for img in test_images:
            src = os.path.join(class_dpath, img)
            dst = os.path.join(test_dir, class_name, img)
            shutil.copy2(src, dst)

    # Remove the original directory
    shutil.rmtree(dpath)
    print(f"Created train/test split with {len(os.listdir(train_dir))} classes")
    print(f"Train directory: {train_dir}")
    print(f"Test directory: {test_dir}")


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# contrib/classification/download/download_cub.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "beartype",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///

import dataclasses
import os
import shutil
import tarfile

import beartype
import requests
import tqdm
import tyro

url = "https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz"

IMG_EXTS = (".jpg", ".jpeg", ".png")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    seed: int = 42
    """Random seed used to generate split."""

    download: bool = True
    """Whether to download."""

    extract: bool = True
    """Whether to extract from .tgz file."""


def main(args: Args):
    """Download CUB-200-2011."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    tgz_path = os.path.join(args.dir, "CUB_200_2011.tgz")

    if args.download:
        # Download dataset.
        r = requests.get(url, stream=True)
        r.raise_for_status()

        n_bytes = int(r.headers["content-length"])

        with open(tgz_path, "wb") as fd:
            for chunk in tqdm.tqdm(
                r.iter_content(chunk_size=chunk_size),
                total=n_bytes / chunk_size,
                unit="b",
                unit_scale=1,
                unit_divisor=1024,
                desc="Downloading dataset",
            ):
                fd.write(chunk)
        print(f"Downloaded dataset: {tgz_path}.")

    if args.download or args.extract:
        with tarfile.open(tgz_path, "r") as tar:
            for member in tqdm.tqdm(tar, desc="Extracting data"):
                tar.extract(member, path=args.dir, filter="data")
        print("Extracted data.")

    # Clean up and organize files for a torchvision.datasets.ImageFolder.
    # Some notes on dataset structure:
    #
    # * args.dir/CUB_200_2011/image_class_labels.txt has space-separated pairs of numbers, with the IMG_NUM first and the CLASS_NUM next.
    # * args.dir/CUB_200_2011/images.txt has space-separated (number, string) pairs, where the number is the IMG_NUM above and the string is the relative path from args.dir/CUB_200_2011/images to the actual image.
    # * args.dir/CUB_200_2011/classes.txt has space-separated (number, string) pairs, where the number is the CLASS_NUM above and the string is the classname.
    # * args.dir/CUB_200_2011/classes.txt has space-separated (number, number) pairs, where the first number is the IMG_NUM above and the second number is either 0 or 1. 1 indicates train, 0 indicates test.
    #
    # The next block of code creates the train/test splits in args.dir/train and args.dir/test folders.

    # Create output directories
    train_dir = os.path.join(args.dir, "train")
    test_dir = os.path.join(args.dir, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    # Read the metadata files
    dataset_dir = os.path.join(args.dir, "CUB_200_2011")

    # Read class names
    classes = {}
    with open(os.path.join(dataset_dir, "classes.txt")) as fd:
        for line in fd:
            class_id, class_name = line.strip().split(" ", 1)
            classes[int(class_id)] = class_name.split(".")[-1]

    # Create class directories
    for class_name in classes.values():
        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)

    # Read image paths
    image_paths = {}
    with open(os.path.join(dataset_dir, "images.txt")) as fd:
        for line in fd:
            img_id, img_path = line.strip().split(" ", 1)
            image_paths[int(img_id)] = img_path

    # Read image labels
    image_labels = {}
    with open(os.path.join(dataset_dir, "image_class_labels.txt")) as fd:
        for line in fd:
            img_id, class_id = line.strip().split()
            image_labels[int(img_id)] = int(class_id)

    # Read train/test split
    image_split = {}
    with open(os.path.join(dataset_dir, "train_test_split.txt")) as fd:
        for line in fd:
            img_id, is_train = line.strip().split()
            image_split[int(img_id)] = int(is_train)

    # Copy images to appropriate directories
    for img_id, img_path in tqdm.tqdm(image_paths.items(), desc="Organizing files"):
        # Get source path
        src_path = os.path.join(dataset_dir, "images", img_path)

        # Get class name
        class_id = image_labels[img_id]
        class_name = classes[class_id]

        # Determine destination directory
        dst_dir = train_dir if image_split[img_id] == 1 else test_dir
        dst_path = os.path.join(dst_dir, class_name, os.path.basename(img_path))

        # Copy the file
        shutil.copy2(src_path, dst_path)

    print(f"Dataset organized in {args.dir}/train and {args.dir}/test")


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# contrib/classification/download/download_flowers.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "beartype",
#     "requests",
#     "scipy",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the Flowers102 dataset.

```sh
uv run contrib/classification/download_flowers.py --help
```
"""

import dataclasses
import os
import shutil
import tarfile
from concurrent.futures import ThreadPoolExecutor

import beartype
import requests
import scipy.io
import tqdm
import tyro

images_url = "https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz"
labels_url = "https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat"
splits_url = "https://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat"


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""


def main(args: Args):
    """Download NeWT."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    labels_mat_path = os.path.join(args.dir, "imagelabels.mat")
    splits_mat_path = os.path.join(args.dir, "setid.mat")
    images_tgz_path = os.path.join(args.dir, "102flowers.tgz")
    images_dir_path = os.path.join(args.dir, "jpg")

    # Download labels
    r = requests.get(labels_url, stream=True)
    r.raise_for_status()

    with open(labels_mat_path, "wb") as fd:
        for chunk in r.iter_content(chunk_size=chunk_size):
            fd.write(chunk)
    print(f"Downloaded labels: {labels_mat_path}.")

    # Download split information.
    r = requests.get(splits_url, stream=True)
    r.raise_for_status()

    with open(splits_mat_path, "wb") as fd:
        for chunk in r.iter_content(chunk_size=chunk_size):
            fd.write(chunk)
    print(f"Downloaded split information: {splits_mat_path}.")

    # Download images.
    r = requests.get(images_url, stream=True)
    r.raise_for_status()

    n_bytes = int(r.headers["content-length"])

    with open(images_tgz_path, "wb") as fd:
        for chunk in tqdm.tqdm(
            r.iter_content(chunk_size=chunk_size),
            total=n_bytes / chunk_size,
            unit="b",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading images",
        ):
            fd.write(chunk)
    print(f"Downloaded images: {images_tgz_path}.")

    mat = scipy.io.loadmat(labels_mat_path)
    labels = mat["labels"].reshape(-1).tolist()

    mat = scipy.io.loadmat(splits_mat_path)
    train_ids = set(mat["trnid"].reshape(-1).tolist())
    val_ids = set(mat["valid"].reshape(-1).tolist())
    test_ids = set(mat["tstid"].reshape(-1).tolist())

    with tarfile.open(images_tgz_path, "r") as tar:
        for member in tqdm.tqdm(tar, desc="Extracting images", total=len(labels)):
            tar.extract(member, path=args.dir, filter="data")
    print(f"Extracted images: {images_dir_path}.")

    # In images_dir_path are files labeled image_00001.jpg, image_00002.jpg, etc. There is one label for each image. The structure for torchvision.datasets.ImageFolder is:
    #
    # root/dog/xxx.png
    # root/dog/xxy.png
    # root/dog/[...]/xxz.png
    #
    # root/cat/123.png
    # root/cat/nsdf3.png
    # root/cat/[...]/asd932_.png
    #
    # We can replicate this structure for the flowers102 dataset by making directories for each label and moving images. This can be done efficiently using the python `threading` module because we are IO bound by `shutil.move()`.

    # Create directories for each unique label
    unique_labels = set(labels)
    for label in tqdm.tqdm(unique_labels, desc="Making class folders."):
        for split in ("train", "val", "test"):
            label_dir = os.path.join(args.dir, split, str(label))
            os.makedirs(label_dir, exist_ok=True)

    @beartype.beartype
    def move_image(i: int):
        """Move a single image to its label directory."""
        idx = i + 1
        if idx in train_ids:
            split = "train"
        elif idx in val_ids:
            split = "val"
        elif idx in test_ids:
            split = "test"
        else:
            raise ValueError(f"Image {idx} not in any split.")

        img_num = str(idx).zfill(5)
        src = os.path.join(images_dir_path, f"image_{img_num}.jpg")
        dst = os.path.join(args.dir, split, str(labels[i]), f"image_{img_num}.jpg")
        shutil.move(src, dst)

    # Move files in parallel using a thread pool
    print("Organizing images into class folders.")
    with ThreadPoolExecutor(max_workers=min(32, len(labels))) as executor:
        list(
            tqdm.tqdm(
                executor.map(move_image, range(len(labels))),
                total=len(labels),
                desc="Moving images",
            )
        )

    # Clean up empty source directory
    try:
        os.rmdir(images_dir_path)
    except OSError:
        pass

    print(f"Organized {len(labels)} images into {len(unique_labels)} class folders.")


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# contrib/mllm/__main__.py

```python


```

# contrib/semprobe/__main__.py

```python
import logging
import os.path
import random
import typing

import beartype
import einops
import torch
import tyro

import saev.activations
import saev.nn

from . import config

logger = logging.getLogger("contrib.semprobe")


@beartype.beartype
@torch.inference_mode
def score(cfg: typing.Annotated[config.Score, tyro.conf.arg(name="")]):
    sae = saev.nn.load(cfg.sae_ckpt)
    sae = sae.to(cfg.device)
    logger.info("Loaded SAE.")

    acts_dataset = saev.activations.Dataset(cfg.acts)
    imgs_dataset = saev.activations.ImageFolder(cfg.imgs.root)

    assert len(acts_dataset) // acts_dataset.metadata.n_patches_per_img == len(
        imgs_dataset
    )

    batch_size = (
        cfg.batch_size
        // acts_dataset.metadata.n_patches_per_img
        * acts_dataset.metadata.n_patches_per_img
    )
    n_imgs_per_batch = batch_size // acts_dataset.metadata.n_patches_per_img

    dataloader = torch.utils.data.DataLoader(
        acts_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=cfg.n_workers,
        drop_last=False,
    )
    logger.info("Loaded data.")
    thresholds_T = torch.tensor(cfg.thresholds, device=cfg.device)
    pred_labels_TSN = torch.zeros((
        len(cfg.thresholds),
        sae.cfg.d_sae,
        len(imgs_dataset),
    ))
    true_labels_N = torch.zeros((len(imgs_dataset)))
    task_N = torch.zeros((len(imgs_dataset)))
    task_lookup = {}

    all_latents = []

    for batch in dataloader:
        vit_acts_BD = batch["act"].to(cfg.device)
        i_im = torch.sort(torch.unique(batch["image_i"])).values
        assert len(i_im) <= n_imgs_per_batch
        _, sae_acts_BS, _ = sae(vit_acts_BD)
        sae_acts_SB = einops.rearrange(sae_acts_BS, "batch d_sae -> d_sae batch")
        sae_acts_SIP = sae_acts_SB.view(
            sae.cfg.d_sae, len(i_im), acts_dataset.metadata.n_patches_per_img
        )
        sae_acts_SI = einops.reduce(
            sae_acts_SIP, "d_sae n_img n_patch -> d_sae n_img", "sum"
        )

        # Predictions for each latent is 1 for sae_acts_SI[latent] > threshold, 0 otherwise.
        for j, i in enumerate(i_im.tolist()):
            task, label = imgs_dataset[i]["label"].split("-")
            if task not in task_lookup:
                task_lookup[task] = len(task_lookup)

            task_N[i] = task_lookup[task]
            true_labels_N[i] = 1.0 if label == "positive" else 0.0
            pred_labels_TSN[:, :, i] = (
                (sae_acts_SI[:, j] > thresholds_T[:, None]).cpu().float()
            )

    logger.info("Made %d predictions.", len(imgs_dataset))
    for task_name, task_value in task_lookup.items():
        true_pos_TS = einops.reduce(
            (pred_labels_TSN == true_labels_N)
            & (true_labels_N == 1)
            & (task_N == task_value),
            "thresholds d_sae n_image -> thresholds d_sae",
            "sum",
        )
        false_pos_TS = einops.reduce(
            (pred_labels_TSN != true_labels_N)
            & (true_labels_N == 0)
            & (task_N == task_value),
            "thresholds d_sae n_image -> thresholds d_sae",
            "sum",
        )
        false_neg_TS = einops.reduce(
            (pred_labels_TSN != true_labels_N)
            & (true_labels_N == 1)
            & (task_N == task_value),
            "thresholds d_sae n_image -> thresholds d_sae",
            "sum",
        )
        f1_TS = (2 * true_pos_TS) / (2 * true_pos_TS + false_pos_TS + false_neg_TS)
        f1_S, best_thresh_i_S = f1_TS.max(dim=0)
        best_thresholds_S = thresholds_T[best_thresh_i_S]

        # Get top performing features
        topk_scores, topk_latents = torch.topk(f1_S, k=cfg.top_k)

        print(f"Top {cfg.top_k} features for {task_name}:")
        for score, latent in zip(topk_scores, topk_latents):
            print(f"{latent:>6} >{best_thresholds_S[latent]}: {score:.3f}")

        print(f"Manually included features for {task_name}:")
        for i in cfg.include_latents:
            print(f"{i:>6} >{best_thresholds_S[latent]}: {f1_S[i]:.3f}")

        all_latents.extend(topk_latents.tolist())
        all_latents.extend(cfg.include_latents)

    # Construct command to visualize top features
    latents_str = " ".join(str(i) for i in all_latents)
    cmd = f"""
uv run python -m saev visuals \\
    --ckpt {cfg.sae_ckpt} \\
    --include-latents {latents_str} \\
    --log-freq-range -4.0 -1.0 \\
    --log-value-range -1.0 1.0 \\
    --n-latents {len(all_latents) + 10} \\
    --dump-to $DUMP_TO
""".strip()
    print(f"\nTo visualize, run:\n\n{cmd}")
    print(
        "\nNote that you need to update/add:\n* $DUMP_TO\n* --data.shard-root\n* images:*\n*--images."
    )


@beartype.beartype
def negatives(cfg: typing.Annotated[config.Negatives, tyro.conf.arg(name="")]):
    """
    Sample negative images for each class.
    """
    imgs = saev.activations.get_dataset(cfg.imgs, img_transform=None)

    indices = list(range(len(imgs)))
    random.seed(cfg.seed)

    for cls in cfg.classes:
        random.shuffle(indices)
        dpath = os.path.join(cfg.dump_to, f"{cls}-negative")
        os.makedirs(dpath, exist_ok=True)
        n_saved = 0
        for i in indices:
            if i in cfg.skip:
                continue

            sample = imgs[i]
            fpath = os.path.join(dpath, f"example_{cls}_{i}.png")
            sample["image"].save(fpath)
            n_saved += 1

            if n_saved >= cfg.n_imgs:
                break


if __name__ == "__main__":
    tyro.extras.subcommand_cli_from_dict({
        "score": score,
        "negatives": negatives,
    })

```

# contrib/semprobe/config.py

```python
import dataclasses
import os.path

import beartype

import saev.config


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Score:
    sae_ckpt: str = os.path.join(".", "checkpoints", "abcdefg", "sae.pt")
    """Path to SAE checkpoint"""

    batch_size: int = 2048
    """Batch size for SAE inference."""
    n_workers: int = 32
    """Number of dataloader workers."""

    thresholds: list[float] = dataclasses.field(
        default_factory=lambda: [0.0, 1.0, 3.0, 10.0, 30.0, 100.0]
    )
    """Threshold(s) for feature activation."""
    top_k: int = 5
    """Number of top features to manually analyze."""

    imgs: saev.config.ImageFolderDataset = dataclasses.field(
        default_factory=saev.config.ImageFolderDataset
    )
    """Where curated examples are stored"""
    acts: saev.config.DataLoad = dataclasses.field(default_factory=saev.config.DataLoad)
    """SAE activations for the curated examples."""

    dump_to: str = os.path.join(".", "logs", "contrib", "semprobe")
    """Where to save results/visualizations."""

    include_latents: list[int] = dataclasses.field(default_factory=list)
    """Latents to manually include."""

    device: str = "cuda"
    """Hardware device."""


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Negatives:
    dump_to: str = os.path.join(".", "data", "contrib", "semprobe", "test")
    """Where to save negative samples."""
    imgs: saev.config.DatasetConfig = dataclasses.field(
        default_factory=saev.config.ImagenetDataset
    )
    """Where to sample images from."""
    classes: list[str] = dataclasses.field(
        default_factory=lambda: ["brazil", "cool", "germany", "crash"]
    )
    """Which classes to randomly sample."""
    n_imgs: int = 20
    """Number of negative images."""
    skip: list[str] = dataclasses.field(default_factory=lambda: [])
    """Which images to skip."""

    seed: int = 42
    """Random seed."""

```

# saev - Sparse Auto-Encoders for Vision

![Coverage](docs/coverage.svg)

Implementation of sparse autoencoders (SAEs) for vision transformers (ViTs) in PyTorch.

This is the codebase used for our preprint "Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models"

* [arXiv preprint](https://arxiv.org/abs/2502.06755)
* [Huggingface Models](https://huggingface.co/collections/osunlp/sae-v-67ab8c4fdf179d117db28195)
* [API Docs](https://osu-nlp-group.github.io/SAE-V/saev)
* [Demos](https://osu-nlp-group.github.io/SAE-V/#demos)

## About

saev is a package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.
It also includes an interactive webapp for looking through a trained SAE's features.

Originally forked from [HugoFry](https://github.com/HugoFry/mats_sae_training_for_ViTs) who forked it from [Joseph Bloom](https://github.com/jbloomAus/SAELens).

Read [logbook.md](logbook.md) for a detailed log of my thought process.

See [related-work.md](saev/related-work.md) for a list of works training SAEs on vision models.
Please open an issue or a PR if there is missing work.

## Installation

Installation is supported with [uv](https://docs.astral.sh/uv/).
saev will likely work with pure pip, conda, etc. but I will not formally support it.

To install, clone this repository (maybe fork it first if you want).

In the project root directory, run `uv run python -m saev --help`.
The first invocation should create a virtual environment and show a help message.

## Using `saev`

See the [docs](https://osu-nlp-group.github.io/SAE-V/saev) for an overview.

I recommend using the [llms.txt](https://osu-nlp-group.github.io/SAE-V/llms.txt) file as a way to use any LLM provider to ask questions.
For example, you can run `curl https://osu-nlp-group.github.io/SAE-V/llms.txt | pbcopy` on macOS to copy the text, then paste it into [https://claude.ai](https://claude.ai) and ask any question you have.


# docs/assets/modelcards/SAE_CLIP_24K_ViT-B-16_IN1K.md

---
license: mit
---

# SAE for OpenAI's CLIP ViT-B/16 trained on ImageNet-1K Activations

![Overview of a CLIP-trained SAE](https://osu-nlp-group.github.io/SAE-V/assets/overview2.webp)

* **Homepage:** https://osu-nlp-group.github.io/SAE-V
* **Code:** https://github.com/OSU-NLP-Group/SAE-V
* **Preprint:** https://arxiv.org/abs/2502.06755
* **Demos:** https://osu-nlp-group.github.io/SAE-V#demos
* **Point of Contact:** [Sam Stevens](mailto:stevens.994@buckeyemail.osu.edu)

## Inference Instructions

Follow the instructions [here](https://osu-nlp-group.github.io/SAE-V/saev/#inference-instructions).


# docs/assets/modelcards/SAE_DINOv2_24K_ViT-B-14_IN1K.md

---
license: mit
---

# SAE for Meta's DINOv2 ViT-B/14 trained on ImageNet-1K Activations

* **Homepage:** https://osu-nlp-group.github.io/SAE-V
* **Code:** https://github.com/OSU-NLP-Group/SAE-V
* **Preprint:** https://arxiv.org/abs/2502.06755
* **Demos:** https://osu-nlp-group.github.io/SAE-V#demos
* **Point of Contact:** [Sam Stevens](mailto:stevens.994@buckeyemail.osu.edu)

## Inference Instructions

Follow the instructions [here](https://osu-nlp-group.github.io/SAE-V/saev/#inference-instructions).


# docs/assets/modelcards/SAE_BioCLIP_24K_ViT-B-16_iNat21.md

---
license: mit
---

# SAE for Imageomics's BioCLIP ViT-B/16 trained on iNat2021 Activations

![Overview of a the features found by a BioCLIP-trained SAE](https://osu-nlp-group.github.io/SAE-V/assets/overview2-bioclip.webp)

* **Homepage:** https://osu-nlp-group.github.io/SAE-V
* **Code:** https://github.com/OSU-NLP-Group/SAE-V
* **Preprint:** https://arxiv.org/abs/2502.06755
* **Demos:** https://osu-nlp-group.github.io/SAE-V#demos
* **Point of Contact:** [Sam Stevens](mailto:stevens.994@buckeyemail.osu.edu)

## Inference Instructions

Follow the instructions [here](https://osu-nlp-group.github.io/SAE-V/saev/#inference-instructions).

